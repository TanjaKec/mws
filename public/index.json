[{"authors":null,"categories":null,"content":"I am a long-time R user with a doctorate in Statistics from the University of Manchester. I spent many years working in the U.K. university sector as a senior lecturer and have published an extensive number of articles and papers in the field of quantile regression. I love sharing knowledge and I use R to create various platforms with education materials that can help people learn to extract insight from data. Unsurprisingly, I am an enthusiastic R user and in addition to my involvement supporting women in STEM related activities, I am dedicated to creating an inclusive culture by developing initiatives supporting all underrepresented groups within the DS community.\n  Download my resum√©.\n","date":1404777600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1404777600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/tatjana-kecojevic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tatjana-kecojevic/","section":"authors","summary":"I am a long-time R user with a doctorate in Statistics from the University of Manchester. I spent many years working in the U.K. university sector as a senior lecturer and have published an extensive number of articles and papers in the field of quantile regression.","tags":["Tatjana Kecojevic"],"title":"Tatjana Kecojevic","type":"authors"},{"authors":["Âê≥ÊÅ©ÈÅî"],"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":" true  My recent courses Here you will find some of the courses which I have recently delivered. They serve as individual units and you can access them by clicking on their titles on the left hand side. I hope you will enjoy them.\n ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b6f477fad82ad1d3b1ef331c06f8e5cb","permalink":"/in_media/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/in_media/example/","section":"in_media","summary":"true  My recent courses Here you will find some of the courses which I have recently delivered. They serve as individual units and you can access them by clicking on their titles on the left hand side.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":"My recent courses Here you will find some of the courses which I have recently delivered. They serve as individual units and you can access them by clicking on their titles on the left hand side. I hope you will enjoy them.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"ea3d8a60b91bf3250c437528b104e234","permalink":"/training/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/training/example/","section":"training","summary":"During my lecturing career I was incredibly fortunate to work with some truly amazing colleagues who helped me explore and develop my own teaching philosophy and practice. I gained valuable experience in developing, designing and teaching data analysis modules at varying levels of undergraduate and graduate courses. In particular I mastered my teaching skills by lecturing with George Rawlings and Ian McGowan on decision modelling modules. They taught me how to teach basic statistical concepts, which theoretically may be perceived as complex, in an effective way by emphasising concepts over formulae, engaging students to reason rather than to memorise.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" The ever emerging Internet of Things (IoT) produces vast amounts of data that can be analysed to gain useful insights into trends. The R language is widely used by statisticians for data analysis, and the popularity of R programming has increased greatly in the recent years.\nIn this workshop we teach you to ‚Äòspeak data using R‚Äô by taking a step by step approach, starting by explaining core programming principles of the R programming language, to importing data and employing basic wrangling techniques and the grammar of graphics‚Äô philosophy. This will enable you to produce compelling visualisations that can ultimately illustrate useful insights from your data.\nThrough a series of demonstrations and hands on exercises you will learn some of the fundamental concepts of R and you will get to know how to use its tools in a typical data science project. We will introduce you to graphical and numerical techniques for exploring the information concealed within a dataset. After we develop an understanding of our data we will use R‚Äôs reproducible approach in telling our data story by creating R-Markdown documents.\nAt the end of the workshop, participants will be able to:\n Import data from different sources into the R environment Carry out basic data pre-processing \u0026amp; wrangling Learn to distinguish the most appropriate visualisations to be used for any given data challenge Build advanced visualisations: graphs and maps Gain proficiency in data pre-processing, data wrangling and visualisation in R by putting acquired knowledge into application, adopting a reproducible approach through the creation of RMarkdown documents   This course is available at üëâ https://introtor.netlify.app/   üëâ Go to the following GitHub repo to download the material: https://github.com/TanjaKec/ASDA\nThis workshop has been delivered for ASDA data development training.\nThe workshop has been delivered in partnership with GEOLYTIX.\n","date":1605826800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605826800,"objectID":"377da8a6067a76f6104b36dd6a5ad203","permalink":"/training/example/example4/","publishdate":"2020-11-20T00:00:00+01:00","relpermalink":"/training/example/example4/","section":"training","summary":"The ever emerging Internet of Things (IoT) produces vast amounts of data that can be analysed to gain useful insights into trends. The R language is widely used by statisticians for data analysis, and the popularity of R programming has increased greatly in the recent years.","tags":null,"title":"Learn to access, organise, analyse and argue with data using R","type":"docs"},{"authors":null,"categories":null,"content":" The workshop provides a practical guide for crateing websites using the blogdown package in R that allows you to create websites from R Markdown files using Hugo, an open-source static site generator written in Go. In this workshop you learn how to create dynamic R Markdown documents to build static websites allowing you to use R code to render the results of your analysis. The blogdown through the use of R Markdown allows technical writing allowing you to add graphs, tables, LaTeX equations, theorems, citations, and references. This makes blogdown an perfect tool for designing websites to communicate your R data story telling or just awesome general-purpose websites. After you create your awesome website using the blogdown and Hugo template you can push it onto your GitHub and deploy on Netlify all free of charge.\nObjectives:\n Apply HUGO theme to create a website using the blogdown Personalise the website Deploy site from your computer to the Internet Use GitHub for version control Use Netlify for continuous deployment Updating your website: serving site, push to GitHub, deploy   This course is available at üëâ https://websiteinr.netlify.app/   üëâ Go to the following GitHub repo to download the material: https://github.com/TanjaKec/eRum2020 and follow the steps given in the Xaringan presentation available from üëâ here\n  Figure 1: Xaringan presentation  The workshop has been delivered at eRum!2020 - European R Users Meeting\n","date":1593903600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593903600,"objectID":"b257820fa8e0a559fed54964fc82d724","permalink":"/training/example/example2/","publishdate":"2020-07-05T00:00:00+01:00","relpermalink":"/training/example/example2/","section":"training","summary":"The workshop provides a practical guide for crateing websites using the blogdown package in R that allows you to create websites from R Markdown files using Hugo, an open-source static site generator written in Go.","tags":null,"title":"Build a website with blogdown in R","type":"docs"},{"authors":null,"categories":null,"content":" This course is designed to give you an appreciation of R programming as a tool for reproducible research. It focuses on packages that will help you access your data, do necessary visualisation and communication in a dynamic and reproducible manner.\nIf you would like to learn to:  Store in one place your research methods, results and interpretation Update your research methodology and automate the update of the results Write your research text and code in R Markdown Generate reproducible reports that display your code and results Assemble your research reporting as either HTML, a PDF or a Word document using the package rmarkdown   then this course is for you!  This course is available at üëâ https://reproducibleresearchinr.rbind.io   The course has been delivered at Belgrade Open School (BOS) and the Institute of Philosophy and Social Theory (IFDT) at the University of Belograde.\n ","date":1581721200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581721200,"objectID":"c7bf03d2a83056a9e286b165163e9d60","permalink":"/training/example/example1/","publishdate":"2020-02-15T00:00:00+01:00","relpermalink":"/training/example/example1/","section":"training","summary":"This course is designed to give you an appreciation of R programming as a tool for reproducible research. It focuses on packages that will help you access your data, do necessary visualisation and communication in a dynamic and reproducible manner.","tags":null,"title":"Reproducible Research","type":"docs"},{"authors":null,"categories":null,"content":" This course is designed to give you an appreciation of R programming as a tool for data exploration. It focuses on packages that will help you do exploratory data analysis, visualisation and communication in a dynamic and reproducible manner.\nIf you would like to:  Discover how to find and access data and prepare it for exploration and visualistion Learn to explore, visualize, and analyse data in a dynamic and reproducible manner Gain experience in data wrangling, exploratory data analysis and data visualisation, and effective communication of results Work on case studies inspired by real problems and based on open data   then this course is for you! üòÄ  This course is available at üëâ https://datachallengewithr.rbind.io/   This workshop is a part of The Autumn Data School program organised as a part of the Open Data - Open Opportunities project.\nYou can download the Autumn Data School program from here üëá\nThe workshop has been delivered in partnership with UNDP Serbia\n ","date":1572735600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572735600,"objectID":"9c89db77dff1f0c94bf1305194fe63dc","permalink":"/training/example/example3/","publishdate":"2019-11-03T00:00:00+01:00","relpermalink":"/training/example/example3/","section":"training","summary":"This course is designed to give you an appreciation of R programming as a tool for data exploration. It focuses on packages that will help you do exploratory data analysis, visualisation and communication in a dynamic and reproducible manner.","tags":null,"title":"Data challenge with R","type":"docs"},{"authors":null,"categories":null,"content":"      This short tutorial on Subset variable Selection in R comes from pp.¬†244-251 of ‚ÄúIntroduction to Statistical Learning with Applications in R‚Äù by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani and chapter ‚ÄúForward, Backward, and Stepwise Selection‚Äù, from pp.¬†511-519 of ‚ÄúApplied Predictive Modeling‚Äù by Max Kuhn and Kjell Johnson.\nFor the large numbers of explanatory variables, and many interactions and non-linear terms, the process of model simplification can take a very long time. There are many algorithms for automatic variable selection that can help to chose the variables to include in a regression model.\nUsing Best Subsets Regression and Stepwise Regression Stepwise regression and Best Subsets regression (BREG) are two of the more common variable selection methods. The stepwise procedure starts from the saturated model (or the maximal model, whichever is appropriate) through a series of simplifications to the minimal adequate model. This progression is made on the basis of deletion tests: F tests, AIC, t-tests or chi-squared tests that assess the significance of the increase in deviance that results when a given term is removed from the current model.\nThe BREG, also known as ‚Äúall possible regressions‚Äù, as the name of the procedure indicates, fits a separate least squares regression for each possible combination of the \\(p\\) predictors, i.e.¬†explanatory variables. After fitting all of the models, BREG then displays the best fitted models with one explanatory variable, two explanatory variables, three explanatory variables, and so on. Usually, either adjusted R-squared or Mallows Cp is the criterion for picking the best fitting models for this process. The result is a display of the best fitted models of different sizes up to the full/maximal model and the final fitted model can be selected by comparing displayed models based on the criteria of parsimony.\nCase Study We want to apply the subset selection approach to the fat data, available in the library(faraway). We wish to predict body fat (variable brozek) using all predictors except for siri, density and free.\nAs instructed, we will start off by creating a subset of all the variables from the fat data set except for siri, density and free. We will save this subset as a new data frame bodyfat. To confirm that the three suggested variables have been removed, we will check for the difference in the dimension of the two data frames and have a glimpse at teh data.\n#If you don\u0026#39;t have the \u0026quot;ISLR\u0026quot; package installed yet, uncomment and run the line below #install.packages(\u0026quot;ISLR\u0026quot;) library(ISLR) library(faraway) library(tidyr) suppressPackageStartupMessages(library(dplyr)) bodyfat \u0026lt;- fat %\u0026gt;% select(-siri, -density, -free) dim(fat) - dim(bodyfat) ## [1] 0 3 glimpse(bodyfat) ## Rows: 252 ## Columns: 15 ## $ brozek \u0026lt;dbl\u0026gt; 12.6, 6.9, 24.6, 10.9, 27.8, 20.6, 19.0, 12.8, 5.1, 12.0, 7.5‚Ä¶ ## $ age \u0026lt;int\u0026gt; 23, 22, 22, 26, 24, 24, 26, 25, 25, 23, 26, 27, 32, 30, 35, 3‚Ä¶ ## $ weight \u0026lt;dbl\u0026gt; 154.25, 173.25, 154.00, 184.75, 184.25, 210.25, 181.00, 176.0‚Ä¶ ## $ height \u0026lt;dbl\u0026gt; 67.75, 72.25, 66.25, 72.25, 71.25, 74.75, 69.75, 72.50, 74.00‚Ä¶ ## $ adipos \u0026lt;dbl\u0026gt; 23.7, 23.4, 24.7, 24.9, 25.6, 26.5, 26.2, 23.6, 24.6, 25.8, 2‚Ä¶ ## $ neck \u0026lt;dbl\u0026gt; 36.2, 38.5, 34.0, 37.4, 34.4, 39.0, 36.4, 37.8, 38.1, 42.1, 3‚Ä¶ ## $ chest \u0026lt;dbl\u0026gt; 93.1, 93.6, 95.8, 101.8, 97.3, 104.5, 105.1, 99.6, 100.9, 99.‚Ä¶ ## $ abdom \u0026lt;dbl\u0026gt; 85.2, 83.0, 87.9, 86.4, 100.0, 94.4, 90.7, 88.5, 82.5, 88.6, ‚Ä¶ ## $ hip \u0026lt;dbl\u0026gt; 94.5, 98.7, 99.2, 101.2, 101.9, 107.8, 100.3, 97.1, 99.9, 104‚Ä¶ ## $ thigh \u0026lt;dbl\u0026gt; 59.0, 58.7, 59.6, 60.1, 63.2, 66.0, 58.4, 60.0, 62.9, 63.1, 5‚Ä¶ ## $ knee \u0026lt;dbl\u0026gt; 37.3, 37.3, 38.9, 37.3, 42.2, 42.0, 38.3, 39.4, 38.3, 41.7, 3‚Ä¶ ## $ ankle \u0026lt;dbl\u0026gt; 21.9, 23.4, 24.0, 22.8, 24.0, 25.6, 22.9, 23.2, 23.8, 25.0, 2‚Ä¶ ## $ biceps \u0026lt;dbl\u0026gt; 32.0, 30.5, 28.8, 32.4, 32.2, 35.7, 31.9, 30.5, 35.9, 35.6, 3‚Ä¶ ## $ forearm \u0026lt;dbl\u0026gt; 27.4, 28.9, 25.2, 29.4, 27.7, 30.6, 27.8, 29.0, 31.1, 30.0, 2‚Ä¶ ## $ wrist \u0026lt;dbl\u0026gt; 17.1, 18.2, 16.6, 18.2, 17.7, 18.8, 17.7, 18.8, 18.2, 19.2, 1‚Ä¶ Using the glimpse() function allows us to notice that all the variables in our new bodyfat data set are the measured type. #### Get to Know Your Data\nLet us take a look at the data using the DT package, which provides an R interface to the JavaScript library DataTables. It will enable us to filter through the displayed data and quickly check for NAs to see if there are any missing values that should be of concern.\nDT::datatable(fat)  {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\"],[12.6,6.9,24.6,10.9,27.8,20.6,19,12.8,5.1,12,7.5,8.5,20.5,20.8,21.7,20.5,28.1,22.4,16.1,16.5,19,15.3,15.7,17.6,14.2,4.6,8.5,22.4,4.7,9.4,12.3,6.5,13.4,20.9,31.1,38.2,23.6,27.5,33.8,31.3,33.1,31.7,30.4,30.8,8.4,14.1,11.2,6.4,13.4,5,10.7,7.4,8.7,7.1,4.9,22.2,20.1,27.1,30.4,24,25.4,28.8,29.6,25.1,31,28.9,21.1,14,7.1,13.2,23.7,9.4,9.1,13.7,12,18.3,9.2,21.7,21.1,18.6,30.2,26,18.2,26.2,26.1,25.8,15,22.6,8.8,14.3,20.2,18.1,9.2,24.2,9.6,17.3,10.1,11.1,17.7,21.7,20.8,20.1,19.8,21.9,24.7,17.8,19.1,18.2,17.2,21,19.5,27.1,21.6,20.9,25.9,16.7,19.8,14.1,25.1,17.9,27,24.6,14.8,16,14,17.4,26.4,17.4,20.4,15,18,22.2,23.1,25.3,23.8,26.3,21.4,28.4,21.8,20.1,24.3,18.1,22.7,9.9,10.8,14.4,19,28.6,6.1,24.5,9.9,19.1,10.6,16.5,20.5,17.2,30.1,10.5,12.8,22,9.9,14.8,13.3,15.2,26.5,19,21.4,20,34.7,16.5,4.1,1.9,20.2,16.8,24.6,10.4,13.4,28.8,22,16.8,25.8,0,11.9,12.4,17.4,9.2,23,20.1,20.2,23.8,11.8,36.5,16,24,22.3,24.8,21.5,17.6,7.3,22.6,12.5,21.7,27.7,6.8,33.4,16.6,31.7,31.5,10.1,11.3,7.8,26.4,19.3,18.5,19.3,45.1,13.8,8.2,23.9,15.1,12.7,25.3,11.9,6.1,11.3,12.8,14.9,24.5,15,16.9,11.1,16.1,15.5,25.9,25.5,18.4,24,26.4,12.7,28.8,17,33.6,29.3,31.4,28.1,15.3,29.1,11.5,32.3,28.3,25.3,30.7],[12.3,6.1,25.3,10.4,28.7,20.9,19.2,12.4,4.1,11.7,7.1,7.8,20.8,21.2,22.1,20.9,29,22.9,16,16.5,19.1,15.2,15.6,17.7,14,3.7,7.9,22.9,3.7,8.8,11.9,5.7,11.8,21.3,32.3,40.1,24.2,28.4,35.2,32.6,34.5,32.9,31.6,32,7.7,13.9,10.8,5.6,13.6,4,10.2,6.6,8,6.3,3.9,22.6,20.4,28,31.5,24.6,26.1,29.8,30.7,25.8,32.3,30,21.5,13.8,6.3,12.9,24.3,8.8,8.5,13.5,11.8,18.5,8.8,22.2,21.5,18.8,31.4,26.8,18.4,27,27,26.6,14.9,23.1,8.3,14.1,20.5,18.2,8.5,24.9,9,17.4,9.6,11.3,17.8,22.2,21.2,20.4,20.1,22.3,25.4,18,19.3,18.3,17.3,21.4,19.7,28,22.1,21.3,26.7,16.7,20.1,13.9,25.8,18.1,27.9,25.3,14.7,16,13.8,17.5,27.2,17.4,20.8,14.9,18.1,22.7,23.6,26.1,24.4,27.1,21.8,29.4,22.4,20.4,24.9,18.3,23.3,9.4,10.3,14.2,19.2,29.6,5.3,25.2,9.4,19.6,10.1,16.5,21,17.3,31.2,10,12.5,22.5,9.4,14.6,13,15.1,27.3,19.2,21.8,20.3,34.3,16.5,3,0.7,20.5,16.9,25.3,9.9,13.1,29.9,22.5,16.9,26.6,0,11.5,12.1,17.5,8.6,23.6,20.4,20.5,24.4,11.4,38.1,15.9,24.7,22.8,25.5,22,17.7,6.6,23.6,12.2,22.1,28.7,6,34.8,16.6,32.9,32.8,9.6,10.8,7.1,27.2,19.5,18.7,19.5,47.5,13.6,7.5,24.5,15,12.4,26,11.5,5.2,10.9,12.5,14.8,25.2,14.9,17,10.6,16.1,15.4,26.7,25.8,18.6,24.8,27.3,12.4,29.9,17,35,30.4,32.6,29,15.2,30.2,11,33.6,29.3,26,31.9],[1.0708,1.0853,1.0414,1.0751,1.034,1.0502,1.0549,1.0704,1.09,1.0722,1.083,1.0812,1.0513,1.0505,1.0484,1.0512,1.0333,1.0468,1.0622,1.061,1.0551,1.064,1.0631,1.0584,1.0668,1.0911,1.0811,1.0468,1.091,1.079,1.0716,1.0862,1.0719,1.0502,1.0263,1.0101,1.0438,1.0346,1.0202,1.0258,1.0217,1.025,1.0279,1.0269,1.0814,1.067,1.0742,1.0665,1.0678,1.0903,1.0756,1.084,1.0807,1.0848,1.0906,1.0473,1.0524,1.0356,1.028,1.043,1.0396,1.0317,1.0298,1.0403,1.0264,1.0313,1.0499,1.0673,1.0847,1.0693,1.0439,1.0788,1.0796,1.068,1.072,1.0666,1.079,1.0483,1.0498,1.056,1.0283,1.0382,1.0568,1.0377,1.0378,1.0386,1.0648,1.0462,1.08,1.0666,1.052,1.0573,1.0795,1.0424,1.0785,1.0991,1.077,1.073,1.0582,1.0484,1.0506,1.0524,1.053,1.048,1.0412,1.0578,1.0547,1.0569,1.0593,1.05,1.0538,1.0355,1.0486,1.0503,1.0384,1.0607,1.0529,1.0671,1.0404,1.0575,1.0358,1.0414,1.0652,1.0623,1.0674,1.0587,1.0373,1.059,1.0515,1.0648,1.0575,1.0472,1.0452,1.0398,1.0435,1.0374,1.0491,1.0325,1.0481,1.0522,1.0422,1.0571,1.0459,1.0775,1.0754,1.0664,1.055,1.0322,1.0873,1.0416,1.0776,1.0542,1.0758,1.061,1.051,1.0594,1.0287,1.0761,1.0704,1.0477,1.0775,1.0653,1.069,1.0644,1.037,1.0549,1.0492,1.0525,1.018,1.061,1.0926,1.0983,1.0521,1.0603,1.0414,1.0763,1.0689,1.0316,1.0477,1.0603,1.0387,1.1089,1.0725,1.0713,1.0587,1.0794,1.0453,1.0524,1.052,1.0434,1.0728,1.014,1.0624,1.0429,1.047,1.0411,1.0488,1.0583,1.0841,1.0462,1.0709,1.0484,1.034,1.0854,1.0209,1.061,1.025,1.0254,1.0771,1.0742,1.0829,1.0373,1.0543,1.0561,1.0543,0.995,1.0678,1.0819,1.0433,1.0646,1.0706,1.0399,1.0726,1.0874,1.074,1.0703,1.065,1.0418,1.0647,1.0601,1.0745,1.062,1.0636,1.0384,1.0403,1.0563,1.0424,1.0372,1.0705,1.0316,1.0599,1.0207,1.0304,1.0256,1.0334,1.0641,1.0308,1.0736,1.0236,1.0328,1.0399,1.0271],[23,22,22,26,24,24,26,25,25,23,26,27,32,30,35,35,34,32,28,33,28,28,31,32,28,27,34,31,27,29,32,29,27,41,41,49,40,50,46,50,45,44,48,41,39,43,40,39,45,47,47,40,51,49,42,54,58,62,54,61,62,56,54,61,57,55,54,55,54,55,62,55,56,55,61,61,57,69,81,66,67,64,64,70,72,67,72,64,46,48,46,44,47,46,47,53,38,50,46,47,49,48,41,49,43,43,43,52,43,40,43,43,47,42,48,40,48,51,40,44,52,44,40,47,50,46,42,43,40,42,49,40,47,50,41,44,39,43,40,49,40,40,52,23,23,24,24,25,25,26,26,26,27,27,27,28,28,28,30,31,31,33,33,34,34,35,35,35,35,35,35,35,35,36,36,37,37,37,38,39,39,40,40,40,40,40,41,41,41,41,41,42,42,42,42,42,42,42,42,43,43,43,43,44,44,44,44,47,47,47,49,49,49,50,50,51,51,51,52,53,54,54,54,55,55,55,55,55,56,56,57,57,58,58,60,62,62,63,64,65,65,65,66,67,67,68,69,70,72,72,72,74],[154.25,173.25,154,184.75,184.25,210.25,181,176,191,198.25,186.25,216,180.5,205.25,187.75,162.75,195.75,209.25,183.75,211.75,179,200.5,140.25,148.75,151.25,159.25,131.5,148,133.25,160.75,182,160.25,168,218.5,247.25,191.75,202.25,196.75,363.15,203,262.75,205,217,212,125.25,164.25,133.5,148.5,135.75,127.5,158.25,139.25,137.25,152.75,136.25,198,181.5,201.25,202.5,179.75,216,178.75,193.25,178,205.5,183.5,151.5,154.75,155.25,156.75,167.5,146.75,160.75,125,143,148.25,162.5,177.75,161.25,171.25,163.75,150.25,190.25,170.75,168,167,157.75,160,176.75,176,177,179.75,165.25,192.5,184.25,224.5,188.75,162.5,156.5,197,198.5,173.75,172.75,196.75,177,165.5,200.25,203.25,194,168.5,170.75,183.25,178.25,163,175.25,158,177.25,179,191,187.5,206.5,185.25,160.25,151.5,161,167,177.5,152.25,192.25,165.25,171.75,171.25,197,157,168.25,186,166.75,187.75,168.25,212.75,176.75,173.25,167,159.75,188.15,156,208.5,206.5,143.75,223,152.25,241.75,146,156.75,200.25,171.5,205.75,182.5,136.5,177.25,151.25,196,184.25,140,218.75,217,166.25,224.75,228.25,172.75,152.25,125.75,177.25,176.25,226.75,145.25,151,241.25,187.25,234.75,219.25,118.5,145.75,159.25,170.5,167.5,232.75,210.5,202.25,185,153,244.25,193.5,224.75,162.75,180,156.25,168,167.25,170.75,178.25,150,200.5,184,223,208.75,166,195,160.5,159.75,140.5,216.25,168.25,194.75,172.75,219,149.25,154.5,199.25,154.5,153.25,230,161.75,142.25,179.75,126.5,169.5,198.5,174.5,167.75,147.75,182.25,175.5,161.75,157.75,168.75,191.5,219.15,155.25,189.75,127.5,224.5,234.25,227.75,199.5,155.5,215.5,134.25,201,186.75,190.75,207.5],[67.75,72.25,66.25,72.25,71.25,74.75,69.75,72.5,74,73.5,74.5,76,69.5,71.25,69.5,66,71,71,67.75,73.5,68,69.75,68.25,70,67.75,71.5,67.5,67.5,64.75,69,73.75,71.25,71.25,71,73.5,65,70,68.25,72.25,67,68.75,29.5,70,71.5,68,73.25,67.5,71.25,68.5,66.75,72.25,69,67.75,73.5,67.5,72,68,69.5,70.75,65.75,73.25,68.5,70.25,67,70,67.5,70.75,71.5,69.25,71.5,71.5,68.75,73.75,64,65.75,67.5,69.5,68.5,70.25,69.25,67.75,67.25,72.75,70,69.25,67.5,67.25,65.75,72.5,73,70,69.5,70.5,71.75,74.5,77.75,73.25,66.5,68.25,72,73.5,72,71.25,73.75,69.25,68.5,73.5,74.25,75.5,69.25,68.5,70,70,70.25,71.75,69.25,72.75,72,74,72.25,74.5,71.5,68.75,66.75,66.5,67,68.75,67.75,73.25,69.75,71.5,70.5,73.25,66.75,69.5,69.75,70.75,74,71.25,75,71,69.5,67.75,72.25,77.5,70.75,72.75,69.75,72.5,70.25,69,74.5,72.25,67.25,73.5,75.25,69,72.25,68.75,71.5,72.25,73,68.75,70.5,72,73.75,68,72.25,69.5,69.5,67.75,65.5,71,71.5,71.75,69.25,67,71.5,69.25,74.5,74.25,68,67.25,69.75,74.25,71.5,74.25,72,72.5,68.25,69.25,76,70.5,74.75,72.75,68.25,69,71.5,72.75,67.5,70.25,69.25,71.5,74,69.75,73,65.5,72.5,70.25,70.75,68,74.5,71.75,70.75,73,64,69.75,70,71.75,69.25,70.5,72.25,67.5,67.25,68.75,66.75,68.25,74.25,69.5,68.5,65.75,71.75,71.5,67.25,67.5,67.5,72.25,69.5,69.5,65.75,65.75,68.25,72,72.75,68.5,69.25,70.5,67,69.75,66,70.5,70],[23.7,23.4,24.7,24.9,25.6,26.5,26.2,23.6,24.6,25.8,23.6,26.3,26.3,28.5,27.4,26.3,27.3,29.2,28.2,27.6,27.3,29.1,21.2,21.4,23.2,21.9,20.3,22.9,22.4,23.8,23.6,22.2,23.3,30.5,32.2,32,29.1,29.7,48.9,31.8,39.1,29.9,31.2,29.2,19.1,21.3,20.6,20.6,20.4,20.2,21.3,20.6,21.1,19.9,21.1,26.9,27.6,29.3,28.4,29.2,28.2,26.8,27.6,27.9,29.5,28.3,21.3,21.3,22.8,21.6,23.1,21.9,20.8,21.5,23.3,22.9,23.7,26.7,23,25.1,25.1,23.4,25.3,24.5,24.7,26,24.6,26,23.7,23.3,25.4,26.2,23.4,26.3,23.4,26.1,24.8,25.9,23.7,26.7,25.9,23.6,24,25.5,26,24.8,26,26,24,24.7,25.6,26.3,25.6,23.3,24,23.4,23.6,24.3,24.6,25.3,26.2,25.5,23.9,23.9,25.6,26.2,26.4,23.4,25.2,23.9,23.7,24.3,25.8,24.8,24.5,26.8,23.5,24.1,23.3,26.6,24.6,25.3,25.6,21.6,22.1,21.9,27.7,29.8,19.3,31.8,22.5,30.7,19.7,24.4,26.1,21.6,30.4,24.6,20.3,24.4,20.4,25.9,24.4,19.8,29.7,28.1,25.3,30.3,33.3,25.2,23.4,20.6,24.8,24.3,31,21.3,23.7,33.2,27.5,29.8,28,18.1,22.7,23,21.8,23.1,29.7,28.6,27,28,22.5,29.8,27.4,28.3,21.6,27.2,23.1,23.1,22.3,26.4,25.4,22,27.6,23.7,32.3,27.6,27.2,26.1,22.9,22.5,21.4,27.4,23,27.4,22.8,37.6,21.6,22.2,27.2,22.7,24.5,31,25,22.2,26.8,20,25.6,25.3,25.4,25.2,24.1,24.9,24.2,25.2,24.1,26.1,25.8,31.9,22.6,30.9,20.8,33.9,31.8,30.3,29.9,22.8,30.5,21.1,29.1,30.2,27,29.8],[134.9,161.3,116,164.7,133.1,167,146.6,153.6,181.3,174.4,172.3,197.7,143.5,162.5,147,129.3,140.8,162.5,154.3,176.8,145.1,169.8,118.2,122.6,129.8,151.9,120.3,114.9,127,145.7,159.7,149.8,142.5,172.7,170.4,118.4,154.5,142.6,240.5,139.4,175.8,140.1,151.1,146.7,114.7,141.1,118.5,139,117.6,121.2,141.4,129,125.3,142,129.6,154.1,145.1,146.7,141,136.7,161.2,127.4,136.1,133.3,141.7,130.4,119.6,133.1,144.2,136.1,127.8,132.9,146.1,107.9,125.9,121.1,147.5,139.1,127.2,139.5,114.3,111.2,155.6,126,124.1,123.9,134.1,123.8,161.1,150.9,141.3,147.3,150.1,145.9,166.6,185.7,169.6,143.5,128.8,154.2,157.2,138.9,138.6,153.7,133.2,136,162,166.3,160.6,133.1,137.5,133.5,139.7,128.9,129.9,131.7,142.1,153.8,143.1,153.8,150.7,139.6,136.5,127.3,138.5,137.9,130.7,125.8,153,140.5,140.9,133.3,151.2,117.2,128.3,137.1,131,134.4,131.6,169.9,133.8,141.8,129,143.9,168.4,133.6,168.9,147.5,135,168.3,137.2,195.1,130.5,130.9,159.3,142,143.9,163.4,119.1,138.3,136.2,167,159.8,118.8,160.8,175.8,130.7,179.7,149.3,144.2,146.1,123.4,141.7,146.6,170.9,130.2,130.8,171.7,146.1,195.3,162.7,118.5,128.4,139.5,140.8,152.1,179.2,168.3,161.4,141,135,155.2,162.6,170.8,126.5,135.4,122.6,138.4,155.1,132.1,155.9,117.5,144.9,171.4,148.5,174.2,113.5,133.6,144.3,141.8,129.5,159.3,135.9,158.7,139.4,120.2,128.7,141.9,151.7,131.2,151.3,171.9,142.6,133.6,159.5,110.3,144.2,149.9,148.3,139.4,131.4,152.9,148.4,119.9,117.5,137.6,145.2,161.2,135.5,135.1,105.9,149.2,165.6,156.3,143.6,131.8,152.7,118.9,136.1,133.9,142.6,143.7],[36.2,38.5,34,37.4,34.4,39,36.4,37.8,38.1,42.1,38.5,39.4,38.4,39.4,40.5,36.4,38.9,42.1,38,40,39.1,41.3,33.9,35.5,34.5,35.7,36.2,38.8,36.4,36.7,38.7,37.3,38.1,39.8,42.1,38.4,38.5,42.1,51.2,40.2,43.2,36.6,37.3,41.5,31.5,35.7,33.6,34.6,32.8,34,34.9,34.3,36.5,35.1,37.8,39.9,39.1,40.5,40.5,38.4,41.4,35.6,38,37.4,40.1,40.9,35.6,36.9,37.5,36.3,35.5,38.7,36.4,33.2,36.5,36,38.7,38.7,37.8,37.4,38.4,38.1,39.3,38.7,38.5,36.5,37.7,36.5,38,36.7,37.2,39.2,37.5,38,37.3,41.1,37.5,38.7,35.9,40,40.1,37,36.3,40.7,39.6,31.1,38.6,42,38.5,34.2,37.2,37.1,40.2,35.3,38,36.3,36.8,41,38.3,38,40.8,39.5,36.9,36.9,37.7,36.6,38.9,37.5,39.8,38.3,35.5,36.3,37.8,37.8,36.5,37.8,37,37.7,34.3,40.8,37.4,36.5,37.5,35.5,38,35.7,39.2,40.9,35.2,40.6,35.4,41.8,34.1,37.9,38.2,35.6,38.5,37,35.9,36.2,35,38.5,40.7,36,39.5,40.5,38.5,43.9,40.4,37.6,37,34,38.4,38.7,41.5,36,35.3,42.1,38,42.8,40,33.8,35.5,35.3,37.7,39.4,41.9,38.5,40.8,38,36.4,41.8,40.7,38.5,35.4,38.5,35.5,36.5,37.6,37.4,37.8,35.2,37.9,37.9,40.9,41.9,39.1,40.2,36,34.5,35.8,40.2,38.3,39,37.4,41.2,34.8,36.9,39.4,37.6,38.5,42.5,37.4,35.2,41.1,33.4,37.2,38.3,38.1,37.4,35.2,39.4,38,35.1,40.4,38.3,40.6,40.2,37.9,40.8,34.7,38.8,41.4,41.3,40.7,36.3,40.8,34.9,40.9,38.9,38.9,40.8],[93.1,93.6,95.8,101.8,97.3,104.5,105.1,99.6,100.9,99.6,101.5,103.6,102,104.1,101.3,99.1,101.9,107.6,106.8,106.2,103.3,111.4,86,86.7,90.2,89.6,88.6,97.4,93.5,97.4,100.5,93.5,93,111.7,117,118.5,106.5,105.6,136.2,114.8,128.3,106,113.3,106.6,85.1,96.6,88.2,89.8,92.3,83.4,90.2,89.2,89.7,93.3,87.6,107.6,100,111.5,115.4,104.8,112.3,102.9,107.6,105.3,105.3,103,90,95.4,89.3,94.4,97.6,88.5,93.6,87.7,93.4,91.6,91.6,102,96.4,102.7,97.7,97.1,103.1,101.8,101.4,98.9,97.5,104.3,97.3,96.7,99.7,101.9,97.2,106.6,99.6,113.2,99.1,99.4,95.1,107.5,106.5,99.1,96.7,103.5,104,93.1,105.2,110,110.1,97.8,96.3,108,99.7,93.5,100.7,97,96,99.2,95.4,101.8,104.3,99.2,99.3,94,98.9,101,98.7,95.9,103.9,96.2,97.8,94.6,103.6,100.4,98.4,104.6,92.9,97.8,98.3,104.7,98.6,99.5,102.7,92.1,96.6,92.7,102,110.9,92.3,114.1,92.9,108.3,88.5,94,101.1,92.1,105.6,98.5,88.7,101.1,94,103.8,98.9,89.2,111.4,107.5,99.1,108.2,114.9,99.1,92.2,90.8,100.5,98.2,115.3,96.8,92.6,119.2,102.7,109.5,108.5,79.3,95.5,92.3,98.9,89.5,117.5,107.4,109.2,103.4,91.4,115.2,104.9,106.7,92.2,101.6,97.8,92,94,103.7,102.7,91.1,107.2,100.8,121.6,105.6,100.6,102.7,99.8,92.9,91.2,115.6,98.3,103.7,98.7,119.8,92.8,93.3,106.8,93.9,99,119.9,94.2,92.7,106.9,88.8,101.7,105.3,104,98.6,99.6,103.4,100.2,94.9,97.2,104.7,104,117.6,95.8,106.4,93,119.6,119.7,115.8,118.3,97.4,113.7,89.2,108.5,111.1,108.3,112.4],[85.2,83,87.9,86.4,100,94.4,90.7,88.5,82.5,88.6,83.6,90.9,91.6,101.8,96.4,92.8,96.4,97.5,89.6,100.5,95.9,98.8,76.4,80,76.3,79.7,74.6,88.7,73.9,83.5,88.7,84.5,79.1,100.5,115.6,113.1,100.9,98.8,148.1,108.1,126.2,104.3,111.2,104.3,76,81.5,73.7,79.5,83.4,70.4,86.7,77.9,82,79.6,77.6,100,99.8,104.2,105.3,98.3,104.8,94.7,102.4,99.7,105.5,100.3,83.9,86.6,78.4,84.6,91.5,82.8,82.9,76,83.3,81.8,78.8,95,95.4,98.6,95.8,89,97.8,94.9,99.8,89.7,88.1,90.9,86,86.5,95.6,93.2,83.1,97.5,88.8,99.2,91.6,86.7,88.2,94,95,92,89.2,95.5,98.6,87.3,102.8,101.6,88.7,92.3,90.6,105,95,89.6,92.4,86.6,90,90,92.4,87.5,99.2,98.1,83.3,86.1,84.1,89.9,92.1,78,93.5,87,90.1,90.3,99.8,89.4,87.2,101.1,86.1,98.6,88.5,106.6,93.1,93,91,77.1,85.3,81.9,99.1,100.5,76.5,106.8,77.6,102.9,72.8,88.2,100.1,83.5,105,90.8,76.6,92.4,81.2,95.6,92.1,83.4,106,95.1,90.4,100.4,115.9,90.8,81.9,75,90.3,90.3,108.8,79.4,83.2,110.3,92.7,104.5,104.6,69.4,83.6,86.8,90.4,83.7,109.3,98.9,98,101.2,80.6,113.7,94.1,105.7,85.6,96.6,86,89.7,78,89.7,89.2,85.7,103.1,89.1,113.9,96.3,93.9,101.3,83.9,84.4,79.4,104,89.7,97.6,87.6,122.1,81.1,81.5,100,88.7,91.8,110.4,87.6,82.8,95.3,78.2,91.1,96.7,89.4,93,86.4,96.7,88.1,94.9,93.3,95.6,98.2,113.8,82.8,100.5,79.7,118,109,113.4,106.1,84.3,107.6,83.6,105,111.5,101.3,108.5],[94.5,98.7,99.2,101.2,101.9,107.8,100.3,97.1,99.9,104.1,98.2,107.7,103.9,108.6,100.1,99.2,105.2,107,102.4,109,104.9,104.8,94.6,93.4,95.8,96.5,85.3,94.7,88.5,98.7,99.8,100.6,94.5,108.3,116.1,113.8,106.2,104.8,147.7,102.5,125.6,115.5,114.1,106,88.2,97.2,88.5,92.7,90.4,87.2,98.3,91,89.1,91.6,88.6,99.6,102.5,105.8,97,99.6,103.1,100.8,99.4,99.7,108.3,104.2,93.9,91.8,96.1,94.3,98.5,95.5,96.3,88.6,93,94.8,94.3,98.3,99.3,100.2,97.1,96.9,99.6,95,96.2,96.2,96.9,93.8,99.3,98.3,102.2,100.6,95.4,100.6,101.4,107.5,102.4,96.2,92.8,103.7,101.7,98.3,98.3,101.6,99.5,96.6,103.6,100.7,102.1,100.6,99.3,103,98.6,99.8,97.5,92.6,99.7,96.4,104.3,101,104.1,101.4,97.5,95.2,94,100,98.5,93.2,99.5,97.8,95.8,99.1,103.2,92.3,98.4,102.1,95.6,100.6,98.3,107.7,101.6,99.3,98.9,93.9,102.5,95.3,110.1,106.2,92.1,113.9,93.5,114.4,91.1,95.2,105,98.3,106.4,102.5,89.8,99.3,91.5,105.1,103.5,89.6,108.8,104.5,95.6,106.8,111.9,98.1,92.8,89.2,98.7,99.9,114.4,89.2,96.4,113.9,101.9,109.9,109.8,85,91.6,96.1,95.5,98.1,108.8,104.1,101.8,103.1,92.3,112.4,102.7,111.8,96.5,100.6,96.2,101,99,94.2,99.2,96.9,105.5,102.6,107.1,102,100.1,101.7,91.8,94,89,109,99.1,104.2,96.1,112.8,96.3,94.4,105,94.5,96.2,105.5,95.6,91.9,98.2,87.5,97.1,106.6,98.4,97,90.1,100.7,97.8,100.2,94,93.7,101.1,111.8,94.5,100.5,87.6,114.3,109.1,109.8,101.6,94.4,110,88.8,104.5,101.7,97.8,107.1],[59,58.7,59.6,60.1,63.2,66,58.4,60,62.9,63.1,59.7,66.2,63.4,66,69,63.1,64.8,66.9,64.2,65.8,63.5,63.4,57.4,54.9,58.4,55,51.7,57.5,50.1,58.9,57.5,58.5,57.3,67.1,71.2,61.9,63.5,66,87.3,61.3,72.5,70.6,67.7,65,50,58.4,53.3,52.7,52,50.6,52.6,51.4,49.3,52.6,51.9,57.2,62.1,61.8,59.1,60.6,61.6,60.9,61,60.8,65,64.8,55,54.3,56,51.2,56.6,58.9,52.9,50.9,55.5,54.5,56.7,55,53.5,56.5,54.8,54.8,58.9,56,56.3,54.7,57.2,57.8,61,60.4,58.3,58.9,56.9,58.9,57.4,61.7,60.6,62.1,54.7,62.7,59,59.3,60,59.1,59.5,54.7,61.2,55.8,57.5,57.5,61.9,63.7,62.3,61.5,59.3,55.9,58.8,56.8,64.6,58.5,58.5,57.1,60.5,58.1,58.5,60.7,60.7,53.5,61.7,57.4,57,60.3,61.2,56.1,56,58.9,58.8,63.6,58.1,66.5,59.1,60.4,57.1,56.1,59.1,56.4,71.2,68.4,51.9,67.6,56.9,72.9,53.6,56.8,62.1,57.3,68.6,60.8,50.1,59.4,52.5,61.4,64,52.4,63.8,64.8,55.5,63.3,74.4,60.1,54.7,50,57.8,59.2,69.2,50.3,60,69.8,64.7,69.5,68.1,47.2,54.1,58,55.4,57.3,67.7,63.5,62.8,61.5,54.3,68.5,60.6,65.3,60.2,61.1,57.7,62.3,57.5,58.5,60.2,55.5,68.8,60.6,63.5,63.3,58.9,60.7,53,56,51.1,63.7,56.3,60,57.1,62.5,53.8,54.7,63.9,53.7,57.7,64.2,59.7,54.4,57.4,50.8,56.6,64,58.4,55.4,53,59.3,57.1,56.8,54.3,54.4,59.3,63.4,61.2,59.2,50.7,61.3,63.7,65.6,58.2,54.3,63.3,49.6,59.6,60.3,56,59.3],[37.3,37.3,38.9,37.3,42.2,42,38.3,39.4,38.3,41.7,39.7,39.2,38.3,41.5,39,38.7,40.8,40,38.7,40.6,38,40.6,35.3,36.2,35.5,36.7,34.7,36,34.5,35.3,38.7,38.8,36.2,44.2,43.3,38.3,39.9,41.5,49.1,41.1,39.6,42.5,40.9,40.2,34.7,38.2,34.5,37.5,35.8,34.4,37.2,34.9,33.7,37.6,34.9,38,39.6,39.8,38,37.7,40.9,38,39.4,40.1,41.2,40.2,36.1,35.4,37.4,37.4,38.6,37.6,37.5,35.4,35.2,37,39.7,38.3,37.5,39.3,38.2,38,39,36.5,36.6,37.8,37.7,39.5,38.4,39.9,38.2,39.7,38.3,40.5,39.6,42.3,39.4,39.3,37.3,39,39.4,38.4,38.4,39.8,36.1,39,39.3,38.7,40,36.8,38,40,38.1,37.8,38.1,36.3,38.4,38.8,41.1,39.2,39.3,40.5,38.7,36.5,36.6,36,36.8,35.8,39,36.9,38.7,38.5,38.1,35.6,36.9,37.9,36.1,39.2,38.4,42.5,39.6,38.2,36.7,36.1,37.6,36.5,43.5,40.8,35.7,42.7,35.9,43.5,36.8,37.4,40,37.8,40,38.5,34.8,39,36.6,40.6,37.3,35.6,42,41.3,34.2,41.7,40.6,39.1,36.2,34.8,37.3,37.7,42.4,34.8,38.1,42.6,39.5,43.1,42.8,33.5,36.2,39.4,38.9,39.7,41.3,39.8,41.3,40.4,36.3,45,38.6,43.3,38.9,38.4,38.6,38,40,39,39.2,35.7,38.3,39,40.3,39.8,37.6,39.4,36.2,38.2,35,40.3,38.8,40.9,38.1,36.9,36.5,39,39.2,36.2,38.1,42.7,40.2,35.2,37.1,33,38.5,42.6,37.4,38.8,35,38.6,38.9,35.9,35.7,37.1,40.3,41.1,39.1,38.1,33.4,42.1,42.4,46,38.8,37.5,44,34.8,40.8,37.3,41.6,42.2],[21.9,23.4,24,22.8,24,25.6,22.9,23.2,23.8,25,25.2,25.9,21.5,23.7,23.1,21.7,23.1,24.4,22.9,24,22.1,24.6,22.2,22.1,22.9,22.5,21.4,21,21.3,22.6,33.9,21.5,24.5,25.2,26.3,21.9,22.6,24.7,29.6,24.7,26.6,23.7,25,23,21,23.4,22.5,21.9,20.6,21.9,22.4,21,21.4,22.6,22.5,22,22.5,22.7,22.5,22.9,23.1,22.1,23.6,22.7,24.7,22.7,21.7,21.5,22.4,21.6,22.4,21.6,23.1,19.1,20.9,21.4,24.2,21.8,21.5,22.7,23.7,22,23,24.1,22,33.7,21.8,23.3,23.8,24.4,22.5,23.1,22.1,24.5,24.6,23.2,22.9,23.3,21.9,22.3,22.3,22.4,23.2,25.4,22,24.8,23.5,23.4,24.8,22.8,22.3,23.6,23.9,21.9,21.8,22.1,22.8,23.3,24.8,24.5,24.6,23.2,22.6,22.1,23.5,21.9,22.2,20.8,21.8,22.2,23.2,23,22.6,20.5,23,22.7,22.4,23.8,22.5,24.5,21.6,22,22.3,22.7,23.2,22,25.2,24.6,22,24.7,20.4,25.1,23.8,22.8,24.9,21.7,25.2,25,21.8,24.6,21,25,23.5,20.4,23.4,25.6,21.9,24.6,24,23.4,22.1,22,22.4,21.5,24,22.2,22,24.8,24.7,25.8,24.1,20.2,21.8,22.7,22.4,22.6,24.7,23.5,24.8,22.9,21.8,25.5,24.7,26,22.4,24.1,24,22.3,22.5,24.1,23.8,22,23.7,24,21.8,24.1,21.4,23.3,22.5,22.6,21.7,23.2,23,25.5,21.8,23.6,21.5,22.6,22.9,22,23.9,27,23.4,22.5,21.8,19.7,22.6,23.4,22.5,23.2,21.3,22.8,23.6,21,21,22.7,23,22.3,22.3,24,20.1,23.4,24.6,25.4,24.1,22.6,22.6,21.5,23.2,21.5,22.7,24.6],[32,30.5,28.8,32.4,32.2,35.7,31.9,30.5,35.9,35.6,32.8,37.2,32.5,36.9,36.1,31.1,36.2,38.2,37.2,37.1,32.5,33,27.9,29.8,31.1,29.9,28.7,29.2,30.5,30.1,32.5,30.1,29,37.5,37.3,32,35.1,33.2,45,34.1,36.4,33.6,36.7,35.8,26.1,29.7,27.9,28.8,28.8,26.8,26,26.7,29.6,38.5,27.7,35.9,33.1,37.7,31.6,34.5,36.2,32.5,32.7,33.6,35.3,34.8,29.6,32.8,32.6,27.3,31.5,30.3,29.7,29.3,29.4,29.3,30.2,30.8,31.4,30.3,29.4,29.9,34.3,31.2,29.7,32.4,32.6,29.2,30.2,28.8,29.1,31.4,30.1,33.3,30.3,32.9,31.6,30.6,31.6,35.3,32.2,27.9,31,31,30.1,31,30.5,35.1,35.1,32.1,33.3,33.5,35.3,30.7,31.8,29.8,29.9,33.4,33.6,32.1,33.9,33,34.4,30.6,34.4,35.6,33.8,33.9,33.3,31.6,27.5,31.2,33.5,33.6,34,30.9,32.7,34.3,31.7,35.5,30.8,32,31.6,30.5,31.8,33.5,36.1,33.3,25.8,36,31.6,38.5,27.8,30.6,33.7,32.2,35.2,31.6,27,30.1,27,31.3,33.5,28.3,34,36.4,30.2,37.2,36.1,32.5,30.4,24.8,31,32.4,35.4,31,31.5,34.4,34.8,39.1,35.6,27.7,31.4,30,30.5,32.9,37.2,36.4,36.6,33.4,29.6,37.1,34,33.7,31.7,32.9,31.2,30.8,30.6,33.8,31.7,29.4,32.1,32.9,34.8,37.3,33.1,36.7,31.4,29,30.9,36.8,29.5,32.7,28.6,34.7,31.3,27.5,35.7,28.5,31.4,38.4,27.9,29.4,34.1,25.3,33.4,33.2,34.6,32.4,31.7,31.8,30.9,27.8,31.3,30.3,32.6,35.1,29.8,35.9,28.5,34.9,35.6,35.3,32.1,29.2,37.5,25.6,35.2,31.3,30.5,33.7],[27.4,28.9,25.2,29.4,27.7,30.6,27.8,29,31.1,30,29.4,30.2,28.6,31.6,30.5,26.4,30.8,31.6,30.5,30.1,30.3,32.8,25.9,26.7,28,28.2,27,26.6,27.9,26.7,27.7,26.4,30,31.5,31.7,29.8,30.6,30.5,29,31,32.7,28.7,29.8,31.5,23.1,27.4,26.2,26.8,25.5,25.8,25.8,26.1,26,27.4,27.5,30.2,28.3,30.9,28.8,29.6,31.8,29.8,29.9,29,31.1,30.1,27.4,27.4,28.1,27.1,27.3,27.3,27.3,25.7,27,27,29.2,25.7,26.8,28.7,27.2,25.2,29.6,27.3,26.3,27.7,28,28.4,29.3,29.6,27.7,28.4,28.2,29.6,27.9,30.8,30.1,27.8,27.5,30.9,31,26.2,29.2,30.3,27.2,29.4,28.5,29.6,30.7,26,28.2,27.8,31.1,27.6,27.3,26.3,28,29.8,29.5,28.6,31.2,29.6,28,27.5,29.2,30.2,30.3,28.2,29.6,27.8,26.5,28.4,28.6,29.3,29.8,28.8,28.3,28.4,27.4,29.8,27.9,28.5,27.5,27.2,29.7,28.3,30.3,29.7,25.2,30.4,29,33.8,26.3,28.3,29.2,27.7,30.7,28,34.9,28.2,26.3,29.2,30.6,26.2,31.2,33.7,28.7,33.1,31.8,29.8,27.4,25.9,28.7,28.4,21,26.9,26.6,29.5,30.3,32.5,29,24.6,28.3,26.4,28.9,29.3,31.8,30.4,32.4,29.2,27.3,31.2,30.1,29.9,27.1,29.8,27.3,27.8,30,28.8,28.4,26.6,28.9,29.2,30.7,23.1,29.5,31.6,27.5,26.2,28.8,31,27.9,30,26.7,29.1,26.3,25.9,30.4,25.7,29.9,32,27,26.8,31.1,22,29.3,30,30.1,29.7,27.3,29.1,29.6,26.1,28.7,26.3,28.5,29.6,28.9,30.5,24.8,30.1,30.7,29.8,29.3,27.3,32.6,25.7,28.6,27.2,29.4,30],[17.1,18.2,16.6,18.2,17.7,18.8,17.7,18.8,18.2,19.2,18.5,19,17.7,18.8,18.2,16.9,17.3,19.3,18.5,18.2,18.4,19.9,16.7,17.1,17.6,17.7,16.5,17,17.2,17.6,18.4,17.9,18.8,18.7,19.7,17,19,19.4,21.4,18.3,21.4,17.4,18.4,18.8,16.1,18.3,17.3,17.9,16.3,16.8,17.3,17.2,16.9,18.5,18.5,18.9,18.5,19.2,18.2,18.5,20.2,18.3,19.1,18.8,18.4,18.7,17.4,18.7,18.1,17.3,18.6,18.3,18.2,16.9,16.8,18.3,18.1,18.8,18.3,19,19,17.7,19,19.2,18,18.2,18.8,18.1,18.8,18.7,17.7,18.8,18.4,19.1,17.8,20.4,18.5,18.2,18.2,18.3,18.6,17,18.4,19.7,17.7,18.8,18.1,19.1,19.2,17.3,18.1,17.4,19.8,17.4,17.5,17.3,18.1,19.5,18.5,18,19.5,18.4,17.6,17.6,18,17.6,17.2,17.4,18.1,17.7,17.6,17.1,17.9,17.3,18.1,17.6,17.1,17.7,17.6,18.7,16.6,17.8,17.9,18.2,18.3,17.3,18.7,18.4,16.9,18.4,17.8,19.6,17.4,17.9,19.4,17.7,19.1,18.6,16.9,18.2,16.5,19.1,19.7,16.5,18.5,19.4,17.7,19.8,18.8,17.4,17.7,16.9,17.7,17.8,20.1,16.9,16.7,18.4,18.1,19.9,19,16.5,17.2,17.4,17.7,18.2,20,19.1,18.8,18.5,17.9,19.9,18.7,18.5,17.1,18.8,17.4,16.9,18.5,18.8,18.6,17.4,18.7,18.4,17.4,19.4,17.3,18.4,17.7,17.6,17.4,18.9,18.6,19,18,18.4,17.8,18.6,19.2,17.1,18.9,19.6,17.8,17,19.2,15.8,18.8,18.4,18.8,19,16.9,19,18,17.6,18.3,18.3,19,18.5,18.3,19.1,16.5,19.4,19.5,19.5,18.5,18.5,18.8,18.5,20.1,18,19.8,20.9]],\"container\":\"\\n \\n \\n  \\n brozek\\n siri\\n density\\n age\\n weight\\n height\\n adipos\\n free\\n neck\\n chest\\n abdom\\n hip\\n thigh\\n knee\\n ankle\\n biceps\\n forearm\\n wrist\\n \\n \\n\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Alternatively, we can use the is.na() function to check if there are any missing observations in particular with respect to the response variable bronzek. For a given input vector the is.na() function returns a vector of TRUE and FALSE value, with a value of TRUE for any elements that are missing, and a FALSE value for non-missing elements. By using the sum() function we can then count all of the missing elements, which can be easily removed from data by using either the na.omit() function or the drop_na function.\nbodyfat %\u0026gt;% select(brozek) %\u0026gt;% is.na() %\u0026gt;% sum()  ## [1] 0  The correlation matrix and its visualisation Previously (see the section on Multiple Regression), we learnt how to examine multivariate data by creating a scatter plot matrix to obtain an in-depth vision into its behaviour. We used the GGally::ggpairs() function that produces a pairwise comparison of multivariate data for both data types: measured and attribute. Creating such a matrix for a data set with a considerable number of variables could be a taxing task with a resulting display that may not be easy to make sense of.\nFabulous Frank Harrell (check his blog Statistical Thinking if you haven‚Äôt stumbled upon it yet!) has created the Hmisc package with many useful functions for data analysis and high-level graphics. It contains the rcorr function for the computation of Pearson or Spearman correlation matrix with pairwise deletion of missing data. It generates one table of correlation coefficients, i.e.¬†the correlation matrix and another table of the \\(p\\)-values.\nbodyfat.rcorr = Hmisc::rcorr(as.matrix(bodyfat)) bodyfat.coeff = bodyfat.rcorr$r bodyfat.p = bodyfat.rcorr$P bodyfat.coeff ## brozek age weight height adipos neck ## brozek 1.00000000 0.28917352 0.61315611 -0.08910641 0.72799418 0.4914889 ## age 0.28917352 1.00000000 -0.01274609 -0.17164514 0.11885126 0.1135052 ## weight 0.61315611 -0.01274609 1.00000000 0.30827854 0.88735216 0.8307162 ## height -0.08910641 -0.17164514 0.30827854 1.00000000 -0.02489094 0.2537099 ## adipos 0.72799418 0.11885126 0.88735216 -0.02489094 1.00000000 0.7778569 ## neck 0.49148893 0.11350519 0.83071622 0.25370988 0.77785691 1.0000000 ## chest 0.70288516 0.17644968 0.89419052 0.13489181 0.91179865 0.7848350 ## abdom 0.81370622 0.23040942 0.88799494 0.08781291 0.92388010 0.7540774 ## hip 0.62569993 -0.05033212 0.94088412 0.17039426 0.88326922 0.7349579 ## thigh 0.56128438 -0.20009576 0.86869354 0.14843561 0.81270609 0.6956973 ## knee 0.50778587 0.01751569 0.85316739 0.28605321 0.71365983 0.6724050 ## ankle 0.26678256 -0.10505810 0.61368542 0.26474369 0.50031664 0.4778924 ## biceps 0.49303089 -0.04116212 0.80041593 0.20781557 0.74638418 0.7311459 ## forearm 0.36327744 -0.08505555 0.63030143 0.22864922 0.55859425 0.6236603 ## wrist 0.34757276 0.21353062 0.72977489 0.32206533 0.62590659 0.7448264 ## chest abdom hip thigh knee ankle ## brozek 0.7028852 0.81370622 0.62569993 0.5612844 0.50778587 0.2667826 ## age 0.1764497 0.23040942 -0.05033212 -0.2000958 0.01751569 -0.1050581 ## weight 0.8941905 0.88799494 0.94088412 0.8686935 0.85316739 0.6136854 ## height 0.1348918 0.08781291 0.17039426 0.1484356 0.28605321 0.2647437 ## adipos 0.9117986 0.92388010 0.88326922 0.8127061 0.71365983 0.5003166 ## neck 0.7848350 0.75407737 0.73495788 0.6956973 0.67240498 0.4778924 ## chest 1.0000000 0.91582767 0.82941992 0.7298586 0.71949640 0.4829879 ## abdom 0.9158277 1.00000000 0.87406618 0.7666239 0.73717888 0.4532227 ## hip 0.8294199 0.87406618 1.00000000 0.8964098 0.82347262 0.5583868 ## thigh 0.7298586 0.76662393 0.89640979 1.0000000 0.79917030 0.5397971 ## knee 0.7194964 0.73717888 0.82347262 0.7991703 1.00000000 0.6116082 ## ankle 0.4829879 0.45322269 0.55838682 0.5397971 0.61160820 1.0000000 ## biceps 0.7279075 0.68498272 0.73927252 0.7614774 0.67870883 0.4848545 ## forearm 0.5801727 0.50331609 0.54501412 0.5668422 0.55589819 0.4190500 ## wrist 0.6601623 0.61983243 0.63008954 0.5586848 0.66450729 0.5661946 ## biceps forearm wrist ## brozek 0.49303089 0.36327744 0.3475728 ## age -0.04116212 -0.08505555 0.2135306 ## weight 0.80041593 0.63030143 0.7297749 ## height 0.20781557 0.22864922 0.3220653 ## adipos 0.74638418 0.55859425 0.6259066 ## neck 0.73114592 0.62366027 0.7448264 ## chest 0.72790748 0.58017273 0.6601623 ## abdom 0.68498272 0.50331609 0.6198324 ## hip 0.73927252 0.54501412 0.6300895 ## thigh 0.76147745 0.56684218 0.5586848 ## knee 0.67870883 0.55589819 0.6645073 ## ankle 0.48485454 0.41904999 0.5661946 ## biceps 1.00000000 0.67825513 0.6321264 ## forearm 0.67825513 1.00000000 0.5855883 ## wrist 0.63212642 0.58558825 1.0000000 bodyfat.p ## brozek age weight height adipos ## brozek NA 3.044832e-06 0.000000e+00 1.584527e-01 0.00000000 ## age 3.044832e-06 NA 8.404326e-01 6.303967e-03 0.05956386 ## weight 0.000000e+00 8.404326e-01 NA 5.989982e-07 0.00000000 ## height 1.584527e-01 6.303967e-03 5.989982e-07 NA 0.69415116 ## adipos 0.000000e+00 5.956386e-02 0.000000e+00 6.941512e-01 NA ## neck 0.000000e+00 7.206646e-02 0.000000e+00 4.614130e-05 0.00000000 ## chest 0.000000e+00 4.966465e-03 0.000000e+00 3.231539e-02 0.00000000 ## abdom 0.000000e+00 2.249495e-04 0.000000e+00 1.646071e-01 0.00000000 ## hip 0.000000e+00 4.263055e-01 0.000000e+00 6.701323e-03 0.00000000 ## thigh 0.000000e+00 1.408117e-03 0.000000e+00 1.838840e-02 0.00000000 ## knee 0.000000e+00 7.820168e-01 0.000000e+00 3.927576e-06 0.00000000 ## ankle 1.770467e-05 9.609999e-02 0.000000e+00 2.062620e-05 0.00000000 ## biceps 0.000000e+00 5.153980e-01 0.000000e+00 9.038055e-04 0.00000000 ## forearm 2.808188e-09 1.783211e-01 0.000000e+00 2.519644e-04 0.00000000 ## wrist 1.446366e-08 6.442181e-04 0.000000e+00 1.721428e-07 0.00000000 ## neck chest abdom hip thigh ## brozek 0.000000e+00 0.000000e+00 0.000000e+00 0.000000000 0.000000000 ## age 7.206646e-02 4.966465e-03 2.249495e-04 0.426305473 0.001408117 ## weight 0.000000e+00 0.000000e+00 0.000000e+00 0.000000000 0.000000000 ## height 4.614130e-05 3.231539e-02 1.646071e-01 0.006701323 0.018388401 ## adipos 0.000000e+00 0.000000e+00 0.000000e+00 0.000000000 0.000000000 ## neck NA 0.000000e+00 0.000000e+00 0.000000000 0.000000000 ## chest 0.000000e+00 NA 0.000000e+00 0.000000000 0.000000000 ## abdom 0.000000e+00 0.000000e+00 NA 0.000000000 0.000000000 ## hip 0.000000e+00 0.000000e+00 0.000000e+00 NA 0.000000000 ## thigh 0.000000e+00 0.000000e+00 0.000000e+00 0.000000000 NA ## knee 0.000000e+00 0.000000e+00 0.000000e+00 0.000000000 0.000000000 ## ankle 8.881784e-16 4.440892e-16 3.597123e-14 0.000000000 0.000000000 ## biceps 0.000000e+00 0.000000e+00 0.000000e+00 0.000000000 0.000000000 ## forearm 0.000000e+00 0.000000e+00 0.000000e+00 0.000000000 0.000000000 ## wrist 0.000000e+00 0.000000e+00 0.000000e+00 0.000000000 0.000000000 ## knee ankle biceps forearm wrist ## brozek 0.000000e+00 1.770467e-05 0.000000e+00 2.808188e-09 1.446366e-08 ## age 7.820168e-01 9.609999e-02 5.153980e-01 1.783211e-01 6.442181e-04 ## weight 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## height 3.927576e-06 2.062620e-05 9.038055e-04 2.519644e-04 1.721428e-07 ## adipos 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## neck 0.000000e+00 8.881784e-16 0.000000e+00 0.000000e+00 0.000000e+00 ## chest 0.000000e+00 4.440892e-16 0.000000e+00 0.000000e+00 0.000000e+00 ## abdom 0.000000e+00 3.597123e-14 0.000000e+00 0.000000e+00 0.000000e+00 ## hip 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## thigh 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## knee NA 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## ankle 0.000000e+00 NA 4.440892e-16 3.886669e-12 0.000000e+00 ## biceps 0.000000e+00 4.440892e-16 NA 0.000000e+00 0.000000e+00 ## forearm 0.000000e+00 3.886669e-12 0.000000e+00 NA 0.000000e+00 ## wrist 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 NA Looking at the figures above, we notice that the response variable brozek is in strong association with all of the explanatory variables from the data set. In fact, it looks as if all variables are in strong correlation with one another, with a very few exceptions such as in the case of height and adipos, hip and age, weight and age to name a few of the most obvious ones. In an example like this one, where there are a high number of variables to consider, it is useful to visualise the correlation matrix. There are several packages available for the visualisation of a correlation matrix in R:\n GGally - extension to ggplot2  ggpairs() function  ggcorrplot - visualisation of a correlation matrix using ggplot2  ggcorrplot() function  corrplot - rich visualisation of a correlation matrix  corrplot.mixed() function  corrr - a tool for exploring correlations  network_plot() function  corrgram - calculates correlation of variables and displays the results graphically, based on the Corrgrams: Exploratory displays for correlation matrices paper (Friendly, 2002)  corrgram function  PerformanceAnalytics - package of econometric functions for performance and risk analysis of financial instruments or portfolios  chart.Correlation() function  psych - a toolbox for personality, psychometric theory and experimental psychology  `pairs.panels()   We will visualise the correlation matrix by using the corrplot package that offers many customization options that have been neatly presented in this short An Introduction to corrplot Package tutorial.\n# If you don\u0026#39;t have the \u0026quot;corrplot\u0026quot; package installed yet, uncomment and run the line below #install.packages(\u0026quot;corrplot\u0026quot;) suppressPackageStartupMessages(library(corrplot)) corrplot(cor(bodyfat), method = \u0026quot;ellipse\u0026quot;, order = \u0026quot;hclust\u0026quot;) For larger and more complex datasets, the construction of a correlogram has obvious advantages for exploratory purposes, because it shows all the correlations in an uncomplicated manner. For example, the information provided through this correlogram is easy to absorb as positive correlations are displayed in blue and negative correlations in red, with the correlation coefficients and the corresponding colours displayed in the legend. Colour intensity and the width of the ellipse are proportional to the correlation coefficients, making it altogether easier to read and understand.\nHere we see, among other things, that a) the majority of variables are positively correlated with one another and that b) age and height are the two variables with weak correlations to most other variables.\nWithout a doubt this is an example in which Stepwise regression and Best Subsets regression (BREG) procedures can be deployed as the effective tools in helping us to identify useful predictors.\n Best Subsets regression The leaps package enables the best subset selection through the application of the regsubsets() function. It identifies the best model that contains a given number of predictors, where best is quantified using residual sum of squares (RSS). The syntax is the same as for the lm() function and the summary() command outputs the best set of variables for each model size.\nThe regsubsets() function (part of the leaps library) performs the best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size. We will save the results of the call of the summary() function as breg_summary, which will allow us to access the parts we need to focus on.\n# # If you don\u0026#39;t have the \u0026quot;leaps\u0026quot; package installed yet, uncomment and run the line below #install.packages(\u0026quot;leaps\u0026quot;) library(leaps) breg_full = regsubsets(brozek ~ ., data = bodyfat) breg_summary = summary(breg_full) breg_summary ## Subset selection object ## Call: regsubsets.formula(brozek ~ ., data = bodyfat) ## 14 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## weight FALSE FALSE ## height FALSE FALSE ## adipos FALSE FALSE ## neck FALSE FALSE ## chest FALSE FALSE ## abdom FALSE FALSE ## hip FALSE FALSE ## thigh FALSE FALSE ## knee FALSE FALSE ## ankle FALSE FALSE ## biceps FALSE FALSE ## forearm FALSE FALSE ## wrist FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## age weight height adipos neck chest abdom hip thigh knee ankle biceps ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 4 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 5 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## forearm wrist ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; Note that the summary prints out the asterisks (‚Äú*‚Äù). The presence of a ‚Äú*‚Äù indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best three-variable model contains only weight, abd and wrist. Note that by default, regsubsets() only reports results up to the best eight-variable model. But the nvmax argument option can be used in order to return as many variables as are desired. As we would like to use all the available explanatory variables we will request a fit up to a 14-variable model, that is:\n\\[Y=f(X_1, X_2, X_3,..., X_{14}) \\]\nbreg_full \u0026lt;- regsubsets(brozek ~., data = bodyfat, nvmax = 14) breg_summary \u0026lt;- summary(breg_full) breg_summary ## Subset selection object ## Call: regsubsets.formula(brozek ~ ., data = bodyfat, nvmax = 14) ## 14 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## weight FALSE FALSE ## height FALSE FALSE ## adipos FALSE FALSE ## neck FALSE FALSE ## chest FALSE FALSE ## abdom FALSE FALSE ## hip FALSE FALSE ## thigh FALSE FALSE ## knee FALSE FALSE ## ankle FALSE FALSE ## biceps FALSE FALSE ## forearm FALSE FALSE ## wrist FALSE FALSE ## 1 subsets of each size up to 14 ## Selection Algorithm: exhaustive ## age weight height adipos neck chest abdom hip thigh knee ankle biceps ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 4 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 5 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 11 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 12 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 13 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 14 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## forearm wrist ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 11 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 12 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 13 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 14 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; üí°Remember that we can use the \\(\\bar{R}^2\\) to select the best model! We need to discover the other pieces of information contained in the breg_summary.\nnames(breg_summary) ## [1] \u0026quot;which\u0026quot; \u0026quot;rsq\u0026quot; \u0026quot;rss\u0026quot; \u0026quot;adjr2\u0026quot; \u0026quot;cp\u0026quot; \u0026quot;bic\u0026quot; \u0026quot;outmat\u0026quot; \u0026quot;obj\u0026quot; In addition to the output that indicates the inclusion of the variables in the given models we get when we print the summary in the console, the summary() function also returns the necessary statistics for the best model selection. It provides \\(R^2\\), \\(RSS\\), \\(\\bar{R}^2\\), \\(C_p\\), and \\(BIC\\), which will help us in the selection of the best overall model.\nKnowing that the \\(R^2\\) statistic increases monotonically as more variables are included, it will not be effective to use it in the model selection procedure. However, we are going to examine it to see the level to which it increases.\nt(t(sprintf(\u0026quot;%0.2f%%\u0026quot;, breg_summary$rsq * 100))) ## [,1] ## [1,] \u0026quot;66.21%\u0026quot; ## [2,] \u0026quot;71.87%\u0026quot; ## [3,] \u0026quot;72.76%\u0026quot; ## [4,] \u0026quot;73.51%\u0026quot; ## [5,] \u0026quot;73.80%\u0026quot; ## [6,] \u0026quot;74.10%\u0026quot; ## [7,] \u0026quot;74.45%\u0026quot; ## [8,] \u0026quot;74.67%\u0026quot; ## [9,] \u0026quot;74.77%\u0026quot; ## [10,] \u0026quot;74.84%\u0026quot; ## [11,] \u0026quot;74.89%\u0026quot; ## [12,] \u0026quot;74.90%\u0026quot; ## [13,] \u0026quot;74.90%\u0026quot; ## [14,] \u0026quot;74.90%\u0026quot; As expected, the \\(R^2\\) statistic increases monotonically from 66.21% when only one variable is included in the model to almost 75% with the inclusion of 12 or more variables.\nCreating the line plots to visualise \\(RSS\\), \\(\\bar{R}^2\\), \\(C_p\\), and \\(BIC\\), for all of the models at once will help us to decide which model to select. We will keep things simple and create the visualisations using the plot function in base R with argument option type = \"l\" onto which we will superimpose the corresponding minimal or maximal values using the point function in respect of the statistic displayed.\npar(mfrow = c(2,2)) # Set up a 2x2 grid for display of 4 plots at once plot(breg_summary$rss, xlab = \u0026quot;Number of Variables\u0026quot;, ylab = \u0026quot;RSS\u0026quot;, type = \u0026quot;l\u0026quot;) # line plot of adjusted R^2 statistic plot(breg_summary$adjr2, xlab = \u0026quot;Number of Variables\u0026quot;, ylab = \u0026quot;Adjusted RSq\u0026quot;, type = \u0026quot;l\u0026quot;) # identify the location of the maximum point adj_r2_max = which.max(breg_summary$adjr2) # plot a red point to indicate the model with the largest adjusted R^2 statistic points(adj_r2_max, breg_summary$adjr2[adj_r2_max], col =\u0026quot;red\u0026quot;, cex = 1, pch = 20) # line plot of C_p and BIC, but this time in a search of the models with the SMALLEST statistic # line plot of C_p statistic plot(breg_summary$cp, xlab = \u0026quot;Number of Variables\u0026quot;, ylab = \u0026quot;Cp\u0026quot;, type = \u0026quot;l\u0026quot;) cp_min = which.min(breg_summary$cp) points(cp_min, breg_summary$cp[cp_min], col = \u0026quot;red\u0026quot;, cex = 1, pch = 20) # line plot of BIC statistic plot(breg_summary$bic, xlab = \u0026quot;Number of Variables\u0026quot;, ylab = \u0026quot;BIC\u0026quot;, type = \u0026quot;l\u0026quot;) bic_min = which.min(breg_summary$bic) points(bic_min, breg_summary$bic[bic_min], col = \u0026quot;red\u0026quot;, cex = 1, pch = 20) We see that the measures are not in sync with one another and we realise that no one measure is going to give us an entirely accurate picture. According to \\(\\bar{R}^2\\) and \\(C_p\\) the best performing model is the one with 8 variables and according to \\(BIC\\) the best performing model has only 4 variables. This outcome suggests that the models with fewer than 4 predictors is insufficient, while a model with more than 8 predictors will overfit.\nThe regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the \\(R^2\\), \\(\\bar{R}^2\\), \\(C_p\\), and \\(BIC\\) statistic. The top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. That is, when creating this plot for the display of the \\(R^2\\) value it is not a surprise to see that the top row indicates inclusion of all 14 predictors for the \\(R^2\\) of 75%.\nplot(breg_full, scale=\u0026quot;r2\u0026quot;) We can now identify the suggested 8 predictors for the best model by observing the top row and checking which of the variables has a black square.\nIn the following plot by observing the top row and checking which of the variable has a black square we can identify the exact 8 predictors to be used for the best model as suggested by \\(\\bar{R}^2\\).\nplot(breg_full, scale =\u0026quot;adjr2\u0026quot;) It suggests that \\[brozek = f(age, weight, neck, abdom, hip, thigh, forearm, wrist)\\] We can use the coef() function to see the coefficient estimates associated with this model.\ncoef(breg_full, 8) ## (Intercept) age weight neck abdom hip ## -20.06213373 0.05921577 -0.08413521 -0.43189267 0.87720667 -0.18641032 ## thigh forearm wrist ## 0.28644340 0.48254563 -1.40486912 plot(breg_full, scale=\u0026quot;Cp\u0026quot;) \\(C_p\\) suggests the same set of predictors as \\(\\bar{R}^2\\).\nplot(breg_full, scale=\u0026quot;bic\u0026quot;) We see that two models share a BIC close to -310. However, the model with the lowest \\(BIC\\) is the four-variable model that contains only weight, abdom, forearm and wrist\ncoef(breg_full, 4) ## (Intercept) weight abdom forearm wrist ## -31.2967858 -0.1255654 0.9213725 0.4463824 -1.3917662  Forward and Backward Stepwise Selection To perform stepwise selection we will also use the regsubsets() function, but this time with the argument method set to either ‚Äúforward‚Äù or ‚Äúbackward‚Äù depending on which of the two stepwise selections we wish to perform.\n# Forward stepwise selection stepw_fwd \u0026lt;- regsubsets(brozek ~ ., data = bodyfat, nvmax = 14, method = \u0026quot;forward\u0026quot;) summary(stepw_fwd) ## Subset selection object ## Call: regsubsets.formula(brozek ~ ., data = bodyfat, nvmax = 14, method = \u0026quot;forward\u0026quot;) ## 14 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## weight FALSE FALSE ## height FALSE FALSE ## adipos FALSE FALSE ## neck FALSE FALSE ## chest FALSE FALSE ## abdom FALSE FALSE ## hip FALSE FALSE ## thigh FALSE FALSE ## knee FALSE FALSE ## ankle FALSE FALSE ## biceps FALSE FALSE ## forearm FALSE FALSE ## wrist FALSE FALSE ## 1 subsets of each size up to 14 ## Selection Algorithm: forward ## age weight height adipos neck chest abdom hip thigh knee ankle biceps ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 4 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 5 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 11 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 12 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 13 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 14 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## forearm wrist ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 11 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 12 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 13 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 14 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; stepw_bwd \u0026lt;- regsubsets(brozek ~ ., data = bodyfat, nvmax = 14, method = \u0026quot;backward\u0026quot;) summary(stepw_bwd) ## Subset selection object ## Call: regsubsets.formula(brozek ~ ., data = bodyfat, nvmax = 14, method = \u0026quot;backward\u0026quot;) ## 14 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## weight FALSE FALSE ## height FALSE FALSE ## adipos FALSE FALSE ## neck FALSE FALSE ## chest FALSE FALSE ## abdom FALSE FALSE ## hip FALSE FALSE ## thigh FALSE FALSE ## knee FALSE FALSE ## ankle FALSE FALSE ## biceps FALSE FALSE ## forearm FALSE FALSE ## wrist FALSE FALSE ## 1 subsets of each size up to 14 ## Selection Algorithm: backward ## age weight height adipos neck chest abdom hip thigh knee ankle biceps ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 4 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 11 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 12 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 13 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 14 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## forearm wrist ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 11 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 12 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 13 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 14 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; stepw_new \u0026lt;- regsubsets(brozek ~ ., data = bodyfat, nvmax = 14, method = \u0026quot;seqrep\u0026quot;) summary(stepw_new) ## Subset selection object ## Call: regsubsets.formula(brozek ~ ., data = bodyfat, nvmax = 14, method = \u0026quot;seqrep\u0026quot;) ## 14 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## weight FALSE FALSE ## height FALSE FALSE ## adipos FALSE FALSE ## neck FALSE FALSE ## chest FALSE FALSE ## abdom FALSE FALSE ## hip FALSE FALSE ## thigh FALSE FALSE ## knee FALSE FALSE ## ankle FALSE FALSE ## biceps FALSE FALSE ## forearm FALSE FALSE ## wrist FALSE FALSE ## 1 subsets of each size up to 14 ## Selection Algorithm: \u0026#39;sequential replacement\u0026#39; ## age weight height adipos neck chest abdom hip thigh knee ankle biceps ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 4 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 5 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 11 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; ## 12 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 13 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 14 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## forearm wrist ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 7 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 11 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 12 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 13 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; ## 14 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; We can see that the best one-variable through four-variable models are each identical for best subset and forward and backward selection. Difference occurs for the five-variable model for which the backward selection method selects different set of predictors from the other two.\ncoef(breg_full, 5) ## (Intercept) weight neck abdom forearm wrist ## -27.4118045 -0.1136977 -0.3382162 0.9325574 0.4964234 -1.1519642 coef(stepw_fwd, 5) ## (Intercept) weight neck abdom forearm wrist ## -27.4118045 -0.1136977 -0.3382162 0.9325574 0.4964234 -1.1519642 coef(stepw_bwd, 5) ## (Intercept) age weight abdom forearm wrist ## -27.89197896 0.03594628 -0.10392579 0.87162722 0.47755107 -1.67587889 Another difference in selection happens again for the six-factor model for which this time forward selection suggests a different set of predictors from the other two.\ncoef(breg_full, 6) ## (Intercept) age weight abdom thigh forearm ## -34.8393955 0.0566420 -0.1280528 0.8459061 0.2081457 0.4592612 ## wrist ## -1.6278694 coef(stepw_fwd, 6) ## (Intercept) age weight neck abdom forearm ## -23.21489668 0.04059830 -0.08818394 -0.36881380 0.87738614 0.53615286 ## wrist ## -1.45115127 coef(stepw_bwd, 6) ## (Intercept) age weight abdom thigh forearm ## -34.8393955 0.0566420 -0.1280528 0.8459061 0.2081457 0.4592612 ## wrist ## -1.6278694 For the models with seven predictors and more all three methods suggest the same variables.\n  The Validation Set Approach and Cross-Validation for the selection of the best model In this section validation set and cross-validation approaches will be used to chose the best model among a set of models of different sizes. When applying these approaches we split the given data into two subsets:\n training data: a subset to train a model test data: a subset to test the model  The training data is used for model estimation and variable selection, whilst the remaining test data is put aside and reserved for testing the accuracy of the model (see section on Machine Learning). That is, the best model variable selection is performed using only the training observations that are randomly selected from the original data.\nWe begin the application of validation approach in R by splitting the bodyaft data into a training set and a test set. First, we set a random seed to ensure the same data split each time we run the randomised process of splitting the bodyfat data into training set and test set. The set.seed() function sets the starting number used to generate a sequence of random numbers (it ensures that you get the same result if you start with that same seed each time you run the same process). Next, we create a random vector, train, of elements equal to TRUE if the corresponding observation is in the training set, and FALSE otherwise. A random vector test is obtained by using the ! command that creates TRUEs to be switched to FALSEs and vice versa. Once both train and test vectors are obtained we perform best subset on the training set of the bodyfat data using the now familiar regsubset() function.\nset.seed(111) train \u0026lt;- sample(c(TRUE, FALSE), nrow(bodyfat), rep = TRUE) test \u0026lt;- (!train ) breg \u0026lt;- regsubsets(brozek ~., data = bodyfat[train, ], nvmax = 14) We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data. The model.matrix() function is used in many regression packages for building an \\(X\\) matrix from data.\nIn the multiple regression setting because of the potentially large number of predictors, it is more convenient to write the expressions using matrix notation. To develop the analogy let us quickly look at the matrix notation for the 14-variable linear regression function:\n\\[y_i = \\beta_0 + \\beta_1x_1 + \\epsilon_i \\;\\;\\;\\;\\;\\; \\text{for} \\;\\; i=1, 2, ...n\\] If we actually let \\(i = 1, ..., n\\), we see that we obtain \\(n\\) equations: \\[y_i = \\beta_0 + \\beta_1x_1 + \\epsilon_1\\] \\[y_i = \\beta_0 + \\beta_1x_2 + \\epsilon_2\\] \\[ \\vdots\\] \\[y_i = \\beta_0 + \\beta_1x_n + \\epsilon_n\\] It is more efficient to use matrices to define the regression model which can be formulated for the above simple linear regression function in matrix notation as: \\[ \\begin{bmatrix} y_1\\\\y_2\\\\ \\vdots\\\\y_n \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_1 \\\\ 1 \u0026amp; x_2 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_n \\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_0\\\\\\beta_1\\\\ \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_1\\\\\\varepsilon_2\\\\ \\vdots\\\\\\varepsilon_n \\end{bmatrix} \\] Instead of writing out the \\(n\\) equations, using matrix notation, the simple linear regression function reduces to a short and simple statement: \\[\\mathbf{Y} = \\mathbf{X\\beta} + \\mathbf{\\varepsilon},\\] where matrices \\(Y\\), \\(\\mathbf{\\varepsilon}\\) are of the size \\(n \\times 1\\), \\(\\mathbf{\\beta}\\) of \\(2 \\times 1\\) and finally matrix \\(Y\\) of \\(n \\times 2\\). \nOnce we have created \\(X\\) matrix we run a loop, and for each \\(i\\), we\nextract the coefficients from breg for the best model of that size multiply them into the appropriate columns of the test model matrix to form the predictions and compute the test MSE  test.mat \u0026lt;- model.matrix(brozek ~., data = bodyfat[test, ]) val.errors \u0026lt;- rep(NA, 14) for(i in 1:14){ coefi \u0026lt;- coef(breg, id = i) pred \u0026lt;- test.mat[ ,names(coefi)] %*% coefi val.errors[i] \u0026lt;- mean((bodyfat$brozek[test] - pred)^2) } val.errors ## [1] 20.43699 16.90849 16.75413 16.20119 16.04250 17.82335 17.46721 17.64863 ## [9] 17.55764 17.06395 16.60744 17.00481 16.94781 16.84543 coef(breg, which.min(val.errors)) ## (Intercept) weight neck abdom forearm wrist ## -25.45466859 -0.09711483 -0.39312889 0.95767906 0.38200504 -1.25643570 It looks as if the best model is the one that contains five variables. As there is no predict() method for regsubsets() we can capture our steps above and write our own predict method.\npredict.regsubsets \u0026lt;- function(object, newdata, id,...){ form \u0026lt;- as.formula(object$call[[2]]) mat \u0026lt;- model.matrix(form, newdata) coefi \u0026lt;- coef(object, id = id) xvars \u0026lt;- names(coefi) mat[,xvars ] %*% coefi } The function almost does what we did above. The only tricky bit is the extraction of the formula used in the call to regsubsets(). We are going to use this function to do cross-validation. We start by performing best subset selection on the full data set, and select the best five-variable model. In order to get more accurate coefficient estimates we make use of the full data set, rather than using the variables that were obtained from the training set. We should realise that the best five-variable model on the full data set may differ from the corresponding five-variable model on the training set.\nbreg \u0026lt;- regsubsets(brozek ~., data = bodyfat , nvmax = 14) coef(breg, 5) ## (Intercept) weight neck abdom forearm wrist ## -27.4118045 -0.1136977 -0.3382162 0.9325574 0.4964234 -1.1519642 We are in luck as the best five-variable model on the full data set has the same set of variables as identified for the best five-variable model on the training set.\nWe now try to choose among the models of different sizes using cross-validation and perform best subset selection within each of the \\(k\\) training sets. This taxing task can be easily performed in R by creating a vector that allocates each observation to one of k = 10 folds and a matrix that will hold the results.\nk \u0026lt;- 10 set.seed(111) folds \u0026lt;- sample (1:k, nrow(bodyfat), replace = TRUE) cv.errors \u0026lt;- matrix(NA, k, 14, dimnames = list(NULL, paste (1:14))) Now we write a for loop that performs cross-validation:\nin the \\(j^{\\text{th}}\\) fold, the elements of folds that equal \\(j\\) are in the test set, and the remainder are in the training set. next,   make our predictions for each model size (using our new predict() method) compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors.  for(j in 1:k){ best.fit \u0026lt;- regsubsets(brozek ~., data = bodyfat[folds!=j, ], nvmax = 14) for(i in 1:14){ pred \u0026lt;- predict(best.fit, bodyfat[folds==j, ], id = i) cv.errors[j, i] \u0026lt;- mean((bodyfat$brozek[folds==j] - pred)^2) } } This creates a 10 \\(\\times\\) 14 matrix, where the \\((i, j)^{\\text{th}}\\) element represents the test MSE for the \\(i^{\\text{th}}\\) cross-validation fold for the best \\(j\\)-variable model. We use apply() to average over the columns of this matrix in order to obtain a vector for which the \\(j^{\\text{th}}\\) element is the cross-validation error for the \\(j\\)-variable model.\nmean.cv.errors \u0026lt;- apply(cv.errors, 2, mean) mean.cv.errors ## 1 2 3 4 5 6 7 8 ## 20.34108 17.51758 17.10110 17.13365 17.53639 17.85260 17.54328 17.54522 ## 9 10 11 12 13 14 ## 31.93117 35.00292 35.25575 34.60876 34.99476 34.96073 cv.errors ## 1 2 3 4 5 6 7 8 ## [1,] 12.32946 9.416819 9.903653 11.10810 12.19012 13.10063 13.37630 13.56035 ## [2,] 12.48594 13.771297 13.852726 12.72790 12.99102 13.28162 13.21003 13.36145 ## [3,] 15.55276 13.954025 14.715055 14.75803 15.87441 14.79482 15.16436 13.98156 ## [4,] 31.60131 23.661676 21.820352 20.87529 22.30423 22.44055 23.90745 22.11288 ## [5,] 21.10524 24.206510 21.984589 21.35164 22.34848 23.97423 20.04423 20.86420 ## [6,] 21.92108 16.523561 16.195050 15.11413 15.57349 15.48747 14.61914 14.04020 ## [7,] 19.97793 21.266331 20.512493 20.99681 20.69149 20.58539 20.26534 19.82136 ## [8,] 17.86496 12.694993 12.796383 13.62946 13.55600 13.63481 12.65788 12.28301 ## [9,] 15.83071 15.834488 14.922103 15.21815 14.95377 15.17270 14.62951 15.25427 ## [10,] 34.74140 23.846053 24.308575 25.55695 24.88093 26.05378 27.55856 30.17297 ## 9 10 11 12 13 14 ## [1,] 13.60245 13.58774 13.41241 13.46334 13.44878 13.45282 ## [2,] 14.11558 14.21637 13.99672 13.94406 14.02302 13.99533 ## [3,] 14.09255 13.99584 14.08496 14.16646 14.69319 14.91532 ## [4,] 22.23907 21.92254 22.40096 22.45297 22.49736 22.50527 ## [5,] 165.51126 197.37461 200.09727 193.90667 196.86868 196.97994 ## [6,] 13.87184 13.77324 13.69918 13.80556 13.97567 13.98346 ## [7,] 19.94554 20.10140 20.08304 20.15145 20.17206 20.19522 ## [8,] 12.32055 12.29925 12.06850 12.06478 12.04933 12.05878 ## [9,] 15.56205 15.55901 15.21091 15.30418 15.32023 15.32022 ## [10,] 28.05086 27.19924 27.50356 26.82817 26.89926 26.20099 par(mfrow=c(1, 1)) plot(mean.cv.errors, type=\u0026quot;b\u0026quot;) We see that cross-validation selects a three-variable model. We will use breg_full in order to obtain the three-variable model.\n#breg \u0026lt;- regsubsets(brozek ~., data = bodyfat, nvmax = 4) coef(breg_full, 3) ## (Intercept) weight abdom wrist ## -24.7612893 -0.1055832 0.9019083 -1.1456968  caret: Stepwise and Cross-Validation put together The caret package makes modelling in R easier. It combines two in one:\n automatically resamples the models while conducting parameter tuning  caret‚Äôs most affable feature is its consistent modelling syntax which enables you to build and compare models with very little splurge. By simply changing the method argument, you can easily investigate varying applicable models adopted from the pre-existing R packages. In total there are over two hundred different models available in caret.\nAnother sublime feature of caret lies in its train() function, which enables the use of the same function to run all of the competing models. It accepts several caret-specific arguments that provide capabilities for different resampling methods, performance measures, and algorithms for choosing the best model. Running the train() function will create a train object with estimated parameters for a selected model from a set of given data with many other useful results.\nWe will use caret to adopt 10-fold cross-validation, that will split bodyfat data into 10 approximately equal chunks to perform stepwise selections using the leapBackward method adopted from the leaps package. The model will be developed based on 9 chunks and predict the \\(\\text{$10^{th}$}\\) until all of the folds are treated as a validation set, while the model is fitted on the remaining 10-1 folds. caret takes care of setting up this resampling with the help of the trainControl() function and the trControl argument of its train() function. As the data set contains 14 predictors, we will vary nvmax from 1 to 14 resulting in the identification of the 14 best models with different sizes: the best 1-variable model, the best 2-variables model, and so on up to the best 14-variables model.\n#If you don\u0026#39;t have the \u0026quot;caret\u0026quot; package installed yet, uncomment and run the line below #install.packages(\u0026quot;caret\u0026quot;) # Train the model suppressPackageStartupMessages(library(caret)) set.seed(111) # Set up repeated k-fold cross-validation train.control \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;, number = 10) # Train the model stepwb \u0026lt;- train(brozek ~., data = bodyfat, method = \u0026quot;leapBackward\u0026quot;, tuneGrid = data.frame(nvmax = 1:14), trControl = train.control ) class(stepwb) ## [1] \u0026quot;train\u0026quot; \u0026quot;train.formula\u0026quot; stepwb$results ## nvmax RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 1 4.500901 0.6779062 3.667717 0.8055119 0.11240896 0.5675990 ## 2 2 4.260896 0.7022554 3.501954 0.7003423 0.11479575 0.6140745 ## 3 3 4.257837 0.7103567 3.493261 0.6797350 0.09896541 0.5393138 ## 4 4 4.173503 0.7218316 3.444086 0.6125089 0.09224353 0.5335659 ## 5 5 4.227939 0.7159505 3.511170 0.6146446 0.08503459 0.5306893 ## 6 6 4.186677 0.7200482 3.465637 0.6156029 0.08406015 0.5571481 ## 7 7 4.139519 0.7272454 3.387318 0.6905944 0.08203603 0.5906919 ## 8 8 4.130202 0.7290699 3.379290 0.6346024 0.07918020 0.5620786 ## 9 9 4.138647 0.7279039 3.397510 0.6191154 0.08006510 0.5422292 ## 10 10 4.151920 0.7245898 3.410971 0.6212871 0.08515454 0.5505494 ## 11 11 4.140745 0.7268756 3.396724 0.6400128 0.08463738 0.5612896 ## 12 12 4.183797 0.7214162 3.423895 0.6614864 0.08789564 0.5794326 ## 13 13 4.196041 0.7197241 3.434989 0.6747641 0.09008652 0.5887054 ## 14 14 4.199835 0.7192288 3.437822 0.6768053 0.09037830 0.5945132 The output above shows different metrics and their standard deviation for comparing the accuracy of the 14 best models for a different number of variables.\n nvmax: the number of variables; in the best model for the given number of variables RMSE and MAE are two different metrics measuring the prediction error of the model, implying that the lower their value, the better the model. Rsquared indicates the correlation between the observed outcome values and the values predicted by the model, meaning that the higher the Rsquared, the better the model.  Using the 10-fold cross-validation we have estimated average prediction error (RMSE) for each of the 14 best models for the given number of variables. The RMSE statistical metric is used to compare the 14 selected models and to automatically choose the model that minimises the RMSE as the best model. It can be seen that the model with 8 variables (nvmax = 8) is the one that has the lowest RMSE. We can call bestTune from the created stepwb object of class train to directly display the ‚Äúbest tuning values‚Äù (nvmax) and finalModel to see selected variables of the best model.\n# Final model coefficients stepwb$finalModel ## Subset selection object ## 14 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## weight FALSE FALSE ## height FALSE FALSE ## adipos FALSE FALSE ## neck FALSE FALSE ## chest FALSE FALSE ## abdom FALSE FALSE ## hip FALSE FALSE ## thigh FALSE FALSE ## knee FALSE FALSE ## ankle FALSE FALSE ## biceps FALSE FALSE ## forearm FALSE FALSE ## wrist FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: backward # Summary of the model stepwb$bestTune ## nvmax ## 8 8 summary(stepwb$finalModel) ## Subset selection object ## 14 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## weight FALSE FALSE ## height FALSE FALSE ## adipos FALSE FALSE ## neck FALSE FALSE ## chest FALSE FALSE ## abdom FALSE FALSE ## hip FALSE FALSE ## thigh FALSE FALSE ## knee FALSE FALSE ## ankle FALSE FALSE ## biceps FALSE FALSE ## forearm FALSE FALSE ## wrist FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: backward ## age weight height adipos neck chest abdom hip thigh knee ankle biceps ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 4 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## forearm wrist ## 1 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot; \u0026quot; \u0026quot;*\u0026quot; ## 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; üí°Typing ?caret::train inside R‚Äôs console will bring a more detailed explanation on how to use the train() function and information about the expected outcomes.\nSummary All of the above approaches have suggested different variable selection for the best fitted model. We have noticed by examining the correlogram high multicollinearity amongst the variables in the bodyfat. It reveals a weak correlation only between three to four variables. This is a frequently encountered problem in multiple regression analysis, where such an interrelationship among explanatory variables obscures their relationship with the response variable. This causes computational instability in model estimation resulting in the different suggestions of the best model for different variable selection approaches:\n Best Subsets Regression: has indicated that the models with a fewer than 4 predictors is insufficient, while a model with more than 8 predictors will overfit Stepwise Forward and Stepwise Backward selections: has suggested different sets of variables for the five-factor model and the six-factor model Validation Set: has recommended the five-factor model as the best one 10-Fold Cross-Validation: has argued the case for the three-factor model Backward Selection with automatic 10-Fold Cross-Validation adopted by the application of the caret package: identifies the eight-factor model as the best one  Our main goal was to derive a model to predict body fat (variable brozek) using all predictors except for siri, density and free from the fat data available from the faraway package. Guided by the outcomes of the above analysis and a desire to describe the behaviour of this multivariate data set parsimoniously (see Miller, 2002) we suggest the four-factor model for which \\(\\bar{R}^2\\) = 73.51%. Adding another 4 variables as suggested by Backward Selection with automatic 10-Fold Cross-Validation would increase \\(\\bar{R}^2\\) statistic by only 1.16%.\ncoef(breg_full, 4) ## (Intercept) weight abdom forearm wrist ## -31.2967858 -0.1255654 0.9213725 0.4463824 -1.3917662 \\[brozek = -31.2967858 - 0.1255654 \\; weight + 0.9213725 \\; abdom + 0.4463824 \\; forearm - 1.3917662 \\; wrist\\] Applying different subset selection approaches we have pruned a list of plausible explanatory variables down to a parsimonious collection of the ‚Äúmost useful‚Äù variables. We need to realise that subset selection approaches for multiple regression modelling should be used as a tool that can help us avoid the tiresome process of trying all possible combinations of explanatory variables, by testing variables one by one. However, as statistical models can serve different purposes we need to incorporate prior knowledge when it is possible. Solely reliance on subset selection approaches is wrong and it could be misleading (Smith, 2018).\nReference\nM. Friendly. Corrgrams: Exploratory displays for correlation matrices, The American Statistician 56(4): 316-324, 2002.\nG. James, D. Witten, T. Hastie, R. Tibshirani. Introduction to Statistical Learning with Applications in R, Springer Texts in Statistics, 2013.\nM. Kuhn, K. Johnson. ‚ÄúApplied Predictive Modeling‚Äù, Springer, 2013.\nM. Kuhn. The caret Package, 2019.\nA. J. Miller. Subset Selection in Regression. Monographs on Statistics and Applied Probability, Chapman \u0026amp; Hall/CRC, 2\\(^\\text{nd}\\) edition, 2002.\nG. Smith. Step away from stepwise. Journal of Big Data, pp.¬†5-32. 2018\n  ","date":1614038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614038400,"objectID":"bba3a40125a0f8f547c74ef320f6381c","permalink":"/post/breg_cv/","publishdate":"2021-02-23T00:00:00Z","relpermalink":"/post/breg_cv/","section":"post","summary":"This short tutorial on Subset variable Selection in R comes from pp.¬†244-251 of ‚ÄúIntroduction to Statistical Learning with Applications in R‚Äù by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani and chapter ‚ÄúForward, Backward, and Stepwise Selection‚Äù, from pp.","tags":["multifactor regression","best subset","stepwise regression","cross-validation"],"title":"Subset Variable Selection in R","type":"post"},{"authors":null,"categories":null,"content":"         Open data belongs to everyone; it empowers people to make informed decisions that are not clouded by misinformation, rumour and gossip. To be able to identify the underlying facts within data sets, it is crucial that individuals and communities possess the necessary skills. Open data is often inconsistent and limited and requires a significant amount of time to organise and structure for presentation purposes.\nThis is a data analysis report concerning the visualisation of the COVID-19 virus within the United Kingdom and Europe. All of the report is created with R. It represents a case study as an illustration of the concepts presented at the workshop on basic R for data analysts. To learn how to use R and develop a report like this visit the Introduction To R website.\nCovid-19: Open Data The novel coronavirus disease 2019 (COVID-19) was first reported in Wuhan, China, where the initial wave of intense community transmissions was cut short by interventions.\n\u0026gt; # If you don\u0026#39;t have the \u0026quot;leaflet\u0026quot; package installed yet, uncomment and run the line below \u0026gt; #install.packages(\u0026quot;leaflet\u0026quot;) \u0026gt; library(leaflet) \u0026gt; # Initialize and assign as the leaflet object \u0026gt; leaflet() %\u0026gt;% + # add tiles to the leaflet object + addTiles() %\u0026gt;% + # setting the centre of the map and the zoom level + setView(lng = 114.3055, lat = 30.5928 , zoom = 10) %\u0026gt;% + # add a popup marker + addMarkers(lng = 114.3055, lat = 30.5928, popup = \u0026quot;\u0026lt;b\u0026gt;Wuhan, capital of Central China‚Äôs Hubei province\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;a href=\u0026#39;https://www.ft.com/content/82574e3d-1633-48ad-8afb-71ebb3fe3dee\u0026#39;\u0026gt;China and Covid-19: what went wrong in Wuhan?\u0026lt;/a\u0026gt;\u0026quot;)  {\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"\u0026copy; OpenStreetMap contributors, CC-BY-SA\"}]},{\"method\":\"addMarkers\",\"args\":[30.5928,114.3055,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Wuhan, capital of Central China‚Äôs Hubei province\nChina and Covid-19: what went wrong in Wuhan?\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"setView\":[[30.5928,114.3055],10,[]],\"limits\":{\"lat\":[30.5928,30.5928],\"lng\":[114.3055,114.3055]}},\"evals\":[],\"jsHooks\":[]} Experience has shown that for public health intervention to be successful governmental authorities need to:\n identify the infected and provide treatment locate and quarantine all those who had contact with the infected sterilise environmental pathogens promote the use of masks and social distancing release to the public the number of new infections and deaths on a daily basis through open data  The importance of hand washing, wearing a mask and social distancing as a tool to limit disease transmission is well recognised, but nonetheless ensuring social distancing especially in densely populated urban areas is still challenging. Well-educated communities are critical for an effective response and for the prevention of local outbreaks. The sharing of factual information that can be understood and trusted by the communities is vital. This will lead to a change in their behaviour to implement efficiently desired public health actions is a must. Trust and transparency are fundamental in obtaining absolute public engagement. The publishing of daily figures that can be freely analysed and scrutinised can help in engaging communities and obtaining its willing and continued support in controlling the spread of infection.\nWorking with a DB in R In big organisations data is often kept in a database and data you wish to access from it might be too large to fit into the memory of your computer. Connecting from R to a database to access the necessary data for an analysis can be very useful, as it allows you to fetch only the chunks needed for the current study. R enables you to access and query a database without having to download the data contained in it. The two most common methods of connection are:\n Option 1)  the RODBC package: uses slightly older code; it can be used to connect to anything that uses ODBC.   \u0026gt; library(\u0026quot;RODBC\u0026quot;) \u0026gt; # Connection with a server called \u0026quot;Walmart\u0026quot; and a database called \u0026quot;Asda\u0026quot;: \u0026gt; RODBC_connection \u0026lt;- odbcDriverConnect(\u0026#39;driver = {SQL Server}; + server = Walmart; + database = Asda; + trusted_connection = true\u0026#39;) #passes your windows credentials to the server; can also specify a username `uid` and a password `pwd` \u0026gt; dt1 \u0026lt;- sqlFetch(channel = RODBC_connection, sqtable = \u0026quot;MyTable\u0026quot;) Using RODBC you can write back to database tables, choosing to append or not:\n\u0026gt; sqlSave(channel = RODBC_connection, + dat = dt2, + tablename = \u0026quot;MyTable_R_version\u0026quot;, + append = FALSE, + safer = FALSE) When you finish working using the database you should disconnect from the server.\n\u0026gt; odbcClose(RODBC_connection) One of the authors of this package is the famous statistician Brian Ripley, and you can find more about the possibilities it offers by playing around with it using the guidance from RDocumentation on RODBC v1.3-17.\n Option 2)  the DBI package: a common database interface in R; can be used with different ‚Äòback-end‚Äô drivers such as MySQL, SQL Server, SQLite, Oracle etc; to write SQL it can be used on its own:   \u0026gt; # Can write an SQL query directly using the `dbSendQuery` function \u0026gt; # Executes the query on the server-side only, but if you want the results back in R, you need to use `dbFetch` \u0026gt; SomeRecords \u0026lt;- dbFetch(dbSendQuery(DBI_Connection, + \u0026quot;SELECT CustomerName_column, City_column FROM Customers_Table\u0026quot;))  You can also write back to a database using the dbWriteTable function.\n\u0026gt; #Writing a new table called \u0026#39;Table_created_in_R\u0026#39; using the R data.frame called \u0026quot;my_df\u0026quot;, with `append` and `overwrite` options \u0026gt; dbWriteTable(DBI_Connection,\u0026quot;Table_created_in_R\u0026quot;, my_df, overwrite = TRUE) We use tbl() to define a table as if it were part of the R work-space, and to specify as the function‚Äôs arguments the connection object and the name of the table in the database.\n\u0026gt; MyTable_Rws \u0026lt;- tbl(DBI_Connection, \u0026quot;MyTable_DB\u0026quot;) If we need to pull the data from the server into R‚Äôs memory we can use the collect() function.\nThe DBI package can be combined with:\n the dplyr package: to make the tbls and to work on them using dplyr syntax the dbplyr package: allows translation of SQL to dplyr the odbcpackage: provides the odbc drivers, but you could use the functions below with other drivers instead  \u0026gt; DBI_Connection \u0026lt;- dbConnect(odbc(), + driver = \u0026quot;SQL Server\u0026quot;, + server = Sys.getenv(\u0026quot;SERVER\u0026quot;), + database = Sys.getenv(\u0026quot;DATABASE\u0026quot;) + ) With the wave of tidy verse evangelists the second option has become more popular as it allows us to convert SQL into R using the dplyr commands chained with the pipe (%\u0026gt;%) operator. dplyr can translate many different query types into SQL. We can use it to do fairly complex queries without translation in just a few lines and obtain the results even though the data is still in the database.\nUseful DBI commands\n  Command Summary    dbConnect() Create a DBI connection object  dbListTables() List the tables on the connection  dbListFields() List the fields for a given table on a given connection  dbSendQuery() Send a query to execute on the server/connection  dbFetch() Fetch the results from the server/connection  dbWriteTable() Write a table to the connection  tbl() Set a table on the connection as a tibble for dplyr  glimpse() See a summary of the rows, data types and top rows     Reading Data European Centre for Disease Prevention and Control provides daily updates of newly reported cases of COVID-19 by country worldwide. The downloadable data file is updated daily and contains the latest available public data on COVID-19. Each row/entry contains the number of new cases reported per country and per day (with a lag of a day).\nhttps://www.ecdc.europa.eu/en/publications-data/download-data-response-measures-covid-19\nhttps://www.ecdc.europa.eu/en/cases-2019-ncov-eueea\nWe will start the analysis by uploading the necessary packages and data into R. If you have not got the packages used in the following code, you will need to uncomment the first line (delete the # symbol) in the code below.\n\u0026gt; #install.packages(c(\u0026quot;dplyr\u0026quot;, \u0026quot;stringr\u0026quot;)) # install multiple packages by passing a vector of package names to the function; this function will install the requested packages, along with any of their non-optional dependencies \u0026gt; suppressPackageStartupMessages(library(readxl)) \u0026gt; suppressPackageStartupMessages(library(tidyverse)) \u0026gt; suppressPackageStartupMessages(library(httr)) \u0026gt; suppressPackageStartupMessages(library(lubridate)) \u0026gt; suppressPackageStartupMessages(library(dplyr)) \u0026gt; suppressPackageStartupMessages(library(plotly)) \u0026gt; suppressPackageStartupMessages(library(ggplot2)) \u0026gt; suppressPackageStartupMessages(library(cowplot)) \u0026gt; suppressPackageStartupMessages(library(scales)) \u0026gt; suppressPackageStartupMessages(library(sf)) \u0026gt; suppressPackageStartupMessages(library(DBI)) \u0026gt; suppressPackageStartupMessages(library(dbplyr)) \u0026gt; suppressPackageStartupMessages(library(tmap)) \u0026gt; suppressPackageStartupMessages(library(tmaptools)) \u0026gt; \u0026gt; \u0026gt; url2ecdc \u0026lt;- \u0026quot;https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide-2020-11-30.xlsx\u0026quot; \u0026gt; \u0026gt; suppressMessages(GET(url2ecdc, write_disk(tf \u0026lt;- tempfile(fileext = \u0026quot;.xlsx\u0026quot;)))) ## Response [https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide-2020-11-30.xlsx] ## Date: 2021-02-24 17:27 ## Status: 200 ## Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet ## Size: 3.41 MB ## \u0026lt;ON DISK\u0026gt; /var/folders/71/96w85flx3yl928r2hzpfwvd00000gp/T//RtmpN0XKYH/file66be7333587b.xlsx \u0026gt; covid_world \u0026lt;- read_excel(tf) We will set up the database connection to work on covid_world data.\n\u0026gt; SQLcon \u0026lt;- dbConnect(RSQLite::SQLite(), \u0026quot;:memory:\u0026quot;) \u0026gt; dbWriteTable(SQLcon, \u0026quot;covid\u0026quot;, covid_world, overwrite=TRUE) Let‚Äôs see what tables we have in our database.\n\u0026gt; dbListTables(SQLcon) ## [1] \u0026quot;covid\u0026quot; We can list the fields in a table:\n\u0026gt; dbListFields(SQLcon, name = \u0026quot;covid\u0026quot;) ## [1] \u0026quot;dateRep\u0026quot; ## [2] \u0026quot;day\u0026quot; ## [3] \u0026quot;month\u0026quot; ## [4] \u0026quot;year\u0026quot; ## [5] \u0026quot;cases\u0026quot; ## [6] \u0026quot;deaths\u0026quot; ## [7] \u0026quot;countriesAndTerritories\u0026quot; ## [8] \u0026quot;geoId\u0026quot; ## [9] \u0026quot;countryterritoryCode\u0026quot; ## [10] \u0026quot;popData2019\u0026quot; ## [11] \u0026quot;continentExp\u0026quot; ## [12] \u0026quot;Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\u0026quot; We can run an SQL query directly from R. To illustrate it we will run a query to obtain distinct values for the field ‚ÄúcontinentExp‚Äù.\n\u0026gt; dbFetch(dbSendQuery(SQLcon, \u0026quot;Select distinct continentExp from covid\u0026quot;)) ## continentExp ## 1 Asia ## 2 Europe ## 3 Africa ## 4 America ## 5 Oceania ## 6 Other Next, we will run a query to count how many entries we have for each continent\n\u0026gt; dbFetch( + dbSendQuery(SQLcon, + \u0026quot;Select continentExp, count(*) as Count + from covid + group by continentExp\u0026quot;)) ## continentExp Count ## 1 Africa 14196 ## 2 America 13056 ## 3 Asia 12653 ## 4 Europe 16601 ## 5 Oceania 2332 ## 6 Other 64 and see how many entries there are for the UK, using a where clause.\n\u0026gt; dbFetch( + dbSendQuery(SQLcon, + \u0026quot;Select continentExp, count(*) as Count + from covid + Where countriesAndTerritories = \u0026#39;United_Kingdom\u0026#39; + group by continentExp\u0026quot;)) ## continentExp Count ## 1 Europe 336 Fortunately, for people whose SQL is rusty, and who don‚Äôt feel like learning both SQL and R, we can do all of this using the diplyr package in R.\nFirst we need to declare covid as a tbl for use with dplyr. We‚Äôll call it covid_ecdc to avoid any confusion.\n\u0026gt; covid_ecdc \u0026lt;- tbl(SQLcon, \u0026quot;covid\u0026quot;) This will now be treated as an R tibble, but it is still in the database!!!\nIt is always useful to have a quick glance at the data set structure to find out how the information it contains is structured. Knowledge of the structure is important, because it allows us later to filter out the desired information very precisely, based on criteria to limit specific dimensions, i.e.¬†variables.\n\u0026gt; covid_ecdc %\u0026gt;% + glimpse() ## Rows: ?? ## Columns: 12 ## Database: sqlite 3.33.0 [:memory:] ## $ dateRep \u0026lt;dbl\u0026gt; 16066944‚Ä¶ ## $ day \u0026lt;dbl\u0026gt; 30, 29, ‚Ä¶ ## $ month \u0026lt;dbl\u0026gt; 11, 11, ‚Ä¶ ## $ year \u0026lt;dbl\u0026gt; 2020, 20‚Ä¶ ## $ cases \u0026lt;dbl\u0026gt; 0, 228, ‚Ä¶ ## $ deaths \u0026lt;dbl\u0026gt; 0, 11, 1‚Ä¶ ## $ countriesAndTerritories \u0026lt;chr\u0026gt; \u0026quot;Afghani‚Ä¶ ## $ geoId \u0026lt;chr\u0026gt; \u0026quot;AF\u0026quot;, \u0026quot;A‚Ä¶ ## $ countryterritoryCode \u0026lt;chr\u0026gt; \u0026quot;AFG\u0026quot;, \u0026quot;‚Ä¶ ## $ popData2019 \u0026lt;dbl\u0026gt; 38041757‚Ä¶ ## $ continentExp \u0026lt;chr\u0026gt; \u0026quot;Asia\u0026quot;, ‚Ä¶ ## $ `Cumulative_number_for_14_days_of_COVID-19_cases_per_100000` \u0026lt;dbl\u0026gt; 6.416633‚Ä¶ Before we go any further and start the analysis of covid data we will replicate the above queries using the dplyr functions. First, to illustrate how easy it is to do the column selection with dplyr we‚Äôll select countriesAndTerritories and continentExpfrom ourcovid_ecdc` data.\n\u0026gt; head(covid_ecdc %\u0026gt;% + select(countriesAndTerritories, continentExp)) # returns first six rows of the vector, i.e. tibble ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.33.0 [:memory:] ## countriesAndTerritories continentExp ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan Asia ## 2 Afghanistan Asia ## 3 Afghanistan Asia ## 4 Afghanistan Asia ## 5 Afghanistan Asia ## 6 Afghanistan Asia The First query was the count of entries for each continent.\n\u0026gt; covid_ecdc %\u0026gt;% + group_by(continentExp) %\u0026gt;% + tally() ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.33.0 [:memory:] ## continentExp n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Africa 14196 ## 2 America 13056 ## 3 Asia 12653 ## 4 Europe 16601 ## 5 Oceania 2332 ## 6 Other 64 The second was to look for the number of entries for the UK.\n\u0026gt; covid_ecdc %\u0026gt;% + filter(countriesAndTerritories == \u0026quot;United_Kingdom\u0026quot;) %\u0026gt;% + tally() ## # Source: lazy query [?? x 1] ## # Database: sqlite 3.33.0 [:memory:] ## n ## \u0026lt;int\u0026gt; ## 1 336 If you do not have to manipulate and do the engineering work with DBs, but you need to access it to obtain data for the analysis, you might find it easier to do it all using the dplyr. It is intuitive and therefore easier to write. You cannot deny that dplyr‚Äôs version also looks neater.\nNext, we‚Äôll check the total number of readings for each country and present it in a table using the DT package. DT provides an R interface to the JavaScript library DataTables, which will enable us to filter through the displayed data.\n\u0026gt; if (!require(\u0026quot;DT\u0026quot;)) install.packages(\u0026#39;DT\u0026#39;) # returns a logical value say, FALSE if the requested package is not found and TRUE if the package is loaded \u0026gt; tt \u0026lt;- covid_ecdc %\u0026gt;% + group_by(countriesAndTerritories) %\u0026gt;% + summarise(no_readings = n()) %\u0026gt;% + arrange(no_readings) \u0026gt; \u0026gt; mdt \u0026lt;- DT::datatable(data.frame(tt)) \u0026gt; widgetframe::frameWidget(mdt)  {\"x\":{\"url\":\"/post/Covid_UK/index_files/figure-html//widgets/widget_covid_ecdc.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]}  Tidying Data We will focus our analysis on European countries and select them from our covid_world data, saving it all as covid_eu data frame object as this data needs tidying and wrangling and we do not want to limit ourselves by using only dplyr functions in R.\n\u0026gt; covid_eu \u0026lt;- rbind(covid_world %\u0026gt;% filter(continentExp == \u0026quot;Europe\u0026quot;), + covid_world %\u0026gt;% filter(countriesAndTerritories == \u0026quot;Turkey\u0026quot;)) \u0026gt; \u0026gt; mdt \u0026lt;- DT::datatable(covid_eu) \u0026gt; widgetframe::frameWidget(mdt)  {\"x\":{\"url\":\"/post/Covid_UK/index_files/figure-html//widgets/widget_ecdc_data.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} You can, however, try to pull the data from the server into R‚Äôs memory and by using dplyr functions do the required manipulations.\n\u0026gt; covid_eu \u0026lt;- covid_ecdc %\u0026gt;% + filter(continentExp == \u0026quot;Europe\u0026quot;) %\u0026gt;% + collect() \u0026gt; \u0026gt; mdt \u0026lt;- DT::datatable(covid_eu) \u0026gt; widgetframe::frameWidget(mdt) The experience with COVID-19 shows that the spread of the disease can be controlled by implementing the measures of prevention as soon as an outbreak has been detected.\n To monitor the effectiveness of the measures introduced, we will focus on daily cumulative cases of COVID-19 infection that can be expressed as \\[F(x) = \\sum_{i=1}^{n} x_i\\]\n  Although \\(F(x)\\) can show the volume of the epidemic, it does not tell us directly about the changes in the acceleration of the spread of infection. This information can be provided by the derivatives of the \\(F(x)\\). The first derivative \\(F^{‚Äô}(x)\\) corresponds to the information on the number of new cases detected every day. The second derivative \\(F^{‚Äô‚Äô}(x)\\) provides the information about the acceleration of the epidemic. \\(F^{‚Äô‚Äô}(x) \\approx 0\\) indicates the state of stagnation, while \\(F^{‚Äô‚Äô}(x) \u0026lt; 0\\) indicates deceleration and of course any \\(F^{‚Äô‚Äô}(x) \u0026gt; 0\\) acceleration.\n We will carry on with the analysis by tidying covid_eu data and adding the information about those derivatives. First, we notice that there are 4 columns used to contain the date of reporting, which will allow us to remove columns 2-4 as redundant. We will only keep the dateRep column which requires some tidying up in respect of the format in which the dates are recorded. We will also carry out the necessary calculations for obtaining the second derivative \\(F^{‚Äô‚Äô}(x)\\) and rename some of the variables to make them easier to display and type. üòÅ All of this is very easy to carry out in R using the tidy verse, opinionated collection of R packages for data science.\n\u0026gt; # --- tidy data --- \u0026gt; glimpse(covid_eu) ## Rows: 16,863 ## Columns: 12 ## $ dateRep \u0026lt;dttm\u0026gt; 2020-11‚Ä¶ ## $ day \u0026lt;dbl\u0026gt; 30, 29, ‚Ä¶ ## $ month \u0026lt;dbl\u0026gt; 11, 11, ‚Ä¶ ## $ year \u0026lt;dbl\u0026gt; 2020, 20‚Ä¶ ## $ cases \u0026lt;dbl\u0026gt; 835, 545‚Ä¶ ## $ deaths \u0026lt;dbl\u0026gt; 11, 16, ‚Ä¶ ## $ countriesAndTerritories \u0026lt;chr\u0026gt; \u0026quot;Albania‚Ä¶ ## $ geoId \u0026lt;chr\u0026gt; \u0026quot;AL\u0026quot;, \u0026quot;A‚Ä¶ ## $ countryterritoryCode \u0026lt;chr\u0026gt; \u0026quot;ALB\u0026quot;, \u0026quot;‚Ä¶ ## $ popData2019 \u0026lt;dbl\u0026gt; 2862427,‚Ä¶ ## $ continentExp \u0026lt;chr\u0026gt; \u0026quot;Europe\u0026quot;‚Ä¶ ## $ `Cumulative_number_for_14_days_of_COVID-19_cases_per_100000` \u0026lt;dbl\u0026gt; 342.1921‚Ä¶ \u0026gt; #covid_eu \u0026lt;- covid_eu[, -c(2:4)] # remove redundant information \u0026gt; covid_eu \u0026lt;- covid_eu %\u0026gt;% + separate(dateRep, c(\u0026quot;dateRep\u0026quot;), sep = \u0026quot;T\u0026quot;) %\u0026gt;% + group_by(countriesAndTerritories) %\u0026gt;% + arrange(dateRep) %\u0026gt;% + mutate(total_cases = cumsum(cases), + total_deaths = cumsum(deaths)) %\u0026gt;% + mutate(Diff_cases = total_cases - lag(total_cases), # 1st derivative (same as cases) + Rate_pc_cases = round(Diff_cases/lag(total_cases) * 100, 2)) %\u0026gt;% # rate of change + mutate(second_der = Diff_cases - lag(Diff_cases)) %\u0026gt;% # 2nd derivative + rename(country = countriesAndTerritories) %\u0026gt;% + rename(country_code = countryterritoryCode) %\u0026gt;% + rename(Fx14dper100K = \u0026quot;Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\u0026quot;) %\u0026gt;% + mutate(Fx14dper100K = round(Fx14dper100K)) \u0026gt; \u0026gt; covid_eu$dateRep \u0026lt;- as.Date(covid_eu$dateRep) \u0026gt; head(covid_eu) # returns first six rows of the df ## # A tibble: 6 x 17 ## # Groups: country [6] ## dateRep day month year cases deaths country geoId country_code ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2019-12-31 31 12 2019 0 0 Armenia AM ARM ## 2 2019-12-31 31 12 2019 0 0 Austria AT AUT ## 3 2019-12-31 31 12 2019 0 0 Azerba‚Ä¶ AZ AZE ## 4 2019-12-31 31 12 2019 0 0 Belarus BY BLR ## 5 2019-12-31 31 12 2019 0 0 Belgium BE BEL ## 6 2019-12-31 31 12 2019 0 0 Croatia HR HRV ## # ‚Ä¶ with 8 more variables: popData2019 \u0026lt;dbl\u0026gt;, continentExp \u0026lt;chr\u0026gt;, ## # Fx14dper100K \u0026lt;dbl\u0026gt;, total_cases \u0026lt;dbl\u0026gt;, total_deaths \u0026lt;dbl\u0026gt;, ## # Diff_cases \u0026lt;dbl\u0026gt;, Rate_pc_cases \u0026lt;dbl\u0026gt;, second_der \u0026lt;dbl\u0026gt;   Writing Functions As we would like to be able to plot this time series of the second derivatives \\(F^{‚Äô‚Äô}(x)\\) for any country, we will create functions that will allow us to extract a country from the covid_eu data and to plot it as a time series.\n\u0026gt; # function for filltering a country from the given df \u0026gt; sep_country \u0026lt;- function(df, ccode){ + df_c \u0026lt;- df %\u0026gt;% + filter(country_code == as.character(ccode)) + return(df_c) + } \u0026gt; \u0026gt; # plotting the 2nd derivative \u0026gt; sec_der_plot \u0026lt;- function(df){ + df %\u0026gt;% + filter(!is.na(second_der)) %\u0026gt;% + ggplot(aes(x = dateRep, y = second_der)) + + geom_line() + geom_point(col = \u0026quot;#00688B\u0026quot;) + + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + + labs (title = \u0026quot;2nd derivative of F(x)\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu\u0026quot;) + + theme_minimal() + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5), + panel.grid.major.x = element_blank(), + panel.grid.minor.x = element_blank()) + }  Data Visualisation Once we have accessed and tidied up our data in R we can carry out the exploitative analysis using visualisation as an effective tool.\nplotly and ggplot We will start reporting by illustrating the time series of the daily number of new cases of infection and deaths. To make this plot more informative, we will create it as an interactive web graphic using the plotly library. You can explore different kinds of plotly graphs in R from https://plotly.com/r/basic-charts/ or by reading the Step-by-Step Data Visualization Guideline with Plotly in R blog post.\necdc data updated on 2020-11-30\n\u0026gt; # Plot cases and deaths day-by-day \u0026gt; \u0026gt; covid_uk \u0026lt;- sep_country(covid_eu, \u0026quot;GBR\u0026quot;) \u0026gt; \u0026gt; x \u0026lt;- list(title = \u0026quot;date reported\u0026quot;) \u0026gt; \u0026gt; fig \u0026lt;- plot_ly(covid_uk, x = ~ dateRep) \u0026gt; fig \u0026lt;- fig %\u0026gt;% add_trace(y = ~cases, name = \u0026#39;cases\u0026#39;, type = \u0026#39;scatter\u0026#39;, mode = \u0026#39;lines\u0026#39;) \u0026gt; fig \u0026lt;- fig %\u0026gt;% add_trace(y = ~deaths, name = \u0026#39;deaths\u0026#39;, type = \u0026#39;scatter\u0026#39;, mode = \u0026#39;lines\u0026#39;) \u0026gt; fig \u0026lt;- fig %\u0026gt;% layout(xaxis = x) \u0026gt; fig  The plots below illustrate dynamic changes based on the \\(F(x)\\) created using the ggplot2 package. What we would like to see is the flattening of the bars indicating the slowdown in the number of new Covid-19 cases.\n\u0026gt; covid_uk %\u0026gt;% + ggplot(aes(x = dateRep, y = total_cases)) + + geom_bar(stat=\u0026quot;identity\u0026quot;, fill = \u0026quot;#00688B\u0026quot;) + + labs (title = \u0026quot;Cumulative number of cases F(x)\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu\u0026quot;, + x = \u0026quot;Date\u0026quot;, y = \u0026quot;number of cases\u0026quot;) + + theme_minimal() + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5), + panel.grid.major.x = element_blank(), + panel.grid.minor.x = element_blank()) + + theme(legend.position=\u0026quot;none\u0026quot;)  We will present the same information this time using the line plot integrating interactivity, displaying the information by using the ggplotly() function.\n\u0026gt; pl1 \u0026lt;- covid_uk %\u0026gt;% + ggplot(aes(x = dateRep, y = total_cases)) + + geom_line() + geom_point(col = \u0026quot;#00688B\u0026quot;) + + xlab(\u0026quot;Date\u0026quot;) + ylab(\u0026quot;Number of Cases\u0026quot;) + + labs (title = \u0026quot;F(x)\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu\u0026quot;) + + theme_minimal() + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5), + panel.grid.major.x = element_blank(), + panel.grid.minor.x = element_blank()) \u0026gt; pl1 The following graph presents the cumulative number of covid-19 cases using a logarithmic scale to emphasise the rate of change in a way that a linear scale does not.\n\u0026gt; pl_log \u0026lt;- covid_uk %\u0026gt;% + mutate(log_total_cases = log(total_cases)) %\u0026gt;% + ggplot(aes(x = dateRep, y = log_total_cases)) + + geom_line() + geom_point(col = \u0026quot;#00688B\u0026quot;) + + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + + labs (title = \u0026quot;F(x) on log scale\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu\u0026quot;) + + theme_minimal() + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5), + panel.grid.major.x = element_blank(), + panel.grid.minor.x = element_blank()) \u0026gt; pl_log Sometimes it might be useful to present several plots next to each other. To do this in R we apply the plot_grid() function from the cowplot package.\n\u0026gt; plot_grid(pl1, pl_log) Next we will illustrate the cumulative number of cases for all selected European countries\n\u0026gt; all_plot \u0026lt;- covid_eu %\u0026gt;% + filter(country_code %in% c(\u0026quot;GBR\u0026quot;, \u0026quot;FRA\u0026quot;, \u0026quot;DEU\u0026quot;, \u0026quot;ITA\u0026quot;, \u0026quot;ESP\u0026quot;, \u0026quot;SWE\u0026quot;)) %\u0026gt;% + filter(dateRep \u0026gt; (max(dateRep) - 21)) %\u0026gt;% + ggplot(aes(x = dateRep, y = total_cases, colour = country_code)) + + geom_line() + + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + + labs (title = \u0026quot;F(x) in the last three weeks\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu\u0026quot;) + + theme_minimal() + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5), + panel.grid.major.x = element_blank(), + panel.grid.minor.x = element_blank()) + + scale_x_date(labels = date_format(\u0026quot;%m-%d\u0026quot;), + breaks = \u0026#39;day\u0026#39;) + + scale_colour_brewer(palette = \u0026quot;Set1\u0026quot;) + + theme_classic() + + theme(legend.position = \u0026quot;bottom\u0026quot;) + + theme(axis.text.x = element_text(angle = 90))  \u0026gt; ggplotly(all_plot)  Again, this would be easier to compare using the log scale\n\u0026gt; covid_eu %\u0026gt;% + filter(country_code %in% c(\u0026quot;GBR\u0026quot;, \u0026quot;FRA\u0026quot;, \u0026quot;DEU\u0026quot;, \u0026quot;ITA\u0026quot;, \u0026quot;ESP\u0026quot;, \u0026quot;SWE\u0026quot;)) %\u0026gt;% + filter(dateRep \u0026gt; (max(dateRep) - 21)) %\u0026gt;% + mutate(log_total_cases = log(total_cases)) %\u0026gt;% + ggplot(aes(x = dateRep, y = log_total_cases, colour = country_code)) + + geom_line() + + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + + labs (title = \u0026quot;logF(x) in the last three weeks\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu\u0026quot;) + + theme_minimal() + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5), + panel.grid.major.x = element_blank(), + panel.grid.minor.x = element_blank()) + + scale_x_date(labels = date_format(\u0026quot;%m-%d\u0026quot;), + breaks = \u0026#39;day\u0026#39;) + + scale_colour_brewer(palette = \u0026quot;Set1\u0026quot;) + + theme_classic() + + theme(legend.position = \u0026quot;bottom\u0026quot;) + + theme(axis.text.x = element_text(angle = 45))  The following plot enables us to observe the change in the acceleration in relation to the governmental measures.\n\u0026gt; covid_uk %\u0026gt;% + filter(!is.na(second_der)) %\u0026gt;% + ggplot(aes(x = dateRep, y = second_der)) + + geom_line() + geom_point(col = \u0026quot;#00688B\u0026quot;) + + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + + labs (title = \u0026quot;2nd derivative of F(x) for the UK\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu\u0026quot;) + + theme_minimal() + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5), + panel.grid.major.x = element_blank(), + panel.grid.minor.x = element_blank()) + + geom_vline(xintercept = as.numeric(as.Date(\u0026quot;2020-03-23\u0026quot;)), linetype = 3, colour = \u0026quot;red\u0026quot;, alpha = 0.5) + + geom_vline(xintercept = as.numeric(as.Date(\u0026quot;2020-05-10\u0026quot;)), linetype = 3, colour = \u0026quot;dodgerblue4\u0026quot;, alpha = 0.5) + + geom_vline(xintercept = as.numeric(as.Date(\u0026quot;2020-07-04\u0026quot;)), linetype = 3, colour = \u0026quot;chartreuse4\u0026quot;, alpha = 0.5) + + geom_vline(xintercept = as.numeric(as.Date(\u0026quot;2020-11-05\u0026quot;)), linetype = 3, colour = \u0026quot;red\u0026quot;, alpha = 0.5) + + annotate(geom=\u0026quot;text\u0026quot;, x=as.Date(\u0026quot;2020-03-23\u0026quot;), y = 8000, + label=\u0026quot;UK wide lockdown\u0026quot;, col = \u0026quot;red\u0026quot;) + + annotate(geom=\u0026quot;text\u0026quot;, x=as.Date(\u0026quot;2020-05-21\u0026quot;), y = 5000, + label=\u0026quot;lockdown lifting plan\u0026quot;, col = \u0026quot;dodgerblue4\u0026quot;) + + annotate(geom=\u0026quot;text\u0026quot;, x=as.Date(\u0026quot;2020-07-04\u0026quot;), y = -5000, + label=\u0026quot;wide-ranging changes\u0026quot; , col = \u0026quot;chartreuse4\u0026quot;) + + annotate(geom=\u0026quot;text\u0026quot;, x=as.Date(\u0026quot;2020-11-05\u0026quot;), y = 8000, + label=\u0026quot;UK wide lockdown\u0026quot;, col = \u0026quot;red\u0026quot;)  Let us see how these figures compare with other countries in particular France and Germany.\nFrance: \\(F^{\u0026#39;\u0026#39;}(x)\\)\n\u0026gt; covid_fr \u0026lt;- sep_country(covid_eu, \u0026quot;FRA\u0026quot;) \u0026gt; sec_der_plot(covid_fr) Germany: \\(F^{\u0026#39;\u0026#39;}(x)\\)\n\u0026gt; covid_de \u0026lt;- sep_country(covid_eu, \u0026quot;DEU\u0026quot;) \u0026gt; sec_der_plot(covid_de) Next, we are going to visualise a comparison between these three countries of the total number of deaths month by month.\n\u0026gt; covid_eu %\u0026gt;% + filter(country %in% c(\u0026quot;United_Kingdom\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;France\u0026quot;)) %\u0026gt;% + mutate(mon = month(dateRep, label = TRUE, abbr = TRUE)) %\u0026gt;% + group_by(country, mon) %\u0026gt;% + summarise(no_readings = n(), tdeath = max(total_deaths)) %\u0026gt;% + ggplot(aes(x = mon, y = tdeath, fill = country)) + + geom_bar(stat=\u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;, color = \u0026quot;black\u0026quot;) + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5)) + + labs (title = \u0026quot;total number of deaths by month\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu/en\u0026quot;, + x = \u0026quot;month\u0026quot;, y = \u0026quot;number of deaths\u0026quot;) + + scale_fill_brewer(palette=\u0026quot;Paired\u0026quot;) + + theme(legend.position=\u0026quot;bottom\u0026quot;)  We can make the same kind of comparisons for the total number of infections. But, before we just copy/paste and make an adjustment for y access, note that the order of the months does not show in the timeline of covid events. The recording of information about the spread of the pandemic started last December, which means that the bars should be in order from December to November. We are also going to flip the bars to see if it will add to its readability.\n\u0026gt; covid_eu %\u0026gt;% + filter(country %in% c(\u0026quot;United_Kingdom\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;France\u0026quot;)) %\u0026gt;% + mutate(mon = month(dateRep, label = TRUE, abbr = TRUE)) %\u0026gt;% + mutate(mon = factor(mon, levels=c(\u0026quot;Dec\u0026quot;, \u0026quot;Jan\u0026quot;, \u0026quot;Feb\u0026quot;, \u0026quot;Mar\u0026quot;, \u0026quot;Apr\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;Jun\u0026quot;, \u0026quot;Jul\u0026quot;, \u0026quot;Aug\u0026quot;, \u0026quot;Sep\u0026quot;, \u0026quot;Oct\u0026quot;, \u0026quot;Nov\u0026quot;))) %\u0026gt;% + group_by(country, mon) %\u0026gt;% + summarise(no_readings = n(), tcases = max(total_cases)) %\u0026gt;% + ggplot(aes(x = mon, y = tcases, fill = country)) + + geom_bar(stat=\u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;, color = \u0026quot;black\u0026quot;) + + coord_flip() + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5)) + + labs (title = \u0026quot;total number of infections by month\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu/en\u0026quot;, + x = \u0026quot;month\u0026quot;, y = \u0026quot;number of infections\u0026quot;) + + scale_fill_brewer(palette=\u0026quot;Set1\u0026quot;) + + theme(legend.position=\u0026quot;bottom\u0026quot;)  We can present the total number of infections for each month. As the numbers are high we are going to ‚Äúcontrol‚Äù the way the values on the y access are going to appear.\n\u0026gt; covid_eu %\u0026gt;% + filter(country %in% c(\u0026quot;United_Kingdom\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;France\u0026quot;)) %\u0026gt;% + mutate(mon = month(dateRep, label = TRUE, abbr = TRUE)) %\u0026gt;% + mutate(mon = factor(mon, levels=c(\u0026quot;Dec\u0026quot;, \u0026quot;Jan\u0026quot;, \u0026quot;Feb\u0026quot;, \u0026quot;Mar\u0026quot;, \u0026quot;Apr\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;Jun\u0026quot;, \u0026quot;Jul\u0026quot;, \u0026quot;Aug\u0026quot;, \u0026quot;Sep\u0026quot;, \u0026quot;Oct\u0026quot;, \u0026quot;Nov\u0026quot;))) %\u0026gt;% + group_by(country, mon) %\u0026gt;% + summarise(month_cases = sum(cases)) %\u0026gt;% + ggplot(aes(x = mon, y = month_cases, fill = country)) + + geom_bar(stat=\u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;, color = \u0026quot;black\u0026quot;) + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5)) + + scale_y_continuous(breaks = seq(0, 800000, 200000), labels = c(\u0026quot;0\u0026quot;, \u0026quot;200K\u0026quot;, \u0026quot;400K\u0026quot;, \u0026quot;600K\u0026quot;, \u0026quot;800K\u0026quot;)) + + labs (title = \u0026quot;total number of infections each month\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu/en\u0026quot;, + x = \u0026quot;month\u0026quot;, y = \u0026quot;number of deaths\u0026quot;) + + scale_fill_brewer(palette=\u0026quot;Dark2\u0026quot;) + + theme(legend.position=\u0026quot;bottom\u0026quot;)  Next, we are going to present the total number of deaths for each of the months since the recording began. Note, that in the code there is a line that is currently set as a comment that allows for the values to appear as texts on the top of the bars. You can go ahead and uncomment this line by removing the hashtag symbol in front of it.\n\u0026gt; covid_eu %\u0026gt;% + filter(country %in% c(\u0026quot;United_Kingdom\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;France\u0026quot;)) %\u0026gt;% + mutate(mon = month(dateRep, label = TRUE, abbr = TRUE)) %\u0026gt;% + mutate(mon = factor(mon, levels=c(\u0026quot;Dec\u0026quot;, \u0026quot;Jan\u0026quot;, \u0026quot;Feb\u0026quot;, \u0026quot;Mar\u0026quot;, \u0026quot;Apr\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;Jun\u0026quot;, \u0026quot;Jul\u0026quot;, \u0026quot;Aug\u0026quot;, \u0026quot;Sep\u0026quot;, \u0026quot;Oct\u0026quot;, \u0026quot;Nov\u0026quot;))) %\u0026gt;% + group_by(country, mon) %\u0026gt;% + summarise(month_deaths = sum(deaths)) %\u0026gt;% + ggplot(aes(x = mon, y = month_deaths, fill = country)) + + geom_bar(stat=\u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;, color = \u0026quot;black\u0026quot;) + + theme(plot.title = element_text(size = 14, vjust = 2, hjust=0.5)) + + # geom_text(aes(label = month_cases), size = 3, hjust = 0.5) + + labs (title = \u0026quot;total number of deaths each month\u0026quot;, + caption = \u0026quot;Data from: https://www.ecdc.europa.eu/en\u0026quot;, + x = \u0026quot;month\u0026quot;, y = \u0026quot;number of cases\u0026quot;) + + scale_fill_brewer(palette=\u0026quot;Accent\u0026quot;) + + theme(legend.position=\u0026quot;bottom\u0026quot;)   Spatial Visualisation We will create a choropleth in which we will colour the EU countries according to the most current value of cumulative numbers for 14 days of COVID-19 cases per 100000. To do this we will use the shape file onto which we will superimpose this value as a colour of the polygon, i.e.¬†country.\n\u0026gt; #points to the shape file \u0026gt; bound \u0026lt;- \u0026quot;shapes/eu_countries_simplified.shp\u0026quot; \u0026gt; \u0026gt; #used the st_read() function to import it \u0026gt; bound \u0026lt;- st_read(bound) ## Reading layer `eu_countries_simplified\u0026#39; from data source `/Users/Tanja1/Documents/My_R/mws/content/post/Covid_UK/shapes/eu_countries_simplified.shp\u0026#39; using driver `ESRI Shapefile\u0026#39; ## replacing null geometries with empty geometries ## Simple feature collection with 54 features and 4 fields (with 4 geometries empty) ## geometry type: GEOMETRY ## dimension: XY ## bbox: xmin: -10.48 ymin: 34.5675 xmax: 44.81861 ymax: 71.13312 ## CRS: 4326 \u0026gt; # plot the shape file \u0026gt; ggplot(bound) + + geom_sf() \u0026gt; covid_EU \u0026lt;- covid_eu %\u0026gt;% + filter(dateRep == max(dateRep)) \u0026gt; \u0026gt; \u0026gt; # tidy up \u0026gt; # make the country names correspond to ecdc data \u0026gt; bound$country \u0026lt;- gsub(\u0026quot; \u0026quot;, \u0026quot;_\u0026quot;, bound$country) \u0026gt; bound \u0026lt;- bound %\u0026gt;% + mutate(country = fct_recode(country, + \u0026quot;Czechia\u0026quot; = \u0026quot;Czech_Republic\u0026quot;, + \u0026quot;North_Macedonia\u0026quot; = \u0026quot;Macedonia\u0026quot;)) \u0026gt; \u0026gt; # join data from the two data frames \u0026gt; my_map \u0026lt;- left_join(bound, covid_EU, + by = c(\u0026quot;country\u0026quot; = \u0026quot;country\u0026quot;)) \u0026gt; \u0026gt; ggplot(my_map) + + geom_sf(aes(fill = Fx14dper100K)) + + scale_fill_distiller(direction = 1, name = \u0026quot;Fx14per100K\u0026quot;) + + labs(title=\u0026quot;Cumulative number for 14 days of COVID-19 cases per 100000\u0026quot;, caption=\u0026quot;Source: ecdc\u0026quot;) \u0026gt; mdt \u0026lt;- DT::datatable(my_map) \u0026gt; widgetframe::frameWidget(mdt)  {\"x\":{\"url\":\"/post/Covid_UK/index_files/figure-html//widgets/widget_unnamed-chunk-35.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} With the tmap package, thematic maps can be generated with great effectiveness presenting several layers of information. The syntax is similar to the one adopted in ggplot. Motivation and the explanation of this package has been proposed and published in the article tmap: Thematic Maps in R.\nIf you are interested in learning more about creating maps in R check the online version of the book Geocomputation with R. Chapter 8: The Making maps with R provides an easy to follow overview on using the tmap and other packages for creating beautiful maps in R.\n\u0026gt; my_map \u0026lt;- my_map %\u0026gt;% + mutate(ln_deaths = log(deaths)^10) \u0026gt; \u0026gt; #tmap_mode(mode = \u0026quot;view\u0026quot;) \u0026gt; \u0026gt; tm_shape(my_map) + + tm_polygons(\u0026quot;Fx14dper100K\u0026quot;, + id = \u0026quot;country\u0026quot;, + palette = \u0026quot;YlGn\u0026quot;, + popup.vars=c(\u0026quot;cases\u0026quot;, + \u0026quot;deaths\u0026quot;)) + + tm_layout(title = \u0026quot;Covid-19 EU\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;data source: \u0026lt;a href=\u0026#39;https://www.ecdc.europa.eu/en/covid-19-pandemic\u0026#39;\u0026gt;ECDC\u0026lt;/a\u0026gt;\u0026quot;, + frame = FALSE, + inner.margins = c(0.1, 0.1, 0.05, 0.05))   Lastly, we should not forget to disconnect from the database.\ndbDisconnect(SQLcon)   Useful Links  Databases using R\n SQL databases and R\n Book: Interactive web-based data visualization with R, plotly, and shiny\n   Working with a DB in R section is an adaptation of the NHS-R Community Database Connections in R webinar created and run by Chris Mainey.  ","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606694400,"objectID":"a1f7f99f7be7767dfb4c7c887d06fecb","permalink":"/post/covid_uk/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/post/covid_uk/","section":"post","summary":"Open data belongs to everyone; it empowers people to make informed decisions that are not clouded by misinformation, rumour and gossip. To be able to identify the underlying facts within data sets, it is crucial that individuals and communities possess the necessary skills.","tags":["SQL","dbplyr","wrangling","visualisation","COVID-19"],"title":"Covid UK: How to do it in R","type":"post"},{"authors":null,"categories":null,"content":" Thematic maps are geographical maps in which spatial data distributions are visualised. Earlier this year I contributed to the CINS‚Äô article about donations to local political parties for the period between 2012-2018. We created an interactive thematic map, choropleth, shading the development group of the town and adding two more layers of information, i.e.¬†attributes: the total amount of money donated (size and the colour of the bubble) and the amount of money received by each of the political parties (popup menu)\nüîéPogledajte odakle dolaze donatori @sns_srbija @socijalisti i @demokrate i ko je prikupio najvi≈°e para od donacija u periodu od sedam godina üí∏üëáhttps://t.co/2HHXrSLSLC ‚Äî CINS (@CINSerbia) May 25, 2020   To create an interactive map like this in R you will need the tmap package. In order to provide the workflow to create thematic maps, additionally you need a set of tools for reading and processing spatial data, available through the tmaptools. I will illustrate how easy it is to create a thematic map using the tmap package, but you can learn more about this package from the tmap: get started! website.\nR has impressive geographic capabilities and can handle different kinds of spatial data file formats including geojson and KML. In this example we will make use of the ESRI Shapefile format, which stores non-topological geometry and attribute information for the spatial features in a data set. A shapefile consists minimally of a main file, an index file, and a dBASE table.\n .shp - lists shape and vertices .shx - has index with offsets .dbf - relationship file between geometry and attributes (data)  To import an ESRI shapefile into R correctly, all three files must be present in the directory and named the same (except for the file extension).\nTo reproduce this example you will need to download spatial files and data that is visualised on the map. You can download this data bundle from here. These shape files with Serbian districts‚Äô boundaries are obtained from GADM maps and data.\nWe will start by uploading the required packages and read simple features from our spatial files as indicated in the code below.\n## If you don\u0026#39;t have ggplot2, dplyr, sf, sp, tm, tmaptools installed yet, uncomment and run the line below #install.packages(\u0026quot;ggplot2\u0026quot;, \u0026quot;dplyr\u0026quot;, \u0026quot;sf\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;readxl\u0026quot;, tm\u0026quot;, \u0026quot;tmaptools\u0026quot;) library(ggplot2) library(dplyr) library(sf) library(sp) library(readxl) library(tmap) library(tmaptools) #pointed to the shape file serbia_location1 \u0026lt;- \u0026quot;spatial/gadm36_SRB_1.shp\u0026quot; #used the st_read() function to import it serbia_districts1 \u0026lt;- st_read(serbia_location1) The visualisation needs to\n indicate the development group for each Serbian municipality display donations to local political parties for the period between 2012-2018 for each municipality  Belgrade city has several municipalities, but donations to local political parties for the period between 2012-2018 is given for the whole of Belgrade. This requires removing the boundary for the whole of the Belgrade district and combining it with the boundaries for the rest of the Serbian municipalities.\nIn order to filter the necessary geometry for the Belgrade district and to combine it with the sf data for Serbian municipalities we will use tidyverse methods for sf objects. Another issue we need to address is that data also contains information about Kosovo which needs to be included on the map. We will read shape files for Serbian and Kosovo municipalities and combine them into one.\n# filter out geometry of Belgrade\u0026#39;s district serbia_districts1 \u0026lt;- serbia_districts1 %\u0026gt;% filter(as.character(VARNAME_1) == \u0026quot;Belgrade\u0026quot;) # pointed to the shape files for municipalities (Serbia and Kosovo) serbia_location \u0026lt;- \u0026quot;gadm36_SRB_2.shp\u0026quot; kosovo_location \u0026lt;- \u0026quot;gadm36_XKO_2.shp\u0026quot; #used the st_read() function to import them serbia_districts \u0026lt;- st_read(serbia_location) kosovo_districts \u0026lt;- st_read(kosovo_location) # replace geometries of Belgrade\u0026#39;s municipalities with Belgrade district BG \u0026lt;- serbia_districts %\u0026gt;% filter(NAME_2 == \u0026quot;Stari Grad\u0026quot;) BG$NAME_2 \u0026lt;- \u0026quot;Beograd\u0026quot; BG$NL_NAME_2 \u0026lt;- BG$NL_NAME_1 BG$geometry \u0026lt;- serbia_districts1$geometry serbia_districts_BG \u0026lt;- serbia_districts %\u0026gt;% filter(NAME_1 != \u0026quot;Grad Beograd\u0026quot;) serbia_districts_BG1 \u0026lt;- rbind(BG, serbia_districts_BG) # combine sf data of serbia and Kosovo sr \u0026lt;- rbind(serbia_districts_BG1, kosovo_districts) In the next step we read data to be mapped given in the Excel file and make sure that the names of the municipalities correspond to the names in the sf file in the old fashioned way üòÅ.\n# read excel file razvijenost \u0026lt;- read_excel(\u0026quot;razvijenost.xlsx\u0026quot;, sheet = 1) ki \u0026lt;- serbia_districts$NL_NAME_2[serbia_districts$NAME_2 == \u0026quot;Kikinda\u0026quot;] gm \u0026lt;- serbia_districts$NL_NAME_2[serbia_districts$NAME_2 == \u0026quot;Gornji Milanovac\u0026quot;] ar \u0026lt;- serbia_districts$NL_NAME_2[serbia_districts$NAME_2 == \u0026quot;Arilje\u0026quot;] pe \u0026lt;- serbia_districts$NL_NAME_2[serbia_districts$NAME_2 == \u0026quot;Petrovac\u0026quot;] dm \u0026lt;- serbia_districts$NL_NAME_2[serbia_districts$NAME_2 == \u0026quot;Dimitrovgrad\u0026quot;] # ---- pr \u0026lt;- kosovo_districts$NL_NAME_2[kosovo_districts$NAME_2 == \u0026quot;Pri≈°tina\u0026quot;] ur \u0026lt;- kosovo_districts$NL_NAME_2[kosovo_districts$NAME_2 == \u0026quot;Uro≈°evac\u0026quot;] razvijenost$Town[razvijenost$Town == \u0026quot;–ö–∏–∫–∏–Ω–¥–∞\u0026quot;] \u0026lt;- ki razvijenost$Town[razvijenost$Town ==\u0026quot;–ì–æ—Ä—ö–∏ –ú–∏–ª–∞–Ω–æ–≤–∞—Ü\u0026quot;] \u0026lt;- gm razvijenost$Town[razvijenost$Town ==\u0026quot;–ê—Ä–∏—ô–µ\u0026quot;] \u0026lt;- ar razvijenost$Town[razvijenost$Town ==\u0026quot;–ü–µ—Ç—Ä–æ–≤–∞—Ü –Ω–∞ –ú–ª–∞–≤–∏\u0026quot;] \u0026lt;- pe razvijenost$Town[razvijenost$Town ==\u0026quot;–î–∏–º–∏—Ç—Ä–æ–≤–≥—Ä–∞–¥\u0026quot;] \u0026lt;- dm # ---- razvijenost$Town[razvijenost$Town ==\u0026quot;–ü—Ä–∏—à—Ç–∏–Ω–∞\u0026quot;] \u0026lt;- pr razvijenost$Town[razvijenost$Town ==\u0026quot;–£—Ä–æ—à–µ–≤–∞—Ü\u0026quot;] \u0026lt;- ur # merge data: sf and excel my_map \u0026lt;- left_join(sr, razvijenost, by=c(\u0026quot;NL_NAME_2\u0026quot; = \u0026quot;Town\u0026quot;)) Once data is organised it can be mapped using tmap. To make it aesthetically more appealing we will rescale the values for total donations so that the bubbles that are going to be used in its visualisation are not too small nor too big. Another matter that needs to be addressed is that the polygon of Negotin‚Äôs municipality has an extra border that needs to be removed before we can map data.\n# ==================== # ----- Mapping ------ library(tmap) library(tmaptools) # scaling `donation` my_map \u0026lt;- my_map %\u0026gt;% mutate(ln_donation = log(donation)^6) # Pull out the geometry for Negotin bad_geo = my_map %\u0026gt;% filter(NAME_2 == \u0026quot;Negotin\u0026quot;) %\u0026gt;% pull(geometry) # Keep only the first of the two polygon borders good_geo = bad_geo[[1]][1] # Replace old geometry with fixed my_map$geometry[which(my_map$NAME_2 == \u0026quot;Negotin\u0026quot;)] \u0026lt;- st_multipolygon(good_geo) # set tmap mode to interactive viewing tmap_mode(mode = \u0026quot;view\u0026quot;) # shade municipalities according to the group development and superimpose bubble which size and colour shade correspond to the total level of donation. Enable pop-up information about individual party donations (show it using Latin alphabet rather than Cyrillic). tm_shape(my_map) + tm_polygons(\u0026quot;development_group\u0026quot;, id = \u0026quot;NAME_2\u0026quot;, palette = \u0026quot;YlGn\u0026quot;, title = \u0026quot;Grupa razvijenosti\u0026quot;, textNA = \u0026quot;nema podataka\u0026quot;, popup.vars=c(\u0026quot;DS\u0026quot;=\u0026quot;–î–°\u0026quot;, \u0026quot;SNS\u0026quot;=\u0026quot;–°–ù–°\u0026quot;, \u0026quot;SPS\u0026quot;=\u0026quot;–°–ü–°\u0026quot;, \u0026quot;Ukupno (RSD)\u0026quot; = \u0026quot;donation\u0026quot;)) + tm_bubbles(size = \u0026quot;ln_donation\u0026quot;, col = \u0026quot;donation\u0026quot;, border.col = \u0026quot;black\u0026quot;, border.alpha = 1, style = \u0026quot;fixed\u0026quot;, breaks=c(0 , 1000000, 10000000, 50000000, 100000000, 200000000, 500000000), palette = \u0026quot;PuRd\u0026quot;, contrast = 1, title.col = \u0026quot;Ukupno donirano u RSD\u0026quot;, id = \u0026quot;NAME_2\u0026quot;, popup.vars=c(\u0026quot;DS\u0026quot;=\u0026quot;–î–°\u0026quot;, \u0026quot;SNS\u0026quot;=\u0026quot;–°–ù–°\u0026quot;, \u0026quot;SPS\u0026quot;=\u0026quot;–°–ü–°\u0026quot;, \u0026quot;Ukupno (RSD)\u0026quot; = \u0026quot;donation\u0026quot;)) + tm_layout(legend.title.size = .5, legend.text.size = .65, legend.frame = TRUE, legend.format = list(fun = function(x) { formatC(x, digits = 0, big.mark = \u0026quot;,\u0026quot;, format = \u0026quot;f\u0026quot;) })) + tm_layout(title = \u0026quot;Prilozi graƒëana za period 2012-2018\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;izvor podataka: \u0026lt;a href=\u0026#39;http://www.acas.rs/\u0026#39;\u0026gt;ACAS\u0026lt;/a\u0026gt; i \u0026lt;a href=\u0026#39;ras.gov.rs\u0026#39;\u0026gt;RAS\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;¬© \u0026lt;a href=\u0026#39;https://www.cins.rs/\u0026#39;\u0026gt;CINS\u0026lt;/a\u0026gt;, maj 2020\u0026quot;, frame = FALSE, inner.margins = c(0.1, 0.1, 0.05, 0.05))   This is information rich visualisation. It incorporates an interactive thematic map, choropleth, shading the development group of the town and adding two more layers of information, i.e.¬†attributes: the total amount of money donated (size and the colour of the bubble) and the amount of money received by each of the political parties (popup menu).\nIncorporating interactive visualisation with clearly presented facts, helped to empower readers by informing them in an effective manner.\nThis was an excellent example of effective data journalism, employed as a direct result of CINS embracing the capacity of the RToolbox.\n","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"6f8d97042a80b89659bf6787e0963c4a","permalink":"/post/cins_map/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/post/cins_map/","section":"post","summary":"Thematic maps are geographical maps in which spatial data distributions are visualised. Earlier this year I contributed to the CINS‚Äô article about donations to local political parties for the period between 2012-2018.","tags":["visualisation","gis","maps"],"title":"Thematic Maps","type":"post"},{"authors":null,"categories":null,"content":" There is no a definitive way for carrying out data analysis. It rather depends on the study domain and the insights you wish to extract from the data that you can collect.\nThis post is an ongoing covid-19 data journey that illustrates the behaviour of Covid-19 pandemic within the Republic of Serbia and neighbouring ex-YU countries. All of the report is created with R and is presented on its‚Äô designated website \u0026lt; https://covid19sr.rbind.io/\u0026gt; built with üíô using the blogdown and Air HUGO theme.\n","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586476800,"objectID":"27d1dd6baaec121ce5d5727005cbed9f","permalink":"/post/covid_sr/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/post/covid_sr/","section":"post","summary":"There is no a definitive way for carrying out data analysis. It rather depends on the study domain and the insights you wish to extract from the data that you can collect.","tags":["visualisation","COVID-19"],"title":"Covid SR: data visualisation","type":"post"},{"authors":null,"categories":null,"content":"       R Markdown For patients critically ill with COVID-19, access to a ventilator could be a matter of life or death. In early March at the beginning of the COVID-19 pandemic the Serbian government was not disclosing information about the available number of ventilators to its citizens. CINS‚Äô journalists took the initiative to obtain this information and to make it available to the public in an article concerning respiratory equipment in the current crisis. Shortly after, the government took more serious action towards protecting its citizens against the virus.\nAs a data scientist I was pleased to assist CINS‚Äô journalists in making interactive maps informing readers of their findings.\nThis article can be found at https://www.cins.rs/en/coronavirus-see-how-many-medical-ventilators-there-are-in-your-town/\nTo create an interactive map like this in R you will need the leaflet package. Leaflet is one of the most popular open-source JavaScript libraries for interactive maps that was integrated into R, by a team of people from RStudio. We will illustrate how easy it is to plot a location map using the leaflet package, but you can learn more about this package from the Leaflet for R website.\nHere is what we did.\nlibrary(leaflet) suppressPackageStartupMessages(library(dplyr)) suppressPackageStartupMessages(library(tidyverse)) mydata \u0026lt;- read.csv(\u0026#39;Gradovi.csv\u0026#39;, header=T, na.strings=c(\u0026quot;\u0026quot;,\u0026quot;NA\u0026quot;), stringsAsFactor = FALSE) glimpse(mydata) ## Rows: 69 ## Columns: 7 ## $ Mesta \u0026lt;chr\u0026gt; \u0026quot;Baƒçka Topola\u0026quot;, \u0026quot;Subotica\u0026quot;, \u0026quot;Kikinda\u0026quot;, \u0026quot;Senta\u0026quot;, \u0026quot;Zrenja‚Ä¶ ## $ Latitude \u0026lt;dbl\u0026gt; 45.81442, 46.10055, 45.82728, 45.92601, 45.38156, 45.77‚Ä¶ ## $ Longitude \u0026lt;dbl\u0026gt; 19.63796, 19.66506, 20.46152, 20.07809, 20.36857, 19.11‚Ä¶ ## $ Koristi.se \u0026lt;int\u0026gt; 1, 19, 6, 6, 13, 14, 3, 1, 4, 14, 98, 48, 1, 1, 0, 16, ‚Ä¶ ## $ Ne.koristi.se \u0026lt;int\u0026gt; 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 5, 8, 0, 0, 1, 0, 0, 0, 0‚Ä¶ ## $ Nepoznato \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶ ## $ Ukupno \u0026lt;int\u0026gt; 1, 19, 6, 7, 13, 14, 3, 1, 6, 14, 103, 56, 1, 1, 1, 16,‚Ä¶ minlat \u0026lt;- min(mydata$Latitude) maxlat \u0026lt;- max(mydata$Latitude) minlng \u0026lt;- min(mydata$Longitude) maxlng \u0026lt;- max(mydata$Longitude) mydata %\u0026gt;% leaflet() %\u0026gt;% addTiles() %\u0026gt;% fitBounds(~minlng, ~minlat, ~maxlng, ~maxlat) %\u0026gt;% addCircles(lng = ~Longitude, lat = ~Latitude, radius = 750, weight = 7, color = \u0026quot;black\u0026quot;, fillColor = \u0026quot;red\u0026quot;, fillOpacity = 0.7, popup = ~paste(\u0026quot;\u0026lt;b\u0026gt;\u0026quot;, Mesta, \u0026quot;\u0026lt;/b\u0026gt;\u0026quot;, \u0026quot;\u0026lt;br\u0026gt;Koristi se = \u0026quot;, Koristi.se,\u0026quot;\u0026lt;br\u0026gt;Ne koristi se = \u0026quot;, Ne.koristi.se, \u0026quot;\u0026lt;br\u0026gt; Nepoznato =\u0026quot;, Nepoznato, \u0026quot;\u0026lt;br\u0026gt;Total =\u0026quot;, Ukupno, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;))  {\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"\u0026copy; OpenStreetMap contributors, CC-BY-SA\"}]},{\"method\":\"addCircles\",\"args\":[[45.8144238,46.1005467,45.8272842,45.9260128,45.3815612,45.773295,44.9018213,45.1071651,45.1181926,44.874,45.2671352,45.2208633,45.6138162,45.415745,45.2497275,45.5701534,45.0485391,45.00806,45.1287251,45.1011851,44.9794968,44.6034915,44.786568,44.5313463,44.7488612,44.2743141,44.367191,44.6658941,44.37366517,44.4766526,44.6208353,44.3082168,44.0127932,44.24984,43.858616,43.9777294,43.925537,44.0698918,44.6035552,44.2288747,44.4191965,43.9015048,43.5660829,43.9705397,43.4676913,45.8461,43.5827565,43.2741145,43.8555729,43.7504777,43.5827565,43.3893574,43.8555729,43.8914144,44.0207262,43.1406976,43.7238483,43.5763232,43.3209022,43.5409901,43.2367913,43.0638347,43.1557012,42.9963758,42.5450345,42.6907233,42.3091712,42.8913909,42.6014008],[19.6379551,19.6650593,20.4615173,20.0780937,20.3685737,19.1151469,21.4232908,20.6595515,21.2944883,20.6475673,19.8335496,19.8470699,20.0459853,19.8931066,19.3967698,19.6449683,20.0808092,19.82222,19.7897422,19.8616812,19.6209662,20.45241,20.4489216,19.2067663,19.6907882,19.8903398,20.9604515,20.9335169,21.41833166,21.6742577,21.1842054,20.5562765,20.9114225,21.2008216,21.403911,21.2572719,21.3748327,22.0985086,22.6098737,22.5310674,21.9495234,22.2738011,22.2466754,19.5649108,19.8125507,20.0365,19.5266606,20.0032054,19.842471,19.7142007,19.5266606,19.6463666,19.842471,20.3501652,20.4633133,20.5213617,20.6872542,21.3335812,21.8957589,21.7183695,21.5915831,22.4061647,22.5856811,21.944034,21.9002712,22.1720303,21.6498686,20.8659995,21.1918761],750,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"black\",\"weight\":7,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"red\",\"fillOpacity\":0.7},[\" Baƒçka Topola  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Subotica  Koristi se = 19 Ne koristi se = 0 Nepoznato = 0 Total = 19 \",\" Kikinda  Koristi se = 6 Ne koristi se = 0 Nepoznato = 0 Total = 6 \",\" Senta  Koristi se = 6 Ne koristi se = 1 Nepoznato = 0 Total = 7 \",\" Zrenjanin  Koristi se = 13 Ne koristi se = 0 Nepoznato = 0 Total = 13 \",\" Sombor  Koristi se = 14 Ne koristi se = 0 Nepoznato = 0 Total = 14 \",\" Bela Crkva  Koristi se = 3 Ne koristi se = 0 Nepoznato = 0 Total = 3 \",\" Kovaƒçica  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Vr≈°ac  Koristi se = 4 Ne koristi se = 2 Nepoznato = 0 Total = 6 \",\" Panƒçevo  Koristi se = 14 Ne koristi se = 0 Nepoznato = 0 Total = 14 \",\" Novi Sad  Koristi se = 98 Ne koristi se = 5 Nepoznato = 6 Total = 103 \",\" Sremska Kamenica  Koristi se = 48 Ne koristi se = 8 Nepoznato = 0 Total = 56 \",\" Beƒçej  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Temerin  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Baƒçka Palanka  Koristi se = 0 Ne koristi se = 1 Nepoznato = 0 Total = 1 \",\" Vrbas  Koristi se = 16 Ne koristi se = 0 Nepoznato = 0 Total = 16 \",\" Inƒëija  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Ruma  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Vrdnik  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Irig  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Sremska Mitrovica  Koristi se = 8 Ne koristi se = 1 Nepoznato = 0 Total = 9 \",\" Barajevo  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Beograd  Koristi se = 430 Ne koristi se = 30 Nepoznato = 0 Total = 460 \",\" Loznica  Koristi se = 5 Ne koristi se = 1 Nepoznato = 0 Total = 6 \",\" ≈†abac  Koristi se = 5 Ne koristi se = 1 Nepoznato = 0 Total = 6 \",\" Valjevo  Koristi se = 9 Ne koristi se = 0 Nepoznato = 0 Total = 9 \",\" Smederevska Palanka  Koristi se = 3 Ne koristi se = 0 Nepoznato = 0 Total = 3 \",\" Smederevo  Koristi se = 10 Ne koristi se = 0 Nepoznato = 0 Total = 10 \",\" Petrovac  Koristi se = 2 Ne koristi se = 0 Nepoznato = 0 Total = 2 \",\" Kuƒçevo  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Po≈æarevac  Koristi se = 8 Ne koristi se = 0 Nepoznato = 0 Total = 8 \",\" Aranƒëelovac  Koristi se = 4 Ne koristi se = 0 Nepoznato = 0 Total = 4 \",\" Kragujevac  Koristi se = 35 Ne koristi se = 2 Nepoznato = 3 Total = 40 \",\" Svilajnac  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Paraƒáin  Koristi se = 3 Ne koristi se = 0 Nepoznato = 0 Total = 3 \",\" Jagodina  Koristi se = 4 Ne koristi se = 0 Nepoznato = 0 Total = 4 \",\" ƒÜuprija  Koristi se = 7 Ne koristi se = 2 Nepoznato = 0 Total = 9 \",\" Bor  Koristi se = 3 Ne koristi se = 2 Nepoznato = 0 Total = 5 \",\" Kladovo  Koristi se = 1 Ne koristi se = 1 Nepoznato = 0 Total = 2 \",\" Negotin  Koristi se = 2 Ne koristi se = 1 Nepoznato = 0 Total = 3 \",\" Majdanpek  Koristi se = 2 Ne koristi se = 0 Nepoznato = 0 Total = 2 \",\" Zajeƒçar  Koristi se = 8 Ne koristi se = 0 Nepoznato = 0 Total = 8 \",\" Knja≈æevac  Koristi se = 3 Ne koristi se = 0 Nepoznato = 0 Total = 3 \",\" Bajina Ba≈°ta  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Nova Varo≈°  Koristi se = 3 Ne koristi se = 0 Nepoznato = 0 Total = 3 \",\" Po≈æega  Koristi se = 3 Ne koristi se = 0 Nepoznato = 0 Total = 3 \",\" Priboj  Koristi se = 3 Ne koristi se = 0 Nepoznato = 0 Total = 3 \",\" Sjenica  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" U≈æice  Koristi se = 2 Ne koristi se = 0 Nepoznato = 0 Total = 2 \",\" ƒåajetina  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Priboj  Koristi se = 2 Ne koristi se = 0 Nepoznato = 0 Total = 2 \",\" Prijepolje  Koristi se = 2 Ne koristi se = 0 Nepoznato = 0 Total = 2 \",\" U≈æice  Koristi se = 10 Ne koristi se = 3 Nepoznato = 0 Total = 13 \",\" ƒåaƒçak  Koristi se = 16 Ne koristi se = 1 Nepoznato = 0 Total = 17 \",\" Gornji Milanovac  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Novi Pazar  Koristi se = 3 Ne koristi se = 0 Nepoznato = 0 Total = 3 \",\" Kraljevo  Koristi se = 10 Ne koristi se = 0 Nepoznato = 0 Total = 10 \",\" Kru≈°evac  Koristi se = 6 Ne koristi se = 0 Nepoznato = 0 Total = 6 \",\" Ni≈°  Koristi se = 43 Ne koristi se = 2 Nepoznato = 0 Total = 45 \",\" Aleksinac  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Prokuplje  Koristi se = 4 Ne koristi se = 6 Nepoznato = 0 Total = 4 \",\" Babu≈°nica  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Pirot  Koristi se = 7 Ne koristi se = 0 Nepoznato = 0 Total = 7 \",\" Leskovac  Koristi se = 7 Ne koristi se = 0 Nepoznato = 0 Total = 7 \",\" Vranje  Koristi se = 13 Ne koristi se = 0 Nepoznato = 0 Total = 13 \",\" Surdulica  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \",\" Pre≈°evo  Koristi se = 2 Ne koristi se = 0 Nepoznato = 0 Total = 2 \",\" Kosovska Mitrovica  Koristi se = 7 Ne koristi se = 2 Nepoznato = 0 Total = 9 \",\" Graƒçanica  Koristi se = 1 Ne koristi se = 0 Nepoznato = 0 Total = 1 \"],null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null,null]}],\"fitBounds\":[42.3091712,19.1151469,46.1005467,22.6098737,[]],\"limits\":{\"lat\":[42.3091712,46.1005467],\"lng\":[19.1151469,22.6098737]}},\"evals\":[],\"jsHooks\":[]} This was data democracy in action, a means of holding governments to account and nudging them into action. By empowering citizens to be data literate they are able to cut through the fog of misinformation and to have a deeper understanding of the world we live in. There is no need for everyone to be a data scientist, but there is an urgent need for data literacy to be available to everyone. Data is democracy.\n ","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585267200,"objectID":"2a055bfd31aa7f717bafa8d5d89ac9ca","permalink":"/post/ventilators/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/post/ventilators/","section":"post","summary":"R Markdown For patients critically ill with COVID-19, access to a ventilator could be a matter of life or death. In early March at the beginning of the COVID-19 pandemic the Serbian government was not disclosing information about the available number of ventilators to its citizens.","tags":["visualisation","COVID-19","leaflet"],"title":"Coronavirus: See How Many Medical Ventilators There Are in Your Town","type":"post"},{"authors":null,"categories":null,"content":" Earlier in March we ran our first ever Open Heroines event here in Belgrade.\nWe got things started by creating a website publicising our event and making it clear from the get go what the participants could expect from the event. The website was created in RStudio using blogdown and HUGO and deployed using GitHub and Netlify. The responses to the site were very positive and we were particularly bowled over by the wide range of backgrounds of the people wishing to attend. From economists to journalists, students to CEOs, the desire to understand more about the power of data acted as the adhesive to the collective ensemble.\nWe illustrated the process of the democratisation of open data. In the belief that we should use a topical subject for analysis, we looked at data related to air pollution in Belgrade sourced from Vazduh Gradjanima ‚Äì a hot talking point within the city. We created R-Markdown reports where the participants created visual data reporting of the core issues related to air pollution. There was an amazing response to everyone being able to learn how to interact with data, as if a new world was now available to them.\nIn the spirit of transparency and reproducibility for those who were not fortunate to join us in learning how to use R and analyse this air pollution data, we uploaded our findings to the GitHub https://tanjakec.github.io/OH_workshop/OHSA.html.\n","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"e321cf3d79a71202c36db7be12ff1748","permalink":"/post/oh/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/post/oh/","section":"post","summary":"Earlier in March we ran our first ever Open Heroines event here in Belgrade.\nWe got things started by creating a website publicising our event and making it clear from the get go what the participants could expect from the event.","tags":["open data","wrangling","visualisation"],"title":"Air pollution in Belgrade","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"   Click on the PDF button above to view the slides.   ","date":1529193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529193600,"objectID":"6d69f3fcb3a9dbfec6398310ef467e41","permalink":"/talk/erum2020/","publishdate":"2020-06-18T00:00:00Z","relpermalink":"/talk/erum2020/","section":"talk","summary":"This study examines the often-tricky process of delivering data literacy programmes to professionals with most to gain from a deeper understanding of data analysis. As such, the author discusses the process of building and delivering training strategies to journalists in regions where press freedom is constrained by numerous factors, not least of all institutionalised corruption. Reporting stories that are supplemented with transparent procedural systems are less likely to be contradicted and challenged by vested interest actors. Journalists are able to present findings supported by charts and info graphics, but these are open to translation. Therefore, most importantly, the data and code of the applied analytical methodology should also be available for scrutiny and is less likely to be subverted or prohibited.As part of creating an accessible programme geared to acquiring skills necessary for data journalism, the author takes a step-by-step approach to discussing the actualities of building online platforms for training purposes. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods in a transparent and reproducible way is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand. The resulting ‚Äòdata toolbox for journalists‚Äô is an accessible open-source resource. It can also be adapted to accommodate the need to provide a deeper understanding of the potential for data proficiency to other professions. The accessibility of R allows for users to build support communities, which in the case of journalists is essential for information gathering. Establishing and implementing transparent channels of communication is the key to scrupulous journalism and is why R is so applicable to this objective.","tags":["rstats","data journalism"],"title":"Transparent Journalism Through the Power of R","type":"talk"},{"authors":null,"categories":null,"content":"   Click on the PDF button above to view the slides.   ","date":1526256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526256000,"objectID":"3ecfa0eadad9522d1eeab32434a2efa4","permalink":"/talk/erum2018/","publishdate":"2018-05-16T00:00:00Z","relpermalink":"/talk/erum2018/","section":"talk","summary":"Setting up the computer environment for all participants of an R workshop can be a daunting and exasperating task. Installing and setting up the softwares and obtaining necessary files could give a confusing tone right at the beginning of the workshop before you have even started with the programme delivery. Not messing up with setup and being able to start using R in the same fashion as other people in the group is desirable not just to the workshop participants, but to the instructor. There are a few options available for setting up RStudio in a browser. Using RStudio Server enables you to setup working environment tailored to your specific audience. It helps by removing any frustrations for the participants caused by downloads of the required files and installations from other workshop activities that you really want them to focus on. Setting up RStudio Server on Amazon Web Service (ASW), called an EC2 (Elastic Compute Cloud) instance, is not difficult. You can customise the working environment to meet your needs by installing globally required packages, adding the data and the script files you want participants to have available when running RStudio in their browsers. Another alternative that has become recently available is RStudio Cloud. Although RStudio Cloud is an alpha version and it is still under development you could already do pretty much everything you are able by setting up your own RStudio Server. This talk will illustrate the possibilities and benefits of using RStudio in a browser that could provide a great environment for teaching and learning data science with R.","tags":["rstats","cloud","RStudio","AWS"],"title":"Setting Up Your R Workshop In The Cloud","type":"talk"},{"authors":null,"categories":null,"content":"  The R language provides a rich and flexible environment for working with data, especially data to be used for statistical modelling or graphics.\n R is a comprehensive public domain language for data analysis, with no licensing costs associated with it.\n Being independent of any platform, R is universally applicable and simple to integrate into existing IT structures.\n  You can download R for free from:\nhttps://cran.r-project.org/\nand RStudio, a free and open-source integrated development environment (IDE) for R\nhttps://www.rstudio.com/\nData Science  The new vast amount of data we have begun to take more and more notice of, has given a rise to the new discipline of data science.\n Growing demand of data volume and easy understandability of extracted knowledge and insights from data is the motivating force of data science.\n With the explosion of ‚ÄúBig Data‚Äù problems, data science has become a very hot field in many scientific areas as well as marketing, finance, and other business and social study disciplines. Hence, there is a growing demand for business and social scientific researchers with statistical, modelling and computing skills.\n We can now identify patterns and regularities in data of all sorts that allow us to advance scholarship, improve the human condition, and create commercial and social value.\n The field of data science is emerging at the intersection of the fields of statistics, computer science and design. R provides grate platform for this multidisciplinarity. It is incredibly powerful and as such it should be the first language for data manipulation, data analysis and visualisation you‚Äôre looking to grow skills in if you want to move towards data science.\n   Why R? The R system has an extensive library of packages that offer state-of-the-art-abilities.\nMany of the analyses that they offer are not even available in any of the standard packages.\n The functionalities of:\n   data manipulation, data analysis and visualisation implemented in R are incomparable.  R enables you to escape from the restrictive environments and sterile analyses offered by commonly used statistical software packages.\n R enables easy experimentation and exploration, which improves data analysis.\n R is a tool behind reporting modern data analyses in a reproducible manner making an analysis more useful to others because the data and code that actually conducted the analysis can be made available.\n   R Community ‚ÄúThe R community is one of R‚Äôs best features!‚Äù Revolutions Daily news about using open source R\n Supported by the R Foundation for Statistical Computing and with the strong and open engagement of developers and users from all walks of background from science to commerce it is hard to envisage that any commercial corporation will be able to develop sustainable business model with the same innovative drive and power as R community.\n The collaboration amongst statisticians and other scientist who are engaged with statistical computing and growing interest and engagement of large companies creates altruistic R community which generates the force within which R is conquering the field of data analytics. As a result it creates a more powerful R resource and becomes more usable and attractive to Data scientists and analysists.\n  List of resources ROpenSci: ‚ÄúR community is not just for ‚Äòpower users‚Äô or developers. It‚Äôs a place for users and people interested in learning more about R‚Äù; Provides list of useful links:\n#rstats hashtag ‚Äî a responsive, welcoming, and inclusive community of R users to interact with on Twitter\nR-Ladies ‚Äî a world-wide organization focused on promoting gender diversity within the R community, with more than 30 local chapters\nLocal R meetup groups ‚Äî a google search may show that there‚Äôs one in your area! If not, maybe consider starting one! Face-to-face meet-ups for users of all levels are incredibly valuable\nRweekly ‚Äî an incredible weekly recap of all things R\nR-bloggers ‚Äî an awesome resource to find posts from many different bloggers using R\nDataCarpentry and Software Carpentry ‚Äî a resource of openly available lessons that promote and model reproducible research\nStack Overflow ‚Äî chances are your R question has already been answered here (with additional resources for people looking for jobs)\n  Who uses R? Some of the major domains using R include:\n Financial Services, Pharmaceuticals, Telecom, Life Sciences and Education sector.  Top companies using R are :\n Google, LinkedIn, Facebook, The Financial Times: Quantitative Journalism, Amazon and many more‚Ä¶   How do we do it? Tools needed in a typical data science project:\nR for Data Science by Garrett Grolemund \u0026amp; Hadley Wickham\nhttp://r4ds.had.co.nz/index.html.\n Real Example Does declawing (onychectomy) cause harm to cats? Analyzing 17 years‚Äô worth of shelter admissions data. - The dataset captures specifics about the individual cat (declawed status, age, breed, coat color, etc.) as well as the primary reason for admission. Some of the admission reasons are unconnected to the animal (e.g., moving, can‚Äôt afford pet, allergies) ‚Äî but some reasons are based on problematic behaviors exhibited by the cat (e.g., house-soiling, aggressive to other animals, aggressive to people). Available to us is a CSV file containing 200 sample records.\nCat_Data\n  Do it in R # Install and load packages and data # The tidyverse is a collection of R packages designed for data science # Install the complete tidyverse with # install.packages(\u0026quot;tidyverse\u0026quot;) # load the complete tidyverse with library(tidyverse) ## ‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.0 ‚îÄ‚îÄ ## ‚úì ggplot2 3.3.1 ‚úì purrr 0.3.4 ## ‚úì tibble 3.0.1 ‚úì dplyr 1.0.2 ## ‚úì tidyr 1.1.0 ‚úì stringr 1.4.0 ## ‚úì readr 1.3.1 ‚úì forcats 0.5.0 ## ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() # Install and load the ggplot2 package: grammer of graphics # Install and load the magick package fo advanced image-processing in R # install.packages(\u0026quot;magick\u0026quot;) library(magick) ## Linking to ImageMagick 6.9.11.32 ## Enabled features: cairo, fontconfig, freetype, lcms, pango, rsvg, webp ## Disabled features: fftw, ghostscript, x11 # load the data saved on your computer # cat_claw \u0026lt;- read.csv(\u0026quot;declawing_data_sample.csv\u0026quot;) # or load the data directly from the website cat_claw \u0026lt;- read.csv(\u0026quot;declawing_data_sample.csv\u0026quot;) # Have a look at the data: head() # let us look at first three raws of the data head(cat_claw, n = 3) ## Animal.ID Animal.Name Species Gender Date.Of.Birth Primary.Breed ## 1 1032415 HARLEY Cat M 9/18/1999 Domestic Shorthair ## 2 1032962 TRUCKER Cat M 4/10/1998 Domestic Shorthair ## 3 1033799 Cat M 2/2/2000 Domestic Longhair ## Secondary.Breed Declawed Distinguishing.Markings Purebred BodyWeight ## 1 Mix None 0 0 ## 2 Mix None 0 0 ## 3 Mix None 0 2 ## BodyWeightUnit PrimaryColor SecondaryColor ColorPattern Intake.Date ## 1 \u0026lt;NA\u0026gt; Black White \u0026lt;NA\u0026gt; 03/18/2000 00:14:00 ## 2 \u0026lt;NA\u0026gt; Grey \u0026lt;NA\u0026gt; Tiger 04/06/2000 00:45:00 ## 3 pound Black \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 05/02/2000 00:37:00 ## Intake.Type Intake.Subtype Reason Reason.Category ## 1 Owner/Guardian Surrender Schedule \u0026lt;NA\u0026gt; ## 2 Stray Walk In \u0026lt;NA\u0026gt; ## 3 Owner/Guardian Surrender Walk In Too Many Pets Owner problem # Have alook at the structure of the data: str() # look at the structure of the data str(cat_claw) ## \u0026#39;data.frame\u0026#39;: 200 obs. of 20 variables: ## $ Animal.ID : int 1032415 1032962 1033799 1033965 1038328 1048494 1052572 1053299 1054811 1057979 ... ## $ Animal.Name : chr \u0026quot;HARLEY\u0026quot; \u0026quot;TRUCKER\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ... ## $ Species : chr \u0026quot;Cat\u0026quot; \u0026quot;Cat\u0026quot; \u0026quot;Cat\u0026quot; \u0026quot;Cat\u0026quot; ... ## $ Gender : chr \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; ... ## $ Date.Of.Birth : chr \u0026quot;9/18/1999\u0026quot; \u0026quot;4/10/1998\u0026quot; \u0026quot;2/2/2000\u0026quot; \u0026quot;3/7/2000\u0026quot; ... ## $ Primary.Breed : chr \u0026quot;Domestic Shorthair\u0026quot; \u0026quot;Domestic Shorthair\u0026quot; \u0026quot;Domestic Longhair\u0026quot; \u0026quot;Domestic Longhair\u0026quot; ... ## $ Secondary.Breed : chr \u0026quot;Mix\u0026quot; \u0026quot;Mix\u0026quot; \u0026quot;Mix\u0026quot; \u0026quot;Mix\u0026quot; ... ## $ Declawed : chr \u0026quot;None\u0026quot; \u0026quot;None\u0026quot; \u0026quot;None\u0026quot; \u0026quot;None\u0026quot; ... ## $ Distinguishing.Markings: chr \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ... ## $ Purebred : int 0 0 0 0 0 0 0 0 0 0 ... ## $ BodyWeight : num 0 0 2 0 0 0 0 12 0 0 ... ## $ BodyWeightUnit : chr NA NA \u0026quot;pound\u0026quot; NA ... ## $ PrimaryColor : chr \u0026quot;Black\u0026quot; \u0026quot;Grey\u0026quot; \u0026quot;Black\u0026quot; \u0026quot;Orange\u0026quot; ... ## $ SecondaryColor : chr \u0026quot;White\u0026quot; NA NA \u0026quot;White\u0026quot; ... ## $ ColorPattern : chr NA \u0026quot;Tiger\u0026quot; NA NA ... ## $ Intake.Date : chr \u0026quot;03/18/2000 00:14:00\u0026quot; \u0026quot;04/06/2000 00:45:00\u0026quot; \u0026quot;05/02/2000 00:37:00\u0026quot; \u0026quot;05/07/2000 00:26:00\u0026quot; ... ## $ Intake.Type : chr \u0026quot;Owner/Guardian Surrender\u0026quot; \u0026quot;Stray\u0026quot; \u0026quot;Owner/Guardian Surrender\u0026quot; \u0026quot;Owner/Guardian Surrender\u0026quot; ... ## $ Intake.Subtype : chr \u0026quot;Schedule\u0026quot; \u0026quot;Walk In\u0026quot; \u0026quot;Walk In\u0026quot; \u0026quot;Walk In\u0026quot; ... ## $ Reason : chr \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;Too Many Pets\u0026quot; \u0026quot;Too Many Pets\u0026quot; ... ## $ Reason.Category : chr NA NA \u0026quot;Owner problem\u0026quot; \u0026quot;Owner problem\u0026quot; ... # Do it in a tidy way: glimpse() # previous output was messy as it didn\u0026#39;t fit on the slide. # we want tolook at the structure of the data as much data # as possible and identify data types for each of the variables glimpse(cat_claw) ## Rows: 200 ## Columns: 20 ## $ Animal.ID \u0026lt;int\u0026gt; 1032415, 1032962, 1033799, 1033965, 1038328, ‚Ä¶ ## $ Animal.Name \u0026lt;chr\u0026gt; \u0026quot;HARLEY\u0026quot;, \u0026quot;TRUCKER\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;PUDDY TAT\u0026quot;,‚Ä¶ ## $ Species \u0026lt;chr\u0026gt; \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Ca‚Ä¶ ## $ Gender \u0026lt;chr\u0026gt; \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, ‚Ä¶ ## $ Date.Of.Birth \u0026lt;chr\u0026gt; \u0026quot;9/18/1999\u0026quot;, \u0026quot;4/10/1998\u0026quot;, \u0026quot;2/2/2000\u0026quot;, \u0026quot;3/7/20‚Ä¶ ## $ Primary.Breed \u0026lt;chr\u0026gt; \u0026quot;Domestic Shorthair\u0026quot;, \u0026quot;Domestic Shorthair\u0026quot;, \u0026quot;‚Ä¶ ## $ Secondary.Breed \u0026lt;chr\u0026gt; \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mi‚Ä¶ ## $ Declawed \u0026lt;chr\u0026gt; \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;Fron‚Ä¶ ## $ Distinguishing.Markings \u0026lt;chr\u0026gt; \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;‚Ä¶ ## $ Purebred \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶ ## $ BodyWeight \u0026lt;dbl\u0026gt; 0.00, 0.00, 2.00, 0.00, 0.00, 0.00, 0.00, 12.‚Ä¶ ## $ BodyWeightUnit \u0026lt;chr\u0026gt; NA, NA, \u0026quot;pound\u0026quot;, NA, NA, NA, NA, \u0026quot;pound\u0026quot;, NA,‚Ä¶ ## $ PrimaryColor \u0026lt;chr\u0026gt; \u0026quot;Black\u0026quot;, \u0026quot;Grey\u0026quot;, \u0026quot;Black\u0026quot;, \u0026quot;Orange\u0026quot;, \u0026quot;Black\u0026quot;, ‚Ä¶ ## $ SecondaryColor \u0026lt;chr\u0026gt; \u0026quot;White\u0026quot;, NA, NA, \u0026quot;White\u0026quot;, \u0026quot;White\u0026quot;, \u0026quot;Brown\u0026quot;, N‚Ä¶ ## $ ColorPattern \u0026lt;chr\u0026gt; NA, \u0026quot;Tiger\u0026quot;, NA, NA, NA, \u0026quot;Tiger\u0026quot;, \u0026quot;Tortoisesh‚Ä¶ ## $ Intake.Date \u0026lt;chr\u0026gt; \u0026quot;03/18/2000 00:14:00\u0026quot;, \u0026quot;04/06/2000 00:45:00\u0026quot;,‚Ä¶ ## $ Intake.Type \u0026lt;chr\u0026gt; \u0026quot;Owner/Guardian Surrender\u0026quot;, \u0026quot;Stray\u0026quot;, \u0026quot;Owner/G‚Ä¶ ## $ Intake.Subtype \u0026lt;chr\u0026gt; \u0026quot;Schedule\u0026quot;, \u0026quot;Walk In\u0026quot;, \u0026quot;Walk In\u0026quot;, \u0026quot;Walk In\u0026quot;, ‚Ä¶ ## $ Reason \u0026lt;chr\u0026gt; \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Too Many Pets\u0026quot;, \u0026quot;Too Many Pets\u0026quot;, \u0026quot;\u0026quot;,‚Ä¶ ## $ Reason.Category \u0026lt;chr\u0026gt; NA, NA, \u0026quot;Owner problem\u0026quot;, \u0026quot;Owner problem\u0026quot;, NA,‚Ä¶ What to focus on? # Note that variable \u0026#39;Declawed\u0026#39; is the main variable of interest # with three possible outcomes summary(cat_claw$Declawed) ## Length Class Mode ## 200 character character # sort the dates (DOB and InatekD) to be in the same format cat_claw$Date.Of.Birth \u0026lt;- as.Date(cat_claw$Date.Of.Birth, format=\u0026#39;%m/%d/%Y\u0026#39;) cat_claw$Intake.Date \u0026lt;- as.Date(cat_claw$Intake.Date, format=\u0026#39;%m/%d/%Y\u0026#39;) # How does it look? # check the data glimpse(cat_claw) ## Rows: 200 ## Columns: 20 ## $ Animal.ID \u0026lt;int\u0026gt; 1032415, 1032962, 1033799, 1033965, 1038328, ‚Ä¶ ## $ Animal.Name \u0026lt;chr\u0026gt; \u0026quot;HARLEY\u0026quot;, \u0026quot;TRUCKER\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;PUDDY TAT\u0026quot;,‚Ä¶ ## $ Species \u0026lt;chr\u0026gt; \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Ca‚Ä¶ ## $ Gender \u0026lt;chr\u0026gt; \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, ‚Ä¶ ## $ Date.Of.Birth \u0026lt;date\u0026gt; 1999-09-18, 1998-04-10, 2000-02-02, 2000-03-‚Ä¶ ## $ Primary.Breed \u0026lt;chr\u0026gt; \u0026quot;Domestic Shorthair\u0026quot;, \u0026quot;Domestic Shorthair\u0026quot;, \u0026quot;‚Ä¶ ## $ Secondary.Breed \u0026lt;chr\u0026gt; \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mi‚Ä¶ ## $ Declawed \u0026lt;chr\u0026gt; \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;Fron‚Ä¶ ## $ Distinguishing.Markings \u0026lt;chr\u0026gt; \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;‚Ä¶ ## $ Purebred \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶ ## $ BodyWeight \u0026lt;dbl\u0026gt; 0.00, 0.00, 2.00, 0.00, 0.00, 0.00, 0.00, 12.‚Ä¶ ## $ BodyWeightUnit \u0026lt;chr\u0026gt; NA, NA, \u0026quot;pound\u0026quot;, NA, NA, NA, NA, \u0026quot;pound\u0026quot;, NA,‚Ä¶ ## $ PrimaryColor \u0026lt;chr\u0026gt; \u0026quot;Black\u0026quot;, \u0026quot;Grey\u0026quot;, \u0026quot;Black\u0026quot;, \u0026quot;Orange\u0026quot;, \u0026quot;Black\u0026quot;, ‚Ä¶ ## $ SecondaryColor \u0026lt;chr\u0026gt; \u0026quot;White\u0026quot;, NA, NA, \u0026quot;White\u0026quot;, \u0026quot;White\u0026quot;, \u0026quot;Brown\u0026quot;, N‚Ä¶ ## $ ColorPattern \u0026lt;chr\u0026gt; NA, \u0026quot;Tiger\u0026quot;, NA, NA, NA, \u0026quot;Tiger\u0026quot;, \u0026quot;Tortoisesh‚Ä¶ ## $ Intake.Date \u0026lt;date\u0026gt; 2000-03-18, 2000-04-06, 2000-05-02, 2000-05-‚Ä¶ ## $ Intake.Type \u0026lt;chr\u0026gt; \u0026quot;Owner/Guardian Surrender\u0026quot;, \u0026quot;Stray\u0026quot;, \u0026quot;Owner/G‚Ä¶ ## $ Intake.Subtype \u0026lt;chr\u0026gt; \u0026quot;Schedule\u0026quot;, \u0026quot;Walk In\u0026quot;, \u0026quot;Walk In\u0026quot;, \u0026quot;Walk In\u0026quot;, ‚Ä¶ ## $ Reason \u0026lt;chr\u0026gt; \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Too Many Pets\u0026quot;, \u0026quot;Too Many Pets\u0026quot;, \u0026quot;\u0026quot;,‚Ä¶ ## $ Reason.Category \u0026lt;chr\u0026gt; NA, NA, \u0026quot;Owner problem\u0026quot;, \u0026quot;Owner problem\u0026quot;, NA,‚Ä¶  How old are the cats? # calculate age in days cat_claw$diff_in_days \u0026lt;- cat_claw$Intake.Date - cat_claw$Date.Of.Birth summary(cat_claw$diff_in_days) # summary for class type: \u0026#39;difftime\u0026#39; ## Length Class Mode ## 200 difftime numeric # summary for diff_in_days as numeric (does everything seem ok?) summary(as.numeric(cat_claw$diff_in_days)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -322 122 730 1070 1822 5114 # Identify \u0026#39;incorrect\u0026#39; observation(s) # identify negative diff_in_days; how many? ind \u0026lt;- which.min(as.numeric(cat_claw$diff_in_days)) ind ## [1] 30 # remove observations with intake date before date of bearth # save it as a new data set cat \u0026lt;- cat_claw[-ind,] # replace empty spaces with NA cat$Animal.Name[cat$Animal.Name == \u0026quot;\u0026quot;] \u0026lt;- NA cat$Distinguishing.Markings[cat$Distinguishing.Markings == \u0026quot;\u0026quot;] \u0026lt;- NA cat$Reason[cat$Reason == \u0026quot;\u0026quot;] \u0026lt;- NA # Check the data glimpse(cat) ## Rows: 199 ## Columns: 21 ## $ Animal.ID \u0026lt;int\u0026gt; 1032415, 1032962, 1033799, 1033965, 1038328, ‚Ä¶ ## $ Animal.Name \u0026lt;chr\u0026gt; \u0026quot;HARLEY\u0026quot;, \u0026quot;TRUCKER\u0026quot;, NA, NA, NA, \u0026quot;PUDDY TAT\u0026quot;,‚Ä¶ ## $ Species \u0026lt;chr\u0026gt; \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Cat\u0026quot;, \u0026quot;Ca‚Ä¶ ## $ Gender \u0026lt;chr\u0026gt; \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, ‚Ä¶ ## $ Date.Of.Birth \u0026lt;date\u0026gt; 1999-09-18, 1998-04-10, 2000-02-02, 2000-03-‚Ä¶ ## $ Primary.Breed \u0026lt;chr\u0026gt; \u0026quot;Domestic Shorthair\u0026quot;, \u0026quot;Domestic Shorthair\u0026quot;, \u0026quot;‚Ä¶ ## $ Secondary.Breed \u0026lt;chr\u0026gt; \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mix\u0026quot;, \u0026quot;Mi‚Ä¶ ## $ Declawed \u0026lt;chr\u0026gt; \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;Fron‚Ä¶ ## $ Distinguishing.Markings \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶ ## $ Purebred \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶ ## $ BodyWeight \u0026lt;dbl\u0026gt; 0.00, 0.00, 2.00, 0.00, 0.00, 0.00, 0.00, 12.‚Ä¶ ## $ BodyWeightUnit \u0026lt;chr\u0026gt; NA, NA, \u0026quot;pound\u0026quot;, NA, NA, NA, NA, \u0026quot;pound\u0026quot;, NA,‚Ä¶ ## $ PrimaryColor \u0026lt;chr\u0026gt; \u0026quot;Black\u0026quot;, \u0026quot;Grey\u0026quot;, \u0026quot;Black\u0026quot;, \u0026quot;Orange\u0026quot;, \u0026quot;Black\u0026quot;, ‚Ä¶ ## $ SecondaryColor \u0026lt;chr\u0026gt; \u0026quot;White\u0026quot;, NA, NA, \u0026quot;White\u0026quot;, \u0026quot;White\u0026quot;, \u0026quot;Brown\u0026quot;, N‚Ä¶ ## $ ColorPattern \u0026lt;chr\u0026gt; NA, \u0026quot;Tiger\u0026quot;, NA, NA, NA, \u0026quot;Tiger\u0026quot;, \u0026quot;Tortoisesh‚Ä¶ ## $ Intake.Date \u0026lt;date\u0026gt; 2000-03-18, 2000-04-06, 2000-05-02, 2000-05-‚Ä¶ ## $ Intake.Type \u0026lt;chr\u0026gt; \u0026quot;Owner/Guardian Surrender\u0026quot;, \u0026quot;Stray\u0026quot;, \u0026quot;Owner/G‚Ä¶ ## $ Intake.Subtype \u0026lt;chr\u0026gt; \u0026quot;Schedule\u0026quot;, \u0026quot;Walk In\u0026quot;, \u0026quot;Walk In\u0026quot;, \u0026quot;Walk In\u0026quot;, ‚Ä¶ ## $ Reason \u0026lt;chr\u0026gt; NA, NA, \u0026quot;Too Many Pets\u0026quot;, \u0026quot;Too Many Pets\u0026quot;, NA,‚Ä¶ ## $ Reason.Category \u0026lt;chr\u0026gt; NA, NA, \u0026quot;Owner problem\u0026quot;, \u0026quot;Owner problem\u0026quot;, NA,‚Ä¶ ## $ diff_in_days \u0026lt;drtn\u0026gt; 182 days, 727 days, 90 days, 61 days, 3 days‚Ä¶  What to plot? Plot Age vs Declawed using Boxplot\nboxplot(as.numeric(diff_in_days) ~ Declawed, data = cat, horizontal = TRUE) Can we make it more attractive looking?\n Create graph using ggplot #The image_graph() function opens a new graphics device similar to e.g. png() or x11(). # It returns an image objec to which the plot(s) will be written fig \u0026lt;- image_graph(width = 600, height = 600, res = 96) # plots Age (in years) vs Declawed and saves it as an image ggplot(cat, aes(Declawed, round(as.numeric(diff_in_days)/365), 2)) + geom_boxplot(outlier.size = 0) + geom_jitter(position=position_jitter(width=0.30), shape = 20, size = 3, aes(colour=Declawed), alpha=0.75) + stat_summary(fun.y=mean, shape=23, size = 3, fill = \u0026quot;orange\u0026quot;, col= \u0026quot;black\u0026quot;, geom=\u0026#39;point\u0026#39;) + labs (title= \u0026quot;Cats: Age vs Declawed \u0026quot;, x = \u0026quot; Declawed\u0026quot;, y = \u0026quot; Age\u0026quot;) + theme(panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = 2)) + theme(plot.title = element_text(size = 20, vjust = 2)) + ggsave(\u0026#39;~/Documents/my_R/RLadiesMNE/ggplot_image.png\u0026#39;)  ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Saving 6.25 x 6.25 in image  Adding an animation to a graph: Read gif and background files\n# read cat gif file cat_gif \u0026lt;- image_read(\u0026quot;http://media.giphy.com/media/q0ujUmppx3Fu0/giphy.gif\u0026quot;) # # Background image graph_bg \u0026lt;- image_read(\u0026quot;~/Documents/my_R/RLadiesMNE/ggplot_image.png\u0026quot;) background \u0026lt;- image_background(image_scale(graph_bg, \u0026quot;650\u0026quot;), \u0026quot;white\u0026quot;, flatten = TRUE) # Combine and flatten frames frames \u0026lt;- image_apply(cat_gif, function(frame) { image_composite(background, frame, offset = \u0026quot;+410+10\u0026quot;) }) # Turn frames into animation animation \u0026lt;- image_animate(frames, fps = 10) print(animation)   Happy Plotting!  ","date":1503792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503792000,"objectID":"79e77de9a5dd850b0f62028b0b949952","permalink":"/post/why_r/","publishdate":"2017-08-27T00:00:00Z","relpermalink":"/post/why_r/","section":"post","summary":"The R language provides a rich and flexible environment for working with data, especially data to be used for statistical modelling or graphics.\n R is a comprehensive public domain language for data analysis, with no licensing costs associated with it.","tags":["rstats","visualisation"],"title":"Why R? An illustration through the use of ggplot and magick","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Tatjana Kecojevic","Alan Derbyshire"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1404777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404777600,"objectID":"2e830a9564f9136d0e04855ec2f1a7b2","permalink":"/publication/vei/","publishdate":"2014-07-08T00:00:00Z","relpermalink":"/publication/vei/","section":"publication","summary":"This study focuses on the development of the Boka Kotorska region of Montenegro. As such it attempts to analyse the viability of sustainable development in the region and considers the role of vernacular architecture and ecology as contributory factors.","tags":null,"title":"The Boka Kotorska$\\colon$ Vernacular Response to Sustainable Urban Environments.","type":"publication"},{"authors":null,"categories":null,"content":"   Click on the PDF button above to view the slides.   ","date":1340841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1340841600,"objectID":"dd07e9966e72d392b581c0de2814585c","permalink":"/talk/user2016/","publishdate":"2016-06-30T00:00:00Z","relpermalink":"/talk/user2016/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":["quantile regression","bootstrapping","kernel smoothing","general linear model","rstats"],"title":"Data Landscapes$\\colon$ a pragmatic and philosophical visualisation of the sustainable urban landscape","type":"talk"},{"authors":null,"categories":null,"content":"   Click on the PDF button above to view the slides.   ","date":1339459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1339459200,"objectID":"f5c60dce51564a33294c3a5c09044c30","permalink":"/talk/user2012/","publishdate":"2012-06-15T00:00:00Z","relpermalink":"/talk/user2012/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":["quantile regression","bootstrapping","kernel smoothing","general linear model","rstats"],"title":"Smooth Bootstrap Inference for Parametric Quantile Regression","type":"talk"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1272672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1272672000,"objectID":"9781849fd556eedb105ef80aa2a3d4eb","permalink":"/publication/regional_disparity/","publishdate":"2010-05-01T00:00:00Z","relpermalink":"/publication/regional_disparity/","section":"publication","summary":"Data for this study were collected over 2 years (2004 and 2005). A cross-sectional representative sample of the Saudi population of healthy children below 5 years of age was used to calculate the prevalence of malnutrition. The study was carried out in the College of Medicine, King Saud University, Riyadh, Kingdom of Saudi Arabia. Body measurements of the weight, length, and height were performed according to standard recommendations. Standard deviation scores were determined using the Lambda, Mu, and Sigma (LMS) statistical methodology. The 1978 NCHS/WHO growth reference was used for the calculation of prevalence of underweight, wasting, and stunting defined as the proportion of children whose weight for age, weight for height, and height for age was below minus standard deviation (-2 SD) for Northern, Southwestern, and Central regions of the Kingdom of Saudi Arabia. Chi-square test was used to assess the difference in prevalence between regions, and a p","tags":null,"title":"Regional disparity in prevalence of malnutrition in Saudi children","type":"publication"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1264982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1264982400,"objectID":"98c1e21799f87fd494d534a6b956b63d","permalink":"/publication/pattern_sex_differences/","publishdate":"2010-02-01T00:00:00Z","relpermalink":"/publication/pattern_sex_differences/","section":"publication","summary":"The data set was based on a cross-sectional representative sample of the Saudi population of healthy children and adolescents from birth to 19 years of age. Body measurements (length, height, weight, and head circumference) were performed according to standard recommendations; body mass index was also determined for each subject. The difference in growth between boys and girls was assessed based on z scores and percentiles (5th, 50th, and 95th) of growth parameters using 2 age groups (0‚Äì3 years and 2‚Äì19 years). The significance of the difference between boys and girls for any growth parameter was tested by ANCOVA.","tags":null,"title":"Pattern of sex differences in growth of Saudi children and adolescents","type":"publication"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1264982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1264982400,"objectID":"9d43579046d0e3dd48cc44ee62eb49f9","permalink":"/publication/prevalence/","publishdate":"2010-02-01T00:00:00Z","relpermalink":"/publication/prevalence/","section":"publication","summary":"The data set of the 2005 Saudi reference was used to calculate the BMI for age in children below 60 months of age. The prevalence of overweight was calculated based on the two references using the WHO cutoffs (weight for height or BMI for age above +2 standard deviation score). A lower cutoff for excess weight, BMI for age  + 1 SDS (equivalent CDC cut off for overweight) was also used. All calculations were performed using the WHO and CDC software as appropriate. Chisquare test was used to compare proportions and a p-value ","tags":null,"title":"Prevalence of overweight in preschool children using the new WHO growth standards","type":"publication"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1260835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1260835200,"objectID":"912855cacdf750483fbd66f850ac3062","permalink":"/publication/reference-growth-charts/","publishdate":"2009-12-15T00:00:00Z","relpermalink":"/publication/reference-growth-charts/","section":"publication","summary":"The purpose of this study is to provide Saudi Arabian population reference growth standards for height, weight, body mass index (BMI), head circumference and weight for length/stature. The estimated distribution centiles are obtained by splitting the population into two separate age groups$\\colon$ infants, birth to 36 months and children and adolescents, age 2 to 19 years. The reference values were derived from cross-sectional data applying the LMS method of Cole and Green (Stat. in Medicine 1992; 11:1305-1319) using the lmsqreg package in R (public domain language for data analysis, 2009). The report provides an overview of how the method has been applied, more specifcally how the relevant issues concerning the construction of the growth charts have been addressed and is illustrated by just using the girls weight data (birth to three years old). These issues include identifying the outliers, diagnosing the appropriate amounts of smoothing and averaging the reference standards for the overlapping 2 to 3 year age range. The use of ANCOVA has been introduced and illustrated as a tool for making growth standard comparisons between different geographical regions and between genders.","tags":null,"title":"Reference growth charts for Saudi Arabian children and adolescents","type":"publication"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.    Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.     Click the Slides button above to demo Academic‚Äôs Markdown slides feature.   ","date":1254355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1254355200,"objectID":"cced6d6894551f53086d48b9731fbfaa","permalink":"/publication/regional_variations/","publishdate":"2009-10-01T00:00:00Z","relpermalink":"/publication/regional_variations/","section":"publication","summary":"The 2005 Saudi reference was based on a cross-sectional representative sample of the Saudi population of healthy children and adolescents from birth to 18 years of age. Body measurements of the length, stature, weight, head circumference and calculation of the BMI were performed according to stan- dard recommendations. Percentile construction and smoothing were performed using the LMS (lambda, mu and sigma) methodology, followed by transformation of all individual measurements into standard deviation scores. Factors such as weight for age, height for age, weight for height, and head circumference for children from birth to 3 years, stature for age, head circumference and body mass index for children between 2-18 years of age were assessed. Subsequently, variations in growth between the three main regions in the north, southwest, and center of Saudi Arabia were calculated, with the Bonferoni$\\colon$  method used to assess the significance of differences between regions.","tags":null,"title":"Regional variations in the growth of Saudi children and adolescents","type":"publication"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1251763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1251763200,"objectID":"98afe4a7a8b9bbdc9a5ed7b26aa0e712","permalink":"/publication/bmi/","publishdate":"2009-09-01T00:00:00Z","relpermalink":"/publication/bmi/","section":"publication","summary":"Data from a stratified multistage probability sample were collected from the 13 health regions in Saudi Arabia, as part of a nationwide health profile survey of Saudi Arabian children and adolescents conducted to establish normal physical growth references. Selected households were visited by a trained team. Weight and length/height were measured and recorded following the WHO recommended procedures using the same equipment, which were subjected to both calibration and intra/interobserver variations.","tags":null,"title":"Body mass index in Saudi Arabian children and adolescents$\\colon$ a national reference and comparison with international standards","type":"publication"},{"authors":null,"categories":null,"content":"   Click on the PDF button above to view the slides.   ","date":1245283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1245283200,"objectID":"dd282a7024ca2048cb35bc1226aecd44","permalink":"/talk/fordham/","publishdate":"2009-06-18T00:00:00Z","relpermalink":"/talk/fordham/","section":"talk","summary":"The purpose of this study is to provide Saudi Arabian population reference growth standards for height, weight, body mass index (BMI), head circumference and weight for length/stature. The estimated distribution centiles are obtained by splitting the population into two separate age groups$\\colon$ infants, birth to 36 months and children and adolescents, age 2 to 19 years. The reference values were derived from cross-sectional data applying the LMS method of Cole and Green (Stat. in Medicine 1992; 11:1305-1319) using the lmsqreg package in R (public domain language for data analysis, 2009). The report provides an overview of how the method has been applied, more specifcally how the relevant issues concerning the construction of the growth charts have been addressed and is illustrated by just using the girls weight data (birth to three years old). These issues include identifying the outliers, diagnosing the appropriate amounts of smoothing and averaging the reference standards for the overlapping 2 to 3 year age range. The use of ANCOVA has been introduced and illustrated as a tool for making growth standard comparisons between different geographical regions and between genders.","tags":["rstats","quantile regression"],"title":"Construction of the Reference Growth Charts for Saudi Arabian Children and Adolescents","type":"talk"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1238544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1238544000,"objectID":"359a7002a2083b8f84600a95cb894600","permalink":"/publication/implications_who/","publishdate":"2009-04-01T00:00:00Z","relpermalink":"/publication/implications_who/","section":"publication","summary":"The World Health Organization (WHO) has recently released a new child growth standard that it recommends for international use. The objective of this study was to demonstrate the differences and the implications of using the WHO child growth standards on Saudi children. The Saudi reference was based on a cross-sectional sample of the population of healthy children and adolescents from birth to 19 years of age. The WHO sample was selected from privileged households in some countries. Percentile construction and smoothing were performed using the lambda, mu, sigma (LMS) methodology in both studies. The data from the WHO study including the 3rd, 5th, 50th, 95th, and 97th percentiles were plotted on the Saudi charts for weight for age, height for age, and weight for height. There are major differences between the 2 studies. Compared with the Saudi charts, the WHO lower percentiles (third and fifth) are shifted upward, whereas the upper percentiles are shifted downward. The use of the WHO standards in Saudi Arabia and possibly in other countries of similar socioeconomic status increases the prevalence of undernutrition, stunting, and wasting, potentially leading to unnecessary referrals, investigations, and parental anxiety. Clear guidelines should be developed by WHO experts to guide clinicians in developing countries in the proper use of the standards not only to determine prevalence but also in the daily clinical assessment of the growth of children.","tags":null,"title":"The Implications of Using the World Health Organization Child Growth Standards in Saudi Arabia","type":"publication"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1228089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1228089600,"objectID":"e1c793b0b2b96afce1f15b6763451ec1","permalink":"/publication/comparison_cdc/","publishdate":"2008-12-01T00:00:00Z","relpermalink":"/publication/comparison_cdc/","section":"publication","summary":"The Saudi reference was based on a cross-sectional representative sample of the Saudi population of healthy children and adolescents from birth to 19 years of age. Measurements of the length/ stature, weight and head circumference were performed according to expert recommendations. The CDC charts from birth to 20 years were based on a cross-sectional representative national sample from five sources collected between 1963 and 1994. The data from the CDC study including the 3rd, 5th, 50th, 95th, and 97th percentiles were plotted against the corresponding percentiles on the Saudi charts for the weight for age, height for age, weight for height for children from 0 to 36 months and weight for age, stature for age and body mass index for children 2 to 19 years of age.","tags":null,"title":"Comparison of the 2005 growth charts for Saudi children and adolescents to the 2000 CDC growth charts","type":"publication"},{"authors":["Tatjana Kecojevic","Peter Foster"],"categories":null,"content":"   Click the Cite button above to enable to import publication metadata into your reference management software.   ","date":1157068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1157068800,"objectID":"2aebe4b1bc1f5053d664e37863955223","permalink":"/publication/adiposity/","publishdate":"2006-09-01T00:00:00Z","relpermalink":"/publication/adiposity/","section":"publication","summary":"Growth measurements of 70 male and 40 female patients with AD followed through childhood and adolescence were studied retrospectively and compared with the 1990 U.K. normal values. Height, weight and body mass index (BMI) were converted to standard deviation scores (SDS). Regression analysis examined whether the mean trend was different from zero.","tags":null,"title":"Pattern of growth and adiposity from infancy to adulthood in atopic dermatitis","type":"publication"}]