---
title: "Subset Variable Selection in R"
author: "Tanja"
date: "2021-02-23T00:00:00Z"
lastmod: "2021-02-23T00:00:00Z"
output: html_document
tags: ['multifactor regression', 'best subset', 'stepwise regression', 'cross-validation']
---

---
```{r, message = FALSE, warning = FALSE, echo = FALSE}
require(knitr)
opts_chunk$set(eval = T)
```


This short tutorial on Subset variable Selection in R comes from pp. 244-251 of ["Introduction to Statistical Learning with Applications in R" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani](https://www.statlearning.com) and chapter "Forward, Backward, and Stepwise Selection", from pp. 511-519 of ["Applied Predictive Modeling"](http://appliedpredictivemodeling.com) by [Max Kuhn](https://twitter.com/topepos) and Kjell Johnson.


For the large numbers of explanatory variables, and many interactions and non-linear terms, the process of model simplification can take a very long time. There are many algorithms for automatic variable selection that can help to chose the variables to include in a regression model.

## Using Best Subsets Regression and Stepwise Regression

**Stepwise regression** and **Best Subsets regression** (**BREG**) are two of the more common variable selection methods. The _stepwise_ procedure starts from the saturated model (or the maximal model, whichever is appropriate) through a series of simplifications to the minimal adequate model. This progression is made on the basis of deletion tests: F tests, AIC, t-tests or chi-squared tests that assess the significance of the increase in deviance that results when a given term is removed from the current model.

The _BREG_, also known as “all possible regressions”, as the name of the procedure indicates, fits a separate least squares regression for each possible combination of the $p$ predictors, i.e. explanatory variables. After fitting all of the models, BREG then displays the best fitted models with one explanatory variable, two explanatory variables, three explanatory variables, and so on. Usually, either adjusted R-squared or Mallows Cp is the criterion for picking the best fitting models for this process. The result is a display of the best fitted models of different sizes up to the full/maximal model and the final fitted model can be selected by comparing displayed models based on the criteria of parsimony.

### Case Study 

We want to apply the subset selection approach to the `fat` data, available in the `library(faraway)`. 
We wish to predict body fat (variable `brozek`) using all predictors except for `siri`, `density` and `free`. 

As instructed, we will start off by creating a subset of all the variables from the `fat` data set except for `siri`, `density` and `free`. We will save this subset as a new data frame `bodyfat`. To confirm that the three suggested variables have been removed, we will check for the difference in the dimension of the two data frames and have a glimpse at teh data.

```{r}
#If you don't have the "ISLR" package installed yet, uncomment and run the line below
#install.packages("ISLR")
library(ISLR) 
library(faraway)
library(tidyr)
suppressPackageStartupMessages(library(dplyr))
bodyfat <- fat %>% 
  select(-siri, -density, -free) 
dim(fat) - dim(bodyfat)
glimpse(bodyfat)
```

Using the `glimpse()` function allows us to notice that all the variables in our new `bodyfat` data set are the measured type.
#### Get to Know Your Data

Let us take a look at the data using the `DT` package, which provides an R interface to the JavaScript library `DataTables`. It will enable us to filter through the displayed data and quickly check for `NA`s to see if there are any missing values that should be of concern. 

```{r}
DT::datatable(fat)
```

Alternatively, we can use the `is.na()` function to check if there are any missing observations in particular with respect to the response variable `bronzek`. For a given input vector the `is.na()` function returns a vector of `TRUE` and `FALSE` value, with a value of `TRUE` for any elements that are missing, and a `FALSE` value for non-missing elements. By using the `sum()` function we can then count all of the missing elements, which can be easily removed from data by using either the `na.omit()` function or the `drop_na` function.

```{r}
bodyfat %>%
  select(brozek) %>%
  is.na() %>%
  sum() 
```

### The correlation matrix and its visualisation

Previously (see the section on [Multiple Regression](https://dataliteracy.rbind.io/module4/mlregression/)), we learnt how to examine multivariate data by creating a scatter plot matrix to obtain an in-depth vision into its behaviour. We used the `GGally::ggpairs()` function that produces a pairwise comparison of multivariate data for both data types: measured and attribute. Creating such a matrix for a data set with a considerable number of variables could be a taxing task with a resulting display that may not be easy to make sense of.     

Fabulous [Frank Harrell](https://www.vumc.org/biostatistics/person/frank-e-harrell-jr-phd) (check his blog [Statistical Thinking](https://www.fharrell.com/) if you haven't stumbled upon it yet!) has created the [`Hmisc`](https://cran.r-project.org/web/packages/Hmisc/index.html) package with many useful functions for data analysis and high-level graphics. It contains the `rcorr` function for the computation of Pearson or Spearman correlation matrix with pairwise deletion of missing data. It generates one table of correlation coefficients, i.e. the correlation matrix and another table of the $p$-values.

```{r}
bodyfat.rcorr = Hmisc::rcorr(as.matrix(bodyfat))
bodyfat.coeff = bodyfat.rcorr$r
bodyfat.p = bodyfat.rcorr$P
bodyfat.coeff
bodyfat.p
```

Looking at the figures above, we notice that the response variable brozek is in strong association with all of the explanatory variables from the data set. In fact, it looks as if all variables are in strong correlation with one another, with a very few exceptions such as in the case of `height` and `adipos`, `hip` and `age`, `weight` and `age` to name a few of the most obvious ones. In an example like this one, where there are a high number of variables to consider, it is useful to visualise the correlation matrix. There are several packages available for the visualisation of a correlation matrix in R:

- [`GGally`](https://cran.r-project.org/web/packages/GGally/index.html) - extension to [`ggplot2`](https://ggplot2.tidyverse.org)
    + [`ggpairs()`](https://cran.r-project.org/web/packages/GGally/GGally.pdf) function
- [`ggcorrplot`](https://cran.r-project.org/web/packages/ggcorrplot/index.html) - visualisation of a correlation matrix using [`ggplot2`](https://ggplot2.tidyverse.org)
    + [`ggcorrplot()`](https://cran.r-project.org/web/packages/ggcorrplot/ggcorrplot.pdf) function
- [`corrplot`](https://cran.r-project.org/web/packages/corrplot/index.html) - rich visualisation of a correlation matrix
    + [`corrplot.mixed()`](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) function
- [`corrr`](https://cran.r-project.org/web/packages/corrr/index.html) - a tool for exploring correlations
    + [`network_plot()`](https://cran.r-project.org/web/packages/corrr/corrr.pdf) function
- [`corrgram`](https://cran.r-project.org/web/packages/corrgram/index.html) - calculates correlation of variables and displays the results graphically, based on the [Corrgrams: Exploratory displays for correlation matrices](http://euclid.psych.yorku.ca/datavis/papers/corrgram.pdf) paper (Friendly, 2002)
    + [`corrgram`](https://cran.r-project.org/web/packages/corrgram/corrgram.pdf) function
- [`PerformanceAnalytics`](https://cran.r-project.org/web/packages/PerformanceAnalytics/Performance+ Analytics.pdf) - package of econometric functions for performance and risk analysis of financial instruments or portfolios
    + [`chart.Correlation()`](https://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf) function
- [`psych`](https://cran.r-project.org/web/packages/psych/index.html) - a toolbox for personality, psychometric theory and experimental psychology
    + [`pairs.panels()](https://cran.r-project.org/web/packages/psych/psych.pdf)

We will visualise the correlation matrix by using the `corrplot` package that offers many customization options that have been neatly presented in this short [`An Introduction to corrplot Package`](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) tutorial.

```{r}
# If you don't have the "corrplot" package installed yet, uncomment and run the line below
#install.packages("corrplot")
suppressPackageStartupMessages(library(corrplot))
corrplot(cor(bodyfat), 
         method = "ellipse",
         order = "hclust")
```

For larger and more complex datasets, the construction of a correlogram has obvious advantages for exploratory purposes, because it shows all the correlations in an uncomplicated manner. For example, the information provided through this correlogram is easy to absorb as positive correlations are displayed in blue and negative correlations in red, with the correlation coefficients and the corresponding colours displayed in the legend. Colour intensity and the width of the ellipse are proportional to the correlation coefficients, making it altogether easier to read and understand.

Here we see, among other things, that a) the majority of variables are positively correlated with one another and  that b) `age` and `height` are the two variables with weak correlations to most other variables.

Without a doubt this is an example in which **Stepwise regression** and **Best Subsets regression** (**BREG**) procedures can be deployed as the effective tools in helping us to identify useful predictors.

### Best Subsets regression

The [`leaps`](https://cran.r-project.org/web/packages/leaps/index.html) package enables the best subset selection through the application of the `regsubsets()` function. It identifies the best model that contains a given number of predictors, where **best** is quantified using residual sum of squares (RSS). The syntax is the same as for the `lm()` function and the `summary()` command outputs the best set of variables for each model size. 

The `regsubsets()` function (part of the [`leaps`](https://cran.r-project.org/web/packages/leaps/index.html) library) performs the best subset selection by identifying the best model that contains a given number of predictors, where **best** is quantified using RSS. The syntax is the same as for `lm()`. The `summary()` command outputs the best set of variables for each model size. We will save the results of the call of the `summary()` function as `breg_summary`, which will allow us to access the parts we need to focus on. 

```{r}
# # If you don't have the "leaps" package installed yet, uncomment and run the line below
#install.packages("leaps")
library(leaps)
breg_full = regsubsets(brozek ~ ., data = bodyfat)
breg_summary = summary(breg_full)
breg_summary
```

Note that the summary prints out the asterisks ("\*"). The presence of a "\*" indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best three-variable model contains only `weight`, `abd` and `wrist`. Note that by default, `regsubsets()` only reports results up to the best eight-variable model. But the `nvmax` argument option can be used in order to return as many variables as are desired. As we would like to use all the available explanatory variables we will request a fit up to a 14-variable model, that is:

$$Y=f(X_1, X_2, X_3,..., X_{14}) $$

```{r}
breg_full <- regsubsets(brozek ~., data = bodyfat, nvmax = 14)
breg_summary <- summary(breg_full)
breg_summary
```

💡Remember that we can use the $\bar{R}^2$ to select the best model! We need to discover the other pieces of information contained in the `breg_summary`. 

```{r}
names(breg_summary)
```

In addition to the output that indicates the inclusion of the variables in the given models we get when we print the summary in the console, the `summary()` function also returns the necessary statistics for the best model selection. It provides $R^2$, $RSS$, $\bar{R}^2$, $C_p$, and $BIC$, which will help us in the selection of the best overall model. 

Knowing that the $R^2$ statistic increases monotonically as more variables are included, it will not be effective to use it in the model selection procedure. However, we are going to examine it to see the level to which it increases.

```{r}
t(t(sprintf("%0.2f%%", breg_summary$rsq * 100)))
```

As expected, the $R^2$ statistic increases monotonically from 66.21% when only one variable is included in the model to almost 75% with the inclusion of 12 or more variables.

Creating the line plots to visualise $RSS$, $\bar{R}^2$, $C_p$, and $BIC$, for all of the models at once will help us to decide which model to select. We will keep things simple and create the visualisations using the `plot` function in `base` R with argument option `type = "l"` onto which we will superimpose the corresponding minimal or maximal values using the `point` function in respect of the statistic displayed.

```{r}
par(mfrow = c(2,2)) # Set up a 2x2 grid for display of 4 plots at once

plot(breg_summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

# line plot of adjusted R^2 statistic
plot(breg_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
# identify the location of the maximum point
adj_r2_max = which.max(breg_summary$adjr2)
# plot a red point to indicate the model with the largest adjusted R^2 statistic
points(adj_r2_max, breg_summary$adjr2[adj_r2_max], col ="red", cex = 1, pch = 20)

# line plot of C_p and BIC, but this time in a search of the models with the SMALLEST statistic
# line plot of C_p statistic
plot(breg_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(breg_summary$cp) 
points(cp_min, breg_summary$cp[cp_min], col = "red", cex = 1, pch = 20)
# line plot of BIC statistic
plot(breg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(breg_summary$bic)
points(bic_min, breg_summary$bic[bic_min], col = "red", cex = 1, pch = 20)
```

We see that the measures are not in sync with one another and we realise that no one measure is going to give us an entirely accurate picture. According to $\bar{R}^2$ and $C_p$ the best performing model is the one with 8 variables and according to $BIC$ the best performing model has only 4 variables. This outcome suggests that the models with fewer than 4 predictors is insufficient, while a model with more than 8 predictors will overfit. 

The `regsubsets()` function has a built-in `plot()` command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the $R^2$, $\bar{R}^2$, $C_p$, and $BIC$ statistic. _The top row_ of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. That is, when creating this plot for the display of the $R^2$ value it is not a surprise to see that the top row indicates inclusion of all 14 predictors for the $R^2$ of 75%. 

```{r}
plot(breg_full, scale="r2")
```

We can now identify the suggested 8 predictors for the best model by observing the top row and checking which of the variables has a black square.  
In the following plot by observing the top row and checking which of the variable has a black square we can identify the exact 8 predictors to be used for the best model as suggested by $\bar{R}^2$.

```{r}
plot(breg_full, scale ="adjr2")
```

It suggests that $$brozek = f(age, weight, neck, abdom, hip, thigh, forearm, wrist)$$
We can use the `coef()` function to see the coefficient estimates associated with this model.

```{r}
coef(breg_full, 8)
```

```{r}
plot(breg_full, scale="Cp")
```

$C_p$ suggests the same set of predictors as $\bar{R}^2$.  


```{r}
plot(breg_full, scale="bic")
```

We see that two models share a BIC close to -310. However, the model with the lowest $BIC$ is the four-variable model that contains only `weight`, `abdom`, `forearm` and `wrist`

```{r}
coef(breg_full, 4)
```

### Forward and Backward Stepwise Selection

To perform stepwise selection we will also use the `regsubsets()` function, but this time with the argument `method` set to either "`forward`" or "`backward`" depending on which of the two stepwise selections we wish to perform.

```{r}
# Forward stepwise selection
stepw_fwd <- regsubsets(brozek ~ ., data = bodyfat, nvmax = 14, method = "forward")
summary(stepw_fwd)
stepw_bwd <- regsubsets(brozek ~ ., data = bodyfat, nvmax = 14, method = "backward")
summary(stepw_bwd)
stepw_new <- regsubsets(brozek ~ ., data = bodyfat, nvmax = 14, method = "seqrep")
summary(stepw_new)
```

We can see that the best one-variable through four-variable models are each identical for best subset and forward and backward selection. Difference occurs for the five-variable model for which the backward selection method selects different set of predictors from the other two. 

```{r}
coef(breg_full, 5)
coef(stepw_fwd, 5)
coef(stepw_bwd, 5)
```

Another difference in selection happens again for the six-factor model for which this time forward selection suggests a different set of predictors from the other two. 

```{r}
coef(breg_full, 6)
coef(stepw_fwd, 6)
coef(stepw_bwd, 6)
```
For the models with seven predictors and more all three methods suggest the same variables.     

## The Validation Set Approach and Cross-Validation for the selection of the best model

In this section validation set and cross-validation approaches will be used to chose the best model among a set of models of different sizes. When applying these approaches we split the given data into two subsets:

- **training data**: a subset to train a model
- **test data**: a subset to test the model

The training data is used for model estimation and variable selection, whilst the remaining test data is put aside and reserved for testing the accuracy of the model (see section on [Machine Learning](http://dataliteracy.rbind.io/module4/what_is_ml/)). That is, the best model variable selection is performed using only the training observations that are randomly selected from the original data. 

We begin the application of validation approach in R by splitting the `bodyaft` data into a training set and a test set. First, we set a random seed to ensure the same data split each time we run the randomised process of splitting the `bodyfat` data into training set and test set. The `set.seed()` function sets the starting number used to generate a sequence of random numbers (it ensures that you get the same result if you start with that same seed each time you run the same process). Next, we create a random vector, `train`, of elements equal to TRUE if the corresponding observation is in the training set, and FALSE otherwise. A random vector `test` is obtained by using the `!` command that creates TRUEs to be switched to FALSEs and vice versa. Once both `train` and `test` vectors are obtained we perform best subset on the `training` set of the `bodyfat` data using the now familiar `regsubset()` function. 

```{r}
set.seed(111)
train <- sample(c(TRUE, FALSE), nrow(bodyfat), rep = TRUE)
test <- (!train )
breg <- regsubsets(brozek ~., data = bodyfat[train, ], nvmax = 14)
```

We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data. The `model.matrix()` function is used in many regression packages for building an **$X$** matrix from data.

<span style="color: gray">

In the multiple regression setting because of the potentially large number of predictors, it is more convenient to write the expressions using matrix notation. To develop the analogy let us quickly look at the matrix notation for the 14-variable linear regression function:

<span style="color: gray">$$y_i = \beta_0 + \beta_1x_1 + \epsilon_i \;\;\;\;\;\; \text{for} \;\; i=1, 2, ...n$$
<span style="color: gray">If we actually let $i = 1, ..., n$, we see that we obtain $n$ equations:
<span style="color: gray">$$y_i = \beta_0 + \beta_1x_1 + \epsilon_1$$
<span style="color: gray">$$y_i = \beta_0 + \beta_1x_2 + \epsilon_2$$
<span style="color: gray">$$ \vdots$$
<span style="color: gray">$$y_i = \beta_0 + \beta_1x_n + \epsilon_n$$
<span style="color: gray">It is more efficient to use matrices to define the regression model which can be formulated for the above simple linear regression function in matrix notation as:
<span style="color: gray">$$
\begin{bmatrix}
y_1\\y_2\\ \vdots\\y_n
\end{bmatrix} =
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n \\
\end{bmatrix}
\begin{bmatrix}
\beta_0\\\beta_1\\ 
\end{bmatrix}
+ \begin{bmatrix}
\varepsilon_1\\\varepsilon_2\\ \vdots\\\varepsilon_n
\end{bmatrix}
$$
<span style="color: gray">Instead of writing out the $n$ equations, using matrix notation, the simple linear regression function reduces to a short and simple statement:
<span style="color: gray">
$$\mathbf{Y} = \mathbf{X\beta} + \mathbf{\varepsilon},$$
where matrices **$Y$**, $\mathbf{\varepsilon}$ are of the size $n \times 1$,
$\mathbf{\beta}$ of $2 \times 1$ and finally matrix **$Y$** of $n \times 2$.
</span>

Once we have created **$X$** matrix we run a loop, and for each $i$, we 

1) extract the coefficients from `breg` for the best model of that size 
2) multiply them into the appropriate columns of the test model matrix to form the predictions and
3) compute the test MSE

```{r}
test.mat <- model.matrix(brozek ~., data = bodyfat[test, ])
val.errors <- rep(NA, 14)
for(i in 1:14){
 coefi <- coef(breg, id = i)
 pred <- test.mat[ ,names(coefi)] %*% coefi
 val.errors[i] <- mean((bodyfat$brozek[test] - pred)^2)
}
val.errors
coef(breg, which.min(val.errors))
```
It looks as if the best model is the one that contains five variables. As there is no `predict()` method for `regsubsets()` we can capture our steps above and write our own predict method.
```{r}
predict.regsubsets <- function(object, newdata, id,...){
 form <- as.formula(object$call[[2]])
 mat <- model.matrix(form, newdata)
 coefi <- coef(object, id = id)
 xvars <- names(coefi)
 mat[,xvars ] %*% coefi
 }
```
The function almost does what we did above. The only tricky bit is the extraction of the formula used in the call to `regsubsets()`. We are going to use this function to do cross-validation. We start by performing best subset selection on the full data set, and select the best five-variable model. In order to get more accurate coefficient estimates we make use of the full data set, rather than using the variables that were obtained from the training set. We should realise that the best five-variable model on the full data set may differ from the corresponding five-variable model on the training set.  
```{r}
breg <- regsubsets(brozek ~., data = bodyfat , nvmax = 14)
coef(breg, 5)
```
We are in luck as the best five-variable model on the full data set has the same set of variables as identified for the best five-variable model on the training set.

We now try to choose among the models of different sizes using cross-validation and perform best subset selection within each of the $k$ training sets. This taxing task can be easily performed in R by creating a vector that allocates each observation to one of k = 10 folds and a matrix that will hold the results.

```{r}
k <- 10
set.seed(111)
folds <- sample (1:k, nrow(bodyfat), replace = TRUE)
cv.errors <- matrix(NA, k, 14, dimnames = list(NULL, paste (1:14)))
```



Now we write a `for` loop that performs cross-validation:

1) in the $j^{\text{th}}$ fold, the elements of folds that equal $j$ are in the _test set_, and the remainder are in the _training set_. 
2) next, 
  - make our predictions for each model size (using our new `predict()` method) 
  - compute the test errors on the appropriate subset, and
  - store them in the appropriate slot in the matrix `cv.errors`.

```{r}
for(j in 1:k){
 best.fit <- regsubsets(brozek ~., data = bodyfat[folds!=j, ], nvmax = 14)
 for(i in 1:14){
    pred <- predict(best.fit, bodyfat[folds==j, ], id = i)
    cv.errors[j, i] <- mean((bodyfat$brozek[folds==j] - pred)^2)
 }
}
```
This creates a 10 $\times$ 14 matrix, where the $(i, j)^{\text{th}}$ element represents the test MSE for the $i^{\text{th}}$ cross-validation fold for the best $j$-variable model. We use `apply()` to average over the columns of this matrix in order to obtain a vector for which the $j^{\text{th}}$ element is the cross-validation error for the $j$-variable model.
```{r}
mean.cv.errors <- apply(cv.errors, 2,  mean)
mean.cv.errors
cv.errors
par(mfrow=c(1, 1))
plot(mean.cv.errors, type="b")
```

We see that cross-validation selects a three-variable model. We will use `breg_full` in order to obtain the three-variable model.
```{r}
#breg <- regsubsets(brozek ~., data = bodyfat, nvmax = 4)
coef(breg_full, 3)
```

## `caret`: Stepwise and Cross-Validation put together

The [`caret`](https://cran.r-project.org/web/packages/caret/index.html) package makes modelling in R easier. It combines two in one:

- automatically resamples the models 
- while conducting parameter tuning

`caret`'s most affable feature is its consistent modelling syntax which enables you to build and compare models with very little splurge. By simply changing the `method` argument, you can easily investigate varying applicable models adopted from the pre-existing R packages. In total there are [over two hundred different models](https://topepo.github.io/caret/available-models.html) available in `caret`. 

Another sublime feature of `caret` lies in its `train()` function, which enables the use of the same function to run all of the competing models. It accepts several caret-specific arguments that provide capabilities for different resampling methods, performance measures, and algorithms for choosing the best model. Running the `train()` function will create a train object with estimated parameters for a selected model from a set of given data with many other useful results.  

We will use `caret` to adopt 10-fold cross-validation, that will split `bodyfat` data into 10 approximately equal chunks to perform stepwise selections using the `leapBackward` method adopted from the `leaps` package.
The model will be developed based on 9 chunks and predict the $\text{$10^{th}$}$ until all of the folds are treated as a validation set, while the model is fitted on the remaining 10-1 folds. `caret` takes care of setting up this resampling with the help of the `trainControl()` function and the `trControl` argument of its `train()` function. As the data set contains 14 predictors, we will vary `nvmax` from 1 to 14 resulting in the identification of the 14 best models with different sizes: the best 1-variable model, the best 2-variables model, and so on up to the best 14-variables model.


```{r}
#If you don't have the "caret" package installed yet, uncomment and run the line below
#install.packages("caret")
# Train the model
suppressPackageStartupMessages(library(caret))
set.seed(111)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
stepwb <- train(brozek ~., data = bodyfat,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:14),
                    trControl = train.control
                    )
class(stepwb)
stepwb$results
```
The output above shows different metrics and their standard deviation for comparing the accuracy of the 14 best models for a different number of variables. 

- `nvmax`: the number of variables; in the best model for the given number of variables
- `RMSE` and `MAE` are two different metrics measuring the prediction error of the model, implying that the lower their value, the better the model. 
- `Rsquared` indicates the correlation between the observed outcome values and the values predicted by the model, meaning that the higher the `Rsquared`, the better the model.
  
Using the 10-fold cross-validation we have estimated average prediction error (`RMSE`) for each of the 14 best models for the given number of variables. The `RMSE` statistical metric is used to compare the 14 selected models and to automatically choose the model that minimises the `RMSE` as the best model. It can be seen that the model with 8 variables (`nvmax` = 8) is the one that has the lowest `RMSE`. We can call `bestTune` from the created `stepwb` object of class `train` to directly display the "best tuning values" (`nvmax`) and `finalModel` to see selected variables of the best model.


```{r}
# Final model coefficients
stepwb$finalModel
# Summary of the model
stepwb$bestTune
summary(stepwb$finalModel)
```

💡Typing ```?caret::train``` inside R's console will bring a more detailed explanation on how to use the `train()` function and information about the expected outcomes.


### Summary

All of the above approaches have suggested different variable selection for the best fitted model. We have noticed by examining the correlogram high multicollinearity amongst the variables in the `bodyfat`. It reveals a weak correlation only between three to four variables. This is a frequently encountered problem in multiple regression analysis, where such an interrelationship among explanatory variables obscures their relationship with the response variable. This causes computational instability in model estimation resulting in the different suggestions of the best model for different variable selection approaches:
 
- Best Subsets Regression: has indicated that the models with a fewer than 4 predictors is insufficient, while a model with more than 8 predictors will overfit
- Stepwise Forward and Stepwise Backward selections: has suggested different sets of variables for the five-factor model and the six-factor model
- Validation Set: has recommended the five-factor model as the best one
- 10-Fold Cross-Validation: has argued the case for the three-factor model
- Backward Selection with automatic 10-Fold Cross-Validation adopted by the application of the `caret` package: identifies the eight-factor model as the best one

Our main goal was to derive a model to predict body fat (variable `brozek`) using all predictors except for `siri`, `density` and `free` from the `fat` data available from the `faraway` package. Guided by the outcomes of the above analysis and a desire to describe the behaviour of this multivariate data set parsimoniously (see Miller, 2002) we suggest the four-factor model for which $\bar{R}^2$ = 73.51%. Adding another 4 variables as suggested by Backward Selection with automatic 10-Fold Cross-Validation would increase $\bar{R}^2$ statistic by only 1.16%.

```{r}
coef(breg_full, 4)
```

$$brozek = -31.2967858 - 0.1255654 \; weight  + 0.9213725 \; abdom + 0.4463824 \; forearm - 1.3917662 \; wrist$$
Applying different subset selection approaches we have pruned a list of plausible explanatory variables down to a parsimonious collection of the “most useful” variables. We need to realise that subset selection approaches for multiple regression modelling should be used as a tool that can help us avoid the tiresome process of trying all possible combinations of explanatory variables, by testing variables one by one. However, as statistical models can serve different purposes we need to incorporate prior knowledge when it is possible. Solely reliance on subset selection approaches is wrong and it could be misleading (Smith, 2018).

**Reference**

M. Friendly. [Corrgrams: Exploratory displays for correlation matrices](http://euclid.psych.yorku.ca/datavis/papers/corrgram.pdf), The American Statistician 56(4): 316-324, 2002.

G. James, D. Witten, T. Hastie, R. Tibshirani. [Introduction to Statistical Learning with Applications in R](https://www.statlearning.com), Springer Texts in Statistics, 2013. 

M. Kuhn, K. Johnson. ["Applied Predictive Modeling"](https://vuquangnguyen2016.files.wordpress.com/2018/03/applied-predictive-modeling-max-kuhn-kjell-johnson_1518.pdf), Springer, 2013.

M. Kuhn. [The caret Package](https://topepo.github.io/caret/), 2019.

A. J. Miller. [Subset Selection in Regression](https://www.taylorfrancis.com/books/subset-selection-regression-alan-miller/10.1201/9781420035933). Monographs on Statistics and Applied Probability, Chapman & Hall/CRC, 2$^\text{nd}$ edition, 2002.

G. Smith. [Step away from stepwise](https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-018-0143-6.pdf). Journal of Big Data, pp. 5-32. 2018
